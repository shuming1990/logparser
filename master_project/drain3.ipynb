{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b3c0ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shuming/Downloads/Hadoop/abnormal_label.txt\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000028.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000026.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000027.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000025.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000024.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000023.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000022.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000024.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000023.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000022.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000022.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000023.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000022.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000024.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000022.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000023.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000022.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000023.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_02_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_02_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_02_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_02_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000008.log\n",
      "copy complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "source_path = os.path.abspath(r'/Users/shuming/Downloads/Hadoop')     # source directory\n",
    "target_path = os.path.abspath(r'/Users/shuming/Downloads/merged_hadoop/')    # target directory\n",
    "\n",
    "if not os.path.exists(target_path):     # creat target directory if it doesn't exist \n",
    "    os.makedirs(target_path)\n",
    "\n",
    "if os.path.exists(source_path):    \n",
    "    \n",
    "    for root, dirs, files in os.walk(source_path):\n",
    "        for file in files:\n",
    "            src_file = os.path.join(root, file)\n",
    "            shutil.copy(src_file, target_path)\n",
    "            print(src_file)\n",
    "\n",
    "print('copy complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad3f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os module consists of many functions about file and directory processing\n",
    "import os  \n",
    "meragefiledir = '/Users/shuming/Downloads/merged_hadoop/'\n",
    "filenames=os.listdir(meragefiledir)  \n",
    "# target file path\n",
    "file=open('/Users/shuming/Downloads/merged_hadoop/merged.log','w')  \n",
    "   \n",
    "for filename in filenames:  \n",
    "    if(filename != 'merged.log'):\n",
    "        filepath=meragefiledir+filename    \n",
    "        for line in open(filepath):  \n",
    "            file.writelines(line)  \n",
    "        file.write('\\n')  \n",
    " \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "782b5b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task succeeded with attempt attempt_1445087491445_0006_r_000000_1000\n"
     ]
    }
   ],
   "source": [
    "line = \"2015-10-17 22:56:20,436 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445087491445_0006_r_000000_1000\"\n",
    "print(line.partition(\": \")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4b98610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (1): loaded properties from hadoop-metrics2.properties\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 1, \"cluster_size\": 1, \"template_mined\": \"loaded properties from hadoop-metrics2.properties\", \"cluster_count\": 1}\n",
      "Input (2): Scheduled snapshot period at 10 second(s).\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 2, \"cluster_size\": 1, \"template_mined\": \"Scheduled snapshot period at <:NUM:> second(s).\", \"cluster_count\": 2}\n",
      "Input (3): MapTask metrics system started\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 3, \"cluster_size\": 1, \"template_mined\": \"MapTask metrics system started\", \"cluster_count\": 3}\n",
      "Input (4): Executing with tokens:\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 4, \"cluster_size\": 1, \"template_mined\": \"Executing with tokens:\", \"cluster_count\": 4}\n",
      "Input (5): Kind: mapreduce.job, Service: job_1445076437777_0005, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@666adef3)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 5, \"cluster_size\": 1, \"template_mined\": \"Kind: mapreduce.job, Service: job <:NUM:> <:NUM:>, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@666adef3)\", \"cluster_count\": 5}\n",
      "Input (6): Sleeping for 0ms before retrying again. Got null now.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 6, \"cluster_size\": 1, \"template_mined\": \"Sleeping for 0ms before retrying again. Got null now.\", \"cluster_count\": 6}\n",
      "Input (7): mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445076437777_0005\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 7, \"cluster_size\": 1, \"template_mined\": \"mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application <:NUM:> <:NUM:>\", \"cluster_count\": 7}\n",
      "Input (8): session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 8, \"cluster_size\": 1, \"template_mined\": \"session.id is deprecated. Instead, use dfs.metrics.session-id\", \"cluster_count\": 8}\n",
      "Input (9): ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 9, \"cluster_size\": 1, \"template_mined\": \"ProcfsBasedProcessTree currently is supported only on Linux.\", \"cluster_count\": 9}\n",
      "Input (10):  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6270c836\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 10, \"cluster_size\": 1, \"template_mined\": \"Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6270c836\", \"cluster_count\": 10}\n",
      "Input (11): Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:268435456+134217728\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 11, \"cluster_size\": 1, \"template_mined\": \"Processing split: hdfs://msra-sa-<:NUM:>:<:NUM:>/pageinput2.txt:<:NUM:>+<:NUM:>\", \"cluster_count\": 11}\n",
      "Input (12): (EQUATOR) 0 kvi 26214396(104857584)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 12, \"cluster_size\": 1, \"template_mined\": \"(EQUATOR) <:NUM:> kvi <:NUM:>(<:NUM:>)\", \"cluster_count\": 12}\n",
      "Input (13): mapreduce.task.io.sort.mb: 100\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 13, \"cluster_size\": 1, \"template_mined\": \"mapreduce.task.io.sort.mb: <:NUM:>\", \"cluster_count\": 13}\n",
      "Input (14): soft limit at 83886080\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 14, \"cluster_size\": 1, \"template_mined\": \"soft limit at <:NUM:>\", \"cluster_count\": 14}\n",
      "Input (15): bufstart = 0; bufvoid = 104857600\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 15, \"cluster_size\": 1, \"template_mined\": \"bufstart = <:NUM:>; bufvoid = <:NUM:>\", \"cluster_count\": 15}\n",
      "Input (16): kvstart = 26214396; length = 6553600\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 16, \"cluster_size\": 1, \"template_mined\": \"kvstart = <:NUM:>; length = <:NUM:>\", \"cluster_count\": 16}\n",
      "Input (17): Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 17, \"cluster_size\": 1, \"template_mined\": \"Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\", \"cluster_count\": 17}\n",
      "Input (18): Spilling map output\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 18, \"cluster_size\": 1, \"template_mined\": \"Spilling map output\", \"cluster_count\": 18}\n",
      "Input (19): bufstart = 0; bufend = 48249276; bufvoid = 104857600\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 19, \"cluster_size\": 1, \"template_mined\": \"bufstart = <:NUM:>; bufend = <:NUM:>; bufvoid = <:NUM:>\", \"cluster_count\": 19}\n",
      "Input (20): kvstart = 26214396(104857584); kvend = 17305200(69220800); length = 8909197/6553600\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 20, \"cluster_size\": 1, \"template_mined\": \"kvstart = <:NUM:>(<:NUM:>); kvend = <:NUM:>(<:NUM:>); length = <:NUM:>/<:NUM:>\", \"cluster_count\": 20}\n",
      "Input (22): Finished spill 0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 21, \"cluster_size\": 1, \"template_mined\": \"Finished spill <:NUM:>\", \"cluster_count\": 21}\n",
      "Input (23): (RESET) equator 57318028 kv 14329500(57318000) kvi 12129788(48519152)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 22, \"cluster_size\": 1, \"template_mined\": \"(RESET) equator <:NUM:> kv <:NUM:>(<:NUM:>) kvi <:NUM:>(<:NUM:>)\", \"cluster_count\": 22}\n",
      "Input (60): Starting flush of map output\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 23, \"cluster_size\": 1, \"template_mined\": \"Starting flush of map output\", \"cluster_count\": 23}\n",
      "Input (65): Merging 8 sorted segments\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 24, \"cluster_size\": 1, \"template_mined\": \"Merging <:NUM:> sorted segments\", \"cluster_count\": 24}\n",
      "Input (66): Down to the last merge-pass, with 8 segments left of total size: 288688442 bytes\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 25, \"cluster_size\": 1, \"template_mined\": \"Down to the last merge-pass, with <:NUM:> segments left of total size: <:NUM:> bytes\", \"cluster_count\": 25}\n",
      "Input (67): Task:attempt_1445076437777_0005_m_000002_0 is done. And is in the process of committing\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 26, \"cluster_size\": 1, \"template_mined\": \"Task:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> is done. And is in the process of committing\", \"cluster_count\": 26}\n",
      "Input (68): Task 'attempt_1445076437777_0005_m_000002_0' done.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 27, \"cluster_size\": 1, \"template_mined\": \"Task 'attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>' done.\", \"cluster_count\": 27}\n",
      "Input (69): \n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 28, \"cluster_size\": 1, \"template_mined\": \"\", \"cluster_count\": 28}\n",
      "Input (74): Kind: mapreduce.job, Service: job_1445087491445_0005, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@7f0eb4b4)\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 5, \"cluster_size\": 2, \"template_mined\": \"Kind: mapreduce.job, Service: job <:NUM:> <:NUM:>, Ident: <:*:>\", \"cluster_count\": 28}\n",
      "Input (79):  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6ad3381f\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 10, \"cluster_size\": 2, \"template_mined\": \"Using ResourceCalculatorProcessTree : <:*:>\", \"cluster_count\": 28}\n",
      "Input (80): Processing split: hdfs://msra-sa-41:9000/wordcount2.txt:402653184+134217728\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 11, \"cluster_size\": 2, \"template_mined\": \"Processing split: <:*:>\", \"cluster_count\": 28}\n",
      "Input (122): I/O error constructing remote block reader.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 29, \"cluster_size\": 1, \"template_mined\": \"I/O error constructing remote block reader.\", \"cluster_count\": 29}\n",
      "Input (123): Connection timed out: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 30, \"cluster_size\": 1, \"template_mined\": \"Connection timed out: no further information\", \"cluster_count\": 30}\n",
      "Input (151): Failed to connect to /10.86.169.121:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 31, \"cluster_size\": 1, \"template_mined\": \"Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information\", \"cluster_count\": 31}\n",
      "Input (180): Successfully connected to /10.190.173.170:50010 for BP-1347369012-10.190.173.170-1444972147527:blk_1073742826_2022\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 32, \"cluster_size\": 1, \"template_mined\": \"Successfully connected to /<:IP:>:<:NUM:> for BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:>\", \"cluster_count\": 32}\n",
      "Input (271): Connection refused: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 33, \"cluster_size\": 1, \"template_mined\": \"Connection refused: no further information\", \"cluster_count\": 33}\n",
      "Input (296): Failed to connect to /10.86.169.121:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection refused: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 34, \"cluster_size\": 1, \"template_mined\": \"Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.ConnectException: Connection refused: no further information\", \"cluster_count\": 34}\n",
      "Input (402): Stopping MapTask metrics system...\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 35, \"cluster_size\": 1, \"template_mined\": \"Stopping MapTask metrics system...\", \"cluster_count\": 35}\n",
      "Input (403): MapTask metrics system stopped.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 3, \"cluster_size\": 6, \"template_mined\": \"MapTask metrics system <:*:>\", \"cluster_count\": 35}\n",
      "Input (404): MapTask metrics system shutdown complete.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 36, \"cluster_size\": 1, \"template_mined\": \"MapTask metrics system shutdown complete.\", \"cluster_count\": 36}\n",
      "Input (406): Created MRAppMaster for application appattempt_1445087491445_0007_000002\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 37, \"cluster_size\": 1, \"template_mined\": \"Created MRAppMaster for application appattempt <:NUM:> <:NUM:> <:NUM:>\", \"cluster_count\": 37}\n",
      "Input (408): Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 7 cluster_timestamp: 1445087491445 } attemptId: 2 } keyId: -1547346236)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 38, \"cluster_size\": 1, \"template_mined\": \"Kind: YARN AM RM TOKEN, Service: , Ident: (appAttemptId { application id { id: <:NUM:> cluster timestamp: <:NUM:> } attemptId: <:NUM:> } keyId: <:NUM:>)\", \"cluster_count\": 38}\n",
      "Input (409): Using mapred newApiCommitter.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 39, \"cluster_size\": 1, \"template_mined\": \"Using mapred newApiCommitter.\", \"cluster_count\": 39}\n",
      "Input (410): OutputCommitter set in config null\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 40, \"cluster_size\": 1, \"template_mined\": \"OutputCommitter set in config null\", \"cluster_count\": 40}\n",
      "Input (411): OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 41, \"cluster_size\": 1, \"template_mined\": \"OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\", \"cluster_count\": 41}\n",
      "Input (412): Registering class org.apache.hadoop.mapreduce.jobhistory.EventType for class org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 42, \"cluster_size\": 1, \"template_mined\": \"Registering class org.apache.hadoop.mapreduce.jobhistory.EventType for class org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler\", \"cluster_count\": 42}\n",
      "Input (413): Registering class org.apache.hadoop.mapreduce.v2.app.job.event.JobEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 42, \"cluster_size\": 2, \"template_mined\": \"Registering class <:*:> for class <:*:>\", \"cluster_count\": 42}\n",
      "Input (420): Default file system [hdfs://msra-sa-41:9000]\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 43, \"cluster_size\": 1, \"template_mined\": \"Default file system [hdfs://msra-sa-<:NUM:>:<:NUM:>]\", \"cluster_count\": 43}\n",
      "Input (423): Emitting job history data to the timeline server is not enabled\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 44, \"cluster_size\": 1, \"template_mined\": \"Emitting job history data to the timeline server is not enabled\", \"cluster_count\": 44}\n",
      "Input (424): Recovery is enabled. Will try to recover from previous life on best effort basis.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 45, \"cluster_size\": 1, \"template_mined\": \"Recovery is enabled. Will try to recover from previous life on best effort basis.\", \"cluster_count\": 45}\n",
      "Input (426): Previous history file is at hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007/job_1445087491445_0007_1.jhist\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 46, \"cluster_size\": 1, \"template_mined\": \"Previous history file is at hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist\", \"cluster_count\": 46}\n",
      "Input (427): Read from history task task_1445087491445_0007_m_000001\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 47, \"cluster_size\": 1, \"template_mined\": \"Read from history task task <:NUM:> <:NUM:> m <:NUM:>\", \"cluster_count\": 47}\n",
      "Input (437): Read completed tasks from history 10\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 48, \"cluster_size\": 1, \"template_mined\": \"Read completed tasks from history <:NUM:>\", \"cluster_count\": 48}\n",
      "Input (441): MRAppMaster metrics system started\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 49, \"cluster_size\": 1, \"template_mined\": \"MRAppMaster metrics system started\", \"cluster_count\": 49}\n",
      "Input (442): Adding job token for job_1445087491445_0007 to jobTokenSecretManager\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 50, \"cluster_size\": 1, \"template_mined\": \"Adding job token for job <:NUM:> <:NUM:> to jobTokenSecretManager\", \"cluster_count\": 50}\n",
      "Input (443): Not uberizing job_1445087491445_0007 because: not enabled; too many maps; too much input;\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 51, \"cluster_size\": 1, \"template_mined\": \"Not uberizing job <:NUM:> <:NUM:> because: not enabled; too many maps; too much input;\", \"cluster_count\": 51}\n",
      "Input (444): Input size for job job_1445087491445_0007 = 1313861632. Number of splits = 10\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 52, \"cluster_size\": 1, \"template_mined\": \"Input size for job job <:NUM:> <:NUM:> = <:NUM:>. Number of splits = <:NUM:>\", \"cluster_count\": 52}\n",
      "Input (445): Number of reduces for job job_1445087491445_0007 = 1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 53, \"cluster_size\": 1, \"template_mined\": \"Number of reduces for job job <:NUM:> <:NUM:> = <:NUM:>\", \"cluster_count\": 53}\n",
      "Input (446): job_1445087491445_0007Job Transitioned from NEW to INITED\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 54, \"cluster_size\": 1, \"template_mined\": \"job <:NUM:> 0007Job Transitioned from NEW to INITED\", \"cluster_count\": 54}\n",
      "Input (447): MRAppMaster launching normal, non-uberized, multi-container job job_1445087491445_0007.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 55, \"cluster_size\": 1, \"template_mined\": \"MRAppMaster launching normal, non-uberized, multi-container job job <:NUM:> <:NUM:>.\", \"cluster_count\": 55}\n",
      "Input (448): Using callQueue class java.util.concurrent.LinkedBlockingQueue\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 56, \"cluster_size\": 1, \"template_mined\": \"Using callQueue class java.util.concurrent.LinkedBlockingQueue\", \"cluster_count\": 56}\n",
      "Input (449): Starting Socket Reader #1 for port 24281\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 57, \"cluster_size\": 1, \"template_mined\": \"Starting Socket Reader #<:NUM:> for port <:NUM:>\", \"cluster_count\": 57}\n",
      "Input (450): Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 58, \"cluster_size\": 1, \"template_mined\": \"Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server\", \"cluster_count\": 58}\n",
      "Input (451): IPC Server listener on 24281: starting\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 59, \"cluster_size\": 1, \"template_mined\": \"IPC Server listener on <:NUM:>: starting\", \"cluster_count\": 59}\n",
      "Input (452): IPC Server Responder: starting\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 60, \"cluster_size\": 1, \"template_mined\": \"IPC Server Responder: starting\", \"cluster_count\": 60}\n",
      "Input (453): Instantiated MRClientService at MSRA-SA-41.fareast.corp.microsoft.com/10.190.173.170:24281\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 61, \"cluster_size\": 1, \"template_mined\": \"Instantiated MRClientService at MSRA-SA-<:NUM:>.fareast.corp.microsoft.com/<:IP:>:<:NUM:>\", \"cluster_count\": 61}\n",
      "Input (454): Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 62, \"cluster_size\": 1, \"template_mined\": \"Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\", \"cluster_count\": 62}\n",
      "Input (455): Http request log for http.requests.mapreduce is not defined\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 63, \"cluster_size\": 1, \"template_mined\": \"Http request log for http.requests.mapreduce is not defined\", \"cluster_count\": 63}\n",
      "Input (456): Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 64, \"cluster_size\": 1, \"template_mined\": \"Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\", \"cluster_count\": 64}\n",
      "Input (457): Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context mapreduce\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 65, \"cluster_size\": 1, \"template_mined\": \"Added filter AM PROXY FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context mapreduce\", \"cluster_count\": 65}\n",
      "Input (458): Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context static\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 65, \"cluster_size\": 2, \"template_mined\": \"Added filter AM PROXY FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context <:*:>\", \"cluster_count\": 65}\n",
      "Input (459): adding path spec: /mapreduce/*\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 66, \"cluster_size\": 1, \"template_mined\": \"adding path spec: /mapreduce/*\", \"cluster_count\": 66}\n",
      "Input (460): adding path spec: /ws/*\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 66, \"cluster_size\": 2, \"template_mined\": \"adding path spec: <:*:>\", \"cluster_count\": 66}\n",
      "Input (461): Jetty bound to port 24289\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 67, \"cluster_size\": 1, \"template_mined\": \"Jetty bound to port <:NUM:>\", \"cluster_count\": 67}\n",
      "Input (462): jetty-6.1.26\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 68, \"cluster_size\": 1, \"template_mined\": \"jetty-<:NUM:>.<:NUM:>.<:NUM:>\", \"cluster_count\": 68}\n",
      "Input (463): Extract jar:file:/D:/hadoop-2.6.0-localbox/share/hadoop/yarn/hadoop-yarn-common-2.6.0-SNAPSHOT.jar!/webapps/mapreduce to C:\\Users\\msrabi\\AppData\\Local\\Temp\\2\\Jetty_0_0_0_0_24289_mapreduce____.6ucjbs\\webapp\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 69, \"cluster_size\": 1, \"template_mined\": \"Extract jar:file:/D:/hadoop-<:NUM:>.<:NUM:>.<:NUM:>-localbox/share/hadoop/yarn/hadoop-yarn-common-<:NUM:>.<:NUM:>.<:NUM:>-SNAPSHOT.jar!/webapps/mapreduce to C:\\\\Users\\\\msrabi\\\\AppData\\\\Local\\\\Temp\\\\<:NUM:>\\\\Jetty <:NUM:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> mapreduce .6ucjbs\\\\webapp\", \"cluster_count\": 69}\n",
      "Input (464): Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:24289\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 70, \"cluster_size\": 1, \"template_mined\": \"Started HttpServer2$SelectChannelConnectorWithSafeStartup@<:IP:>:<:NUM:>\", \"cluster_count\": 70}\n",
      "Input (465): Web app /mapreduce started at 24289\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 71, \"cluster_size\": 1, \"template_mined\": \"Web app /mapreduce started at <:NUM:>\", \"cluster_count\": 71}\n",
      "Input (466): Registered webapp guice modules\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 72, \"cluster_size\": 1, \"template_mined\": \"Registered webapp guice modules\", \"cluster_count\": 72}\n",
      "Input (467): JOB_CREATE job_1445087491445_0007\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 73, \"cluster_size\": 1, \"template_mined\": \"JOB CREATE job <:NUM:> <:NUM:>\", \"cluster_count\": 73}\n",
      "Input (472): nodeBlacklistingEnabled:true\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 74, \"cluster_size\": 1, \"template_mined\": \"nodeBlacklistingEnabled:true\", \"cluster_count\": 74}\n",
      "Input (473): maxTaskFailuresPerNode is 3\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 75, \"cluster_size\": 1, \"template_mined\": \"maxTaskFailuresPerNode is <:NUM:>\", \"cluster_count\": 75}\n",
      "Input (474): blacklistDisablePercent is 33\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 76, \"cluster_size\": 1, \"template_mined\": \"blacklistDisablePercent is <:NUM:>\", \"cluster_count\": 76}\n",
      "Input (475): Connecting to ResourceManager at MSRA-SA-41/10.190.173.170:8030\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 77, \"cluster_size\": 1, \"template_mined\": \"Connecting to ResourceManager at MSRA-SA-<:NUM:>/<:IP:>:<:NUM:>\", \"cluster_count\": 77}\n",
      "Input (476): maxContainerCapability: <memory:8192, vCores:32>\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 78, \"cluster_size\": 1, \"template_mined\": \"maxContainerCapability: <memory:<:NUM:>, vCores:<:NUM:>>\", \"cluster_count\": 78}\n",
      "Input (477): queue: default\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 79, \"cluster_size\": 1, \"template_mined\": \"queue: default\", \"cluster_count\": 79}\n",
      "Input (478): Upper limit on the thread pool size is 500\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 80, \"cluster_size\": 1, \"template_mined\": \"Upper limit on the thread pool size is <:NUM:>\", \"cluster_count\": 80}\n",
      "Input (479): yarn.client.max-cached-nodemanagers-proxies : 0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 81, \"cluster_size\": 1, \"template_mined\": \"yarn.client.max-cached-nodemanagers-proxies : <:NUM:>\", \"cluster_count\": 81}\n",
      "Input (480): job_1445087491445_0007Job Transitioned from INITED to SETUP\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 54, \"cluster_size\": 2, \"template_mined\": \"job <:NUM:> 0007Job Transitioned from <:*:> to <:*:>\", \"cluster_count\": 81}\n",
      "Input (481): Processing the event EventType: JOB_SETUP\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 82, \"cluster_size\": 1, \"template_mined\": \"Processing the event EventType: JOB SETUP\", \"cluster_count\": 82}\n",
      "Input (483): Recovering task task_1445087491445_0007_m_000000 from prior app attempt, status was SUCCEEDED\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 83, \"cluster_size\": 1, \"template_mined\": \"Recovering task task <:NUM:> <:NUM:> m <:NUM:> from prior app attempt, status was SUCCEEDED\", \"cluster_count\": 83}\n",
      "Input (484): Resolved MININT-75DGDAM1.fareast.corp.microsoft.com to /default-rack\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 84, \"cluster_size\": 1, \"template_mined\": \"Resolved MININT-75DGDAM1.fareast.corp.microsoft.com to /default-rack\", \"cluster_count\": 84}\n",
      "Input (485): Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 84, \"cluster_size\": 2, \"template_mined\": \"Resolved <:*:> to /default-rack\", \"cluster_count\": 84}\n",
      "Input (487): TaskAttempt: [attempt_1445087491445_0007_m_000000_0] using containerId: [container_1445087491445_0007_01_000002 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:30535]\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 85, \"cluster_size\": 1, \"template_mined\": \"TaskAttempt: [attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>] using containerId: [container <:NUM:> <:NUM:> <:NUM:> <:NUM:> on NM: [MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>]\", \"cluster_count\": 85}\n",
      "Input (488): attempt_1445087491445_0007_m_000000_0 TaskAttempt Transitioned from NEW to SUCCEEDED\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 86, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> TaskAttempt Transitioned from NEW to SUCCEEDED\", \"cluster_count\": 86}\n",
      "Input (489): Task succeeded with attempt attempt_1445087491445_0007_m_000000_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 87, \"cluster_size\": 1, \"template_mined\": \"Task succeeded with attempt attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 87}\n",
      "Input (490): task_1445087491445_0007_m_000000 Task Transitioned from NEW to SUCCEEDED\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 88, \"cluster_size\": 1, \"template_mined\": \"task <:NUM:> <:NUM:> m <:NUM:> Task Transitioned from NEW to SUCCEEDED\", \"cluster_count\": 88}\n",
      "Input (500): Event Writer setup for JobId: job_1445087491445_0007, File: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007/job_1445087491445_0007_2.jhist\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 89, \"cluster_size\": 1, \"template_mined\": \"Event Writer setup for JobId: job <:NUM:> <:NUM:>, File: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist\", \"cluster_count\": 89}\n",
      "Input (564): task_1445087491445_0007_r_000000 Task Transitioned from NEW to SCHEDULED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 88, \"cluster_size\": 11, \"template_mined\": \"task <:NUM:> <:NUM:> <:*:> <:NUM:> Task Transitioned from NEW to <:*:>\", \"cluster_count\": 89}\n",
      "Input (565): Num completed Tasks: 1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 90, \"cluster_size\": 1, \"template_mined\": \"Num completed Tasks: <:NUM:>\", \"cluster_count\": 90}\n",
      "Input (575): attempt_1445087491445_0007_r_000000_1000 TaskAttempt Transitioned from NEW to UNASSIGNED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 86, \"cluster_size\": 11, \"template_mined\": \"attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from NEW to <:*:>\", \"cluster_count\": 90}\n",
      "Input (576): reduceResourceRequest:<memory:1024, vCores:1>\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 91, \"cluster_size\": 1, \"template_mined\": \"reduceResourceRequest:<memory:<:NUM:>, vCores:<:NUM:>>\", \"cluster_count\": 91}\n",
      "Input (577): Before Scheduling: PendingReds:1 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:10 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 92, \"cluster_size\": 1, \"template_mined\": \"Before Scheduling: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\", \"cluster_count\": 92}\n",
      "Input (578): Recalculating schedule, headroom=<memory:16384, vCores:-26>\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 93, \"cluster_size\": 1, \"template_mined\": \"Recalculating schedule, headroom=<memory:<:NUM:>, vCores:<:NUM:>>\", \"cluster_count\": 93}\n",
      "Input (579): Reduce slow start threshold reached. Scheduling reduces.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 94, \"cluster_size\": 1, \"template_mined\": \"Reduce slow start threshold reached. Scheduling reduces.\", \"cluster_count\": 94}\n",
      "Input (580): All maps assigned. Ramping up all remaining reduces:1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 95, \"cluster_size\": 1, \"template_mined\": \"All maps assigned. Ramping up all remaining reduces:<:NUM:>\", \"cluster_count\": 95}\n",
      "Input (581): After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:1 AssignedMaps:0 AssignedReds:0 CompletedMaps:10 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 96, \"cluster_size\": 1, \"template_mined\": \"After Scheduling: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\", \"cluster_count\": 96}\n",
      "Input (582): getResources() for application_1445087491445_0007: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:16384, vCores:-26> knownNMs=6\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 97, \"cluster_size\": 1, \"template_mined\": \"getResources() for application <:NUM:> <:NUM:>: ask=<:NUM:> release= <:NUM:> newContainers=<:NUM:> finishedContainers=<:NUM:> resourcelimit=<memory:<:NUM:>, vCores:<:NUM:>> knownNMs=<:NUM:>\", \"cluster_count\": 97}\n",
      "Input (583): Got allocated containers 1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 98, \"cluster_size\": 1, \"template_mined\": \"Got allocated containers <:NUM:>\", \"cluster_count\": 98}\n",
      "Input (584): Assigned to reduce\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 99, \"cluster_size\": 1, \"template_mined\": \"Assigned to reduce\", \"cluster_count\": 99}\n",
      "Input (585): Assigned container container_1445087491445_0007_02_000003 to attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 100, \"cluster_size\": 1, \"template_mined\": \"Assigned container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> to attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 100}\n",
      "Input (588): The job-jar file on the remote FS is hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007/job.jar\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 101, \"cluster_size\": 1, \"template_mined\": \"The job-jar file on the remote FS is hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job.jar\", \"cluster_count\": 101}\n",
      "Input (589): The job-conf file on the remote FS is /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007/job.xml\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 101, \"cluster_size\": 2, \"template_mined\": \"The <:*:> file on the remote FS is <:*:> <:NUM:> <:*:>\", \"cluster_count\": 101}\n",
      "Input (590): Adding #0 tokens and #1 secret keys for NM use for launching container\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 102, \"cluster_size\": 1, \"template_mined\": \"Adding #<:NUM:> tokens and #<:NUM:> secret keys for NM use for launching container\", \"cluster_count\": 102}\n",
      "Input (591): Size of containertokens_dob is 1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 103, \"cluster_size\": 1, \"template_mined\": \"Size of containertokens dob is <:NUM:>\", \"cluster_count\": 103}\n",
      "Input (592): Putting shuffle token in serviceData\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 104, \"cluster_size\": 1, \"template_mined\": \"Putting shuffle token in serviceData\", \"cluster_count\": 104}\n",
      "Input (593): attempt_1445087491445_0007_r_000000_1000 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 86, \"cluster_size\": 12, \"template_mined\": \"attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> to <:*:>\", \"cluster_count\": 104}\n",
      "Input (594): Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445087491445_0007_02_000003 taskAttempt attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 105, \"cluster_size\": 1, \"template_mined\": \"Processing the event EventType: CONTAINER REMOTE LAUNCH for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> taskAttempt attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 105}\n",
      "Input (595): Launching attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 106, \"cluster_size\": 1, \"template_mined\": \"Launching attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 106}\n",
      "Input (596): Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:49130\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 107, \"cluster_size\": 1, \"template_mined\": \"Opening proxy : MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>\", \"cluster_count\": 107}\n",
      "Input (597): Shuffle port returned by ContainerManager for attempt_1445087491445_0007_r_000000_1000 : 13562\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 108, \"cluster_size\": 1, \"template_mined\": \"Shuffle port returned by ContainerManager for attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> : <:NUM:>\", \"cluster_count\": 108}\n",
      "Input (598): TaskAttempt: [attempt_1445087491445_0007_r_000000_1000] using containerId: [container_1445087491445_0007_02_000003 on NM: [MSRA-SA-39.fareast.corp.microsoft.com:49130]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 85, \"cluster_size\": 11, \"template_mined\": \"TaskAttempt: [attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>] using containerId: [container <:NUM:> <:NUM:> <:NUM:> <:NUM:> on NM: [MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>]\", \"cluster_count\": 108}\n",
      "Input (600): ATTEMPT_START task_1445087491445_0007_r_000000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 109, \"cluster_size\": 1, \"template_mined\": \"ATTEMPT START task <:NUM:> <:NUM:> r <:NUM:>\", \"cluster_count\": 109}\n",
      "Input (601): task_1445087491445_0007_r_000000 Task Transitioned from SCHEDULED to RUNNING\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 88, \"cluster_size\": 12, \"template_mined\": \"task <:NUM:> <:NUM:> <:*:> <:NUM:> Task Transitioned from <:*:> to <:*:>\", \"cluster_count\": 109}\n",
      "Input (603): Auth successful for job_1445087491445_0007 (auth:SIMPLE)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 110, \"cluster_size\": 1, \"template_mined\": \"Auth successful for job <:NUM:> <:NUM:> (auth:SIMPLE)\", \"cluster_count\": 110}\n",
      "Input (604): JVM with ID : jvm_1445087491445_0007_r_000003 asked for a task\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 111, \"cluster_size\": 1, \"template_mined\": \"JVM with ID : jvm <:NUM:> <:NUM:> r <:NUM:> asked for a task\", \"cluster_count\": 111}\n",
      "Input (605): JVM with ID: jvm_1445087491445_0007_r_000003 given task: attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 112, \"cluster_size\": 1, \"template_mined\": \"JVM with ID: jvm <:NUM:> <:NUM:> r <:NUM:> given task: attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 112}\n",
      "Input (606): MapCompletionEvents request from attempt_1445087491445_0007_r_000000_1000. startIndex 0 maxEvents 10000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 113, \"cluster_size\": 1, \"template_mined\": \"MapCompletionEvents request from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>. startIndex <:NUM:> maxEvents <:NUM:>\", \"cluster_count\": 113}\n",
      "Input (612): Progress of TaskAttempt attempt_1445087491445_0007_r_000000_1000 is : 0.13333334\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 114, \"cluster_size\": 1, \"template_mined\": \"Progress of TaskAttempt attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> is : <:NUM:>.<:NUM:>\", \"cluster_count\": 114}\n",
      "Input (699): DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743028_2240\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 115, \"cluster_size\": 1, \"template_mined\": \"DFSOutputStream ResponseProcessor exception for block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:>\", \"cluster_count\": 115}\n",
      "Input (700): Bad response ERROR for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743028_2240 from datanode 10.86.169.121:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 116, \"cluster_size\": 1, \"template_mined\": \"Bad response ERROR for block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> from datanode <:IP:>:<:NUM:>\", \"cluster_count\": 116}\n",
      "Input (702): Error Recovery for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743028_2240 in pipeline 10.190.173.170:50010, 10.86.169.121:50010: bad datanode 10.86.169.121:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 117, \"cluster_size\": 1, \"template_mined\": \"Error Recovery for block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> in pipeline <:IP:>:<:NUM:>, <:IP:>:<:NUM:>: bad datanode <:IP:>:<:NUM:>\", \"cluster_count\": 117}\n",
      "Input (819): Commit-pending state update from attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 118, \"cluster_size\": 1, \"template_mined\": \"Commit-pending state update from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 118}\n",
      "Input (820): attempt_1445087491445_0007_r_000000_1000 TaskAttempt Transitioned from RUNNING to COMMIT_PENDING\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 119, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> TaskAttempt Transitioned from RUNNING to COMMIT PENDING\", \"cluster_count\": 119}\n",
      "Input (821): attempt_1445087491445_0007_r_000000_1000 given a go for committing the task output.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 120, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> given a go for committing the task output.\", \"cluster_count\": 120}\n",
      "Input (822): Commit go/no-go request from attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 121, \"cluster_size\": 1, \"template_mined\": \"Commit go/no-go request from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 121}\n",
      "Input (823): Result of canCommit for attempt_1445087491445_0007_r_000000_1000:true\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 122, \"cluster_size\": 1, \"template_mined\": \"Result of canCommit for attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>:true\", \"cluster_count\": 122}\n",
      "Input (825): Done acknowledgement from attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 123, \"cluster_size\": 1, \"template_mined\": \"Done acknowledgement from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 123}\n",
      "Input (826): attempt_1445087491445_0007_r_000000_1000 TaskAttempt Transitioned from COMMIT_PENDING to SUCCESS_CONTAINER_CLEANUP\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 124, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> TaskAttempt Transitioned from COMMIT PENDING to SUCCESS CONTAINER CLEANUP\", \"cluster_count\": 124}\n",
      "Input (827): Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445087491445_0007_02_000003 taskAttempt attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 105, \"cluster_size\": 2, \"template_mined\": \"Processing the event EventType: CONTAINER REMOTE <:*:> for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> taskAttempt attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 124}\n",
      "Input (828): KILLING attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 125, \"cluster_size\": 1, \"template_mined\": \"KILLING attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 125}\n",
      "Input (830): attempt_1445087491445_0007_r_000000_1000 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 120, \"cluster_size\": 2, \"template_mined\": \"attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:>\", \"cluster_count\": 125}\n",
      "Input (831): Task succeeded with attempt attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 87, \"cluster_size\": 11, \"template_mined\": \"Task succeeded with attempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 125}\n",
      "Input (835): Processing the event EventType: JOB_COMMIT\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 82, \"cluster_size\": 2, \"template_mined\": \"Processing the event EventType: JOB <:*:>\", \"cluster_count\": 125}\n",
      "Input (836): Calling handler for JobFinishedEvent\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 126, \"cluster_size\": 1, \"template_mined\": \"Calling handler for JobFinishedEvent\", \"cluster_count\": 126}\n",
      "Input (838): We are finishing cleanly so this is the last retry\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 127, \"cluster_size\": 1, \"template_mined\": \"We are finishing cleanly so this is the last retry\", \"cluster_count\": 127}\n",
      "Input (839): Notify RMCommunicator isAMLastRetry: true\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 128, \"cluster_size\": 1, \"template_mined\": \"Notify RMCommunicator isAMLastRetry: true\", \"cluster_count\": 128}\n",
      "Input (840): RMCommunicator notified that shouldUnregistered is: true\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 129, \"cluster_size\": 1, \"template_mined\": \"RMCommunicator notified that shouldUnregistered is: true\", \"cluster_count\": 129}\n",
      "Input (841): Notify JHEH isAMLastRetry: true\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 128, \"cluster_size\": 2, \"template_mined\": \"Notify <:*:> isAMLastRetry: true\", \"cluster_count\": 129}\n",
      "Input (842): JobHistoryEventHandler notified that forceJobCompletion is true\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 130, \"cluster_size\": 1, \"template_mined\": \"JobHistoryEventHandler notified that forceJobCompletion is true\", \"cluster_count\": 130}\n",
      "Input (843): Calling stop for all the services\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 131, \"cluster_size\": 1, \"template_mined\": \"Calling stop for all the services\", \"cluster_count\": 131}\n",
      "Input (844): Stopping JobHistoryEventHandler. Size of the outstanding queue size is 0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 132, \"cluster_size\": 1, \"template_mined\": \"Stopping JobHistoryEventHandler. Size of the outstanding queue size is <:NUM:>\", \"cluster_count\": 132}\n",
      "Input (846): Copying hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007/job_1445087491445_0007_2.jhist to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 133, \"cluster_size\": 1, \"template_mined\": \"Copying hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist to hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\", \"cluster_count\": 133}\n",
      "Input (848): Copied to done location: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 134, \"cluster_size\": 1, \"template_mined\": \"Copied to done location: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\", \"cluster_count\": 134}\n",
      "Input (850): Copying hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007/job_1445087491445_0007_2_conf.xml to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 135, \"cluster_size\": 1, \"template_mined\": \"Copying hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:> conf.xml to hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\", \"cluster_count\": 135}\n",
      "Input (854): Moved tmp to done: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 136, \"cluster_size\": 1, \"template_mined\": \"Moved tmp to done: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\", \"cluster_count\": 136}\n",
      "Input (863): Stopped JobHistoryEventHandler. super.stop()\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 137, \"cluster_size\": 1, \"template_mined\": \"Stopped JobHistoryEventHandler. super.stop()\", \"cluster_count\": 137}\n",
      "Input (864): Setting job diagnostics to\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 138, \"cluster_size\": 1, \"template_mined\": \"Setting job diagnostics to\", \"cluster_count\": 138}\n",
      "Input (865): History url is http://MSRA-SA-41.fareast.corp.microsoft.com:19888/jobhistory/job/job_1445087491445_0007\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 139, \"cluster_size\": 1, \"template_mined\": \"History url is http://MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>/jobhistory/job/job <:NUM:> <:NUM:>\", \"cluster_count\": 139}\n",
      "Input (866): Waiting for application to be successfully unregistered.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 140, \"cluster_size\": 1, \"template_mined\": \"Waiting for application to be successfully unregistered.\", \"cluster_count\": 140}\n",
      "Input (867): Final Stats: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:1 CompletedMaps:10 CompletedReds:1 ContAlloc:1 ContRel:0 HostLocal:0 RackLocal:0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 141, \"cluster_size\": 1, \"template_mined\": \"Final Stats: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\", \"cluster_count\": 141}\n",
      "Input (868): Deleting staging directory hdfs://msra-sa-41:9000 /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 142, \"cluster_size\": 1, \"template_mined\": \"Deleting staging directory hdfs://msra-sa-<:NUM:>:<:NUM:> /tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>\", \"cluster_count\": 142}\n",
      "Input (869): Stopping server on 24300\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 143, \"cluster_size\": 1, \"template_mined\": \"Stopping server on <:NUM:>\", \"cluster_count\": 143}\n",
      "Input (870): Stopping IPC Server listener on 24300\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 144, \"cluster_size\": 1, \"template_mined\": \"Stopping IPC Server listener on <:NUM:>\", \"cluster_count\": 144}\n",
      "Input (871): TaskHeartbeatHandler thread interrupted\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 145, \"cluster_size\": 1, \"template_mined\": \"TaskHeartbeatHandler thread interrupted\", \"cluster_count\": 145}\n",
      "Input (872): Stopping IPC Server Responder\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 146, \"cluster_size\": 1, \"template_mined\": \"Stopping IPC Server Responder\", \"cluster_count\": 146}\n",
      "Input (1571): job_1445182159119_0016Job Transitioned from NEW to INITED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 54, \"cluster_size\": 6, \"template_mined\": \"job <:NUM:> <:*:> Transitioned from <:*:> to <:*:>\", \"cluster_count\": 146}\n",
      "Input (1588): Extract jar:file:/D:/hadoop-2.6.0-localbox/share/hadoop/yarn/hadoop-yarn-common-2.6.0-SNAPSHOT.jar!/webapps/mapreduce to C:\\Users\\msrabi\\AppData\\Local\\Temp\\2\\Jetty_0_0_0_0_55793_mapreduce____.11rul4\\webapp\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 69, \"cluster_size\": 2, \"template_mined\": \"Extract jar:file:/D:/hadoop-<:NUM:>.<:NUM:>.<:NUM:>-localbox/share/hadoop/yarn/hadoop-yarn-common-<:NUM:>.<:NUM:>.<:NUM:>-SNAPSHOT.jar!/webapps/mapreduce to C:\\\\Users\\\\msrabi\\\\AppData\\\\Local\\\\Temp\\\\<:NUM:>\\\\Jetty <:NUM:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> mapreduce <:*:>\", \"cluster_count\": 146}\n",
      "Input (1600): Connecting to ResourceManager at msra-sa-41/10.190.173.170:8030\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 77, \"cluster_size\": 2, \"template_mined\": \"Connecting to ResourceManager at <:*:>\", \"cluster_count\": 146}\n",
      "Input (1650): mapResourceRequest:<memory:1024, vCores:1>\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 147, \"cluster_size\": 1, \"template_mined\": \"mapResourceRequest:<memory:<:NUM:>, vCores:<:NUM:>>\", \"cluster_count\": 147}\n",
      "Input (1656): Reduce slow start threshold not met. completedMapsForReduceSlowstart 1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 148, \"cluster_size\": 1, \"template_mined\": \"Reduce slow start threshold not met. completedMapsForReduceSlowstart <:NUM:>\", \"cluster_count\": 148}\n",
      "Input (1658): Assigned container container_1445182159119_0016_01_000002 to attempt_1445182159119_0016_m_000000_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 100, \"cluster_size\": 2, \"template_mined\": \"Assigned container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> to attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 148}\n",
      "Input (1678): Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445182159119_0016_01_000002 taskAttempt attempt_1445182159119_0016_m_000000_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 105, \"cluster_size\": 3, \"template_mined\": \"Processing the event EventType: CONTAINER REMOTE <:*:> for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> taskAttempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 148}\n",
      "Input (1682): Launching attempt_1445182159119_0016_m_000003_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 106, \"cluster_size\": 2, \"template_mined\": \"Launching attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 148}\n",
      "Input (1690): Shuffle port returned by ContainerManager for attempt_1445182159119_0016_m_000002_0 : 13562\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 108, \"cluster_size\": 2, \"template_mined\": \"Shuffle port returned by ContainerManager for attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> : <:NUM:>\", \"cluster_count\": 148}\n",
      "Input (1702): ATTEMPT_START task_1445182159119_0016_m_000003\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 109, \"cluster_size\": 2, \"template_mined\": \"ATTEMPT START task <:NUM:> <:NUM:> <:*:> <:NUM:>\", \"cluster_count\": 148}\n",
      "Input (1725): Opening proxy : MININT-FNANLI5.fareast.corp.microsoft.com:59190\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 107, \"cluster_size\": 7, \"template_mined\": \"Opening proxy : <:*:>\", \"cluster_count\": 148}\n",
      "Input (1727): TaskAttempt: [attempt_1445182159119_0016_m_000004_0] using containerId: [container_1445182159119_0016_01_000006 on NM: [MININT-FNANLI5.fareast.corp.microsoft.com:59190]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 85, \"cluster_size\": 16, \"template_mined\": \"TaskAttempt: [attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>] using containerId: [container <:NUM:> <:NUM:> <:NUM:> <:NUM:> on NM: <:*:>\", \"cluster_count\": 148}\n",
      "Input (1749): JVM with ID : jvm_1445182159119_0016_m_000002 asked for a task\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 111, \"cluster_size\": 2, \"template_mined\": \"JVM with ID : jvm <:NUM:> <:NUM:> <:*:> <:NUM:> asked for a task\", \"cluster_count\": 148}\n",
      "Input (1750): JVM with ID: jvm_1445182159119_0016_m_000002 given task: attempt_1445182159119_0016_m_000000_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 112, \"cluster_size\": 2, \"template_mined\": \"JVM with ID: jvm <:NUM:> <:NUM:> <:*:> <:NUM:> given task: attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 148}\n",
      "Input (1828): Cannot assign container Container: [ContainerId: container_1445182159119_0016_01_000012, NodeId: MININT-75DGDAM1.fareast.corp.microsoft.com:58081, NodeHttpAddress: MININT-75DGDAM1.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.86.165.66:58081 }, ] for a map as either  container memory less than required <memory:1024, vCores:1> or no pending map tasks - maps.isEmpty=true\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 149, \"cluster_size\": 1, \"template_mined\": \"Cannot assign container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: MININT-75DGDAM1.fareast.corp.microsoft.com:<:NUM:>, NodeHttpAddress: MININT-75DGDAM1.fareast.corp.microsoft.com:<:NUM:>, Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] for a map as either container memory less than required <memory:<:NUM:>, vCores:<:NUM:>> or no pending map tasks - maps.isEmpty=true\", \"cluster_count\": 149}\n",
      "Input (1829): Cannot assign container Container: [ContainerId: container_1445182159119_0016_01_000013, NodeId: MININT-FNANLI5.fareast.corp.microsoft.com:59190, NodeHttpAddress: MININT-FNANLI5.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.86.169.121:59190 }, ] for a map as either  container memory less than required <memory:1024, vCores:1> or no pending map tasks - maps.isEmpty=true\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 149, \"cluster_size\": 2, \"template_mined\": \"Cannot assign container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] for a map as either container memory less than required <memory:<:NUM:>, vCores:<:NUM:>> or no pending map tasks - maps.isEmpty=true\", \"cluster_count\": 149}\n",
      "Input (1837): Received completed container container_1445182159119_0016_01_000013\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 150, \"cluster_size\": 1, \"template_mined\": \"Received completed container container <:NUM:> <:NUM:> <:NUM:> <:NUM:>\", \"cluster_count\": 150}\n",
      "Input (1838): Container complete event for unknown container id container_1445182159119_0016_01_000013\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 151, \"cluster_size\": 1, \"template_mined\": \"Container complete event for unknown container id container <:NUM:> <:NUM:> <:NUM:> <:NUM:>\", \"cluster_count\": 151}\n",
      "Input (1851): Progress of TaskAttempt attempt_1445182159119_0016_m_000000_0 is : 0.10291508\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 114, \"cluster_size\": 164, \"template_mined\": \"Progress of TaskAttempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> is : <:NUM:>.<:NUM:>\", \"cluster_count\": 151}\n",
      "Input (2195): Socket Reader #1 for port 55796: readAndProcess from client 10.86.165.66 threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 152, \"cluster_size\": 1, \"template_mined\": \"Socket Reader #<:NUM:> for port <:NUM:>: readAndProcess from client <:IP:> threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]\", \"cluster_count\": 152}\n",
      "Input (2196): An existing connection was forcibly closed by the remote host\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 153, \"cluster_size\": 1, \"template_mined\": \"An existing connection was forcibly closed by the remote host\", \"cluster_count\": 153}\n",
      "Input (2246): Done acknowledgement from attempt_1445182159119_0016_m_000003_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 123, \"cluster_size\": 2, \"template_mined\": \"Done acknowledgement from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 153}\n",
      "Input (2247): attempt_1445182159119_0016_m_000003_0 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 154, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> TaskAttempt Transitioned from RUNNING to SUCCESS CONTAINER CLEANUP\", \"cluster_count\": 154}\n",
      "Input (2249): KILLING attempt_1445182159119_0016_m_000003_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 125, \"cluster_size\": 2, \"template_mined\": \"KILLING attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 154}\n",
      "Input (2251): attempt_1445182159119_0016_m_000003_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 154, \"cluster_size\": 2, \"template_mined\": \"attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> <:*:> <:*:> <:*:> <:*:>\", \"cluster_count\": 154}\n",
      "Input (2255): DefaultSpeculator.addSpeculativeAttempt -- we are speculating task_1445182159119_0016_m_000004\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 155, \"cluster_size\": 1, \"template_mined\": \"DefaultSpeculator.addSpeculativeAttempt -- we are speculating task <:NUM:> <:NUM:> m <:NUM:>\", \"cluster_count\": 155}\n",
      "Input (2256): We launched 1 speculations.  Sleeping 15000 milliseconds.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 156, \"cluster_size\": 1, \"template_mined\": \"We launched <:NUM:> speculations. Sleeping <:NUM:> milliseconds.\", \"cluster_count\": 156}\n",
      "Input (2257): Scheduling a redundant attempt for task task_1445182159119_0016_m_000004\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 157, \"cluster_size\": 1, \"template_mined\": \"Scheduling a redundant attempt for task task <:NUM:> <:NUM:> m <:NUM:>\", \"cluster_count\": 157}\n",
      "Input (2266): completedMapPercent 0.1 totalResourceLimit:<memory:10240, vCores:-17> finalMapResourceLimit:<memory:9216, vCores:-16> finalReduceResourceLimit:<memory:1024, vCores:-1> netScheduledMapResource:<memory:11264, vCores:11> netScheduledReduceResource:<memory:0, vCores:0>\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 158, \"cluster_size\": 1, \"template_mined\": \"completedMapPercent <:NUM:>.<:NUM:> totalResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> finalMapResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> finalReduceResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> netScheduledMapResource:<memory:<:NUM:>, vCores:<:NUM:>> netScheduledReduceResource:<memory:<:NUM:>, vCores:<:NUM:>>\", \"cluster_count\": 158}\n",
      "Input (2267): Ramping up 1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 159, \"cluster_size\": 1, \"template_mined\": \"Ramping up <:NUM:>\", \"cluster_count\": 159}\n",
      "Input (2284): Diagnostics report from attempt_1445182159119_0016_m_000003_0: Container killed by the ApplicationMaster.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 160, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Container killed by the ApplicationMaster.\", \"cluster_count\": 160}\n",
      "Input (2771): Issuing kill to other attempt attempt_1445182159119_0016_m_000004_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 161, \"cluster_size\": 1, \"template_mined\": \"Issuing kill to other attempt attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 161}\n",
      "Input (2801): attempt_1445182159119_0016_m_000004_0 TaskAttempt Transitioned from KILL_CONTAINER_CLEANUP to KILL_TASK_CLEANUP\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 162, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> TaskAttempt Transitioned from KILL CONTAINER CLEANUP to KILL TASK CLEANUP\", \"cluster_count\": 162}\n",
      "Input (2802): Processing the event EventType: TASK_ABORT\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 82, \"cluster_size\": 4, \"template_mined\": \"Processing the event EventType: <:*:> <:*:>\", \"cluster_count\": 162}\n",
      "Input (2803): Could not delete hdfs://msra-sa-41:9000/pageout/out1/_temporary/1/_temporary/attempt_1445182159119_0016_m_000004_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 163, \"cluster_size\": 1, \"template_mined\": \"Could not delete hdfs://msra-sa-<:NUM:>:<:NUM:>/pageout/out1/ temporary/<:NUM:>/ temporary/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 163}\n",
      "Input (3287): Retrying connect to server: MININT-75DGDAM1.fareast.corp.microsoft.com/10.86.165.66:58081. Already tried 0 time(s); maxRetries=45\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 164, \"cluster_size\": 1, \"template_mined\": \"Retrying connect to server: MININT-75DGDAM1.fareast.corp.microsoft.com/<:IP:>:<:NUM:>. Already tried <:NUM:> time(s); maxRetries=<:NUM:>\", \"cluster_count\": 164}\n",
      "Input (3667): Retrying connect to server: MININT-75DGDAM1.fareast.corp.microsoft.com/10.86.165.66:58081. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 165, \"cluster_size\": 1, \"template_mined\": \"Retrying connect to server: MININT-75DGDAM1.fareast.corp.microsoft.com/<:IP:>:<:NUM:>. Already tried <:NUM:> time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=<:NUM:>, sleepTime=<:NUM:> MILLISECONDS)\", \"cluster_count\": 165}\n",
      "Input (3913): Diagnostics report from attempt_1445182159119_0016_m_000009_0: AttemptID:attempt_1445182159119_0016_m_000009_0 Timed out after 600 secs\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 166, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: AttemptID:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> Timed out after <:NUM:> secs\", \"cluster_count\": 166}\n",
      "Input (3993): Killing taskAttempt:attempt_1445182159119_0016_m_000005_0 because it is running on unusable node:MININT-75DGDAM1.fareast.corp.microsoft.com:58081\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 167, \"cluster_size\": 1, \"template_mined\": \"Killing taskAttempt:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> because it is running on unusable node:MININT-75DGDAM1.fareast.corp.microsoft.com:<:NUM:>\", \"cluster_count\": 167}\n",
      "Input (4000): Diagnostics report from attempt_1445182159119_0016_m_000009_0: Container released on a *lost* node\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 168, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Container released on a *lost* node\", \"cluster_count\": 168}\n",
      "Input (5523): Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"04DN8IQ/10.86.164.9\"; destination host is: \"minint-fnanli5.fareast.corp.microsoft.com\":52839;\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 169, \"cluster_size\": 1, \"template_mined\": \"Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \\\"04DN8IQ/<:IP:>\\\"; destination host is: \\\"minint-fnanli5.fareast.corp.microsoft.com\\\":<:NUM:>;\", \"cluster_count\": 169}\n",
      "Input (5531): java.io.IOException: An existing connection was forcibly closed by the remote host\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 170, \"cluster_size\": 1, \"template_mined\": \"java.io.IOException: An existing connection was forcibly closed by the remote host\", \"cluster_count\": 170}\n",
      "Input (5552): Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 0 time(s); maxRetries=45\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 164, \"cluster_size\": 12, \"template_mined\": \"Retrying connect to server: <:*:> Already tried <:NUM:> time(s); maxRetries=<:NUM:>\", \"cluster_count\": 170}\n",
      "Input (6134): ReduceTask metrics system started\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 171, \"cluster_size\": 1, \"template_mined\": \"ReduceTask metrics system started\", \"cluster_count\": 171}\n",
      "Input (6142): Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@64bfc93b\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 172, \"cluster_size\": 1, \"template_mined\": \"Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@64bfc93b\", \"cluster_count\": 172}\n",
      "Input (6143): MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 173, \"cluster_size\": 1, \"template_mined\": \"MergerManager: memoryLimit=<:NUM:>, maxSingleShuffleLimit=<:NUM:>, mergeThreshold=<:NUM:>, ioSortFactor=<:NUM:>, memToMemMergeOutputsThreshold=<:NUM:>\", \"cluster_count\": 173}\n",
      "Input (6145): attempt_1445094324383_0005_r_000000_0: Got 6 new map-outputs\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 174, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>: Got <:NUM:> new map-outputs\", \"cluster_count\": 174}\n",
      "Input (6146): Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 6 to fetcher#4\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 175, \"cluster_size\": 1, \"template_mined\": \"Assigning MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:> with <:NUM:> to fetcher#<:NUM:>\", \"cluster_count\": 175}\n",
      "Input (6147): assigned 6 of 6 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#4\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 176, \"cluster_size\": 1, \"template_mined\": \"assigned <:NUM:> of <:NUM:> to MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:> to fetcher#<:NUM:>\", \"cluster_count\": 176}\n",
      "Input (6148): for url=13562/mapOutput?job=job_1445094324383_0005&reduce=0&map=attempt_1445094324383_0005_m_000000_0,attempt_1445094324383_0005_m_000005_0,attempt_1445094324383_0005_m_000003_0,attempt_1445094324383_0005_m_000001_0,attempt_1445094324383_0005_m_000004_0,attempt_1445094324383_0005_m_000002_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 177, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 177}\n",
      "Input (6149): attempt_1445094324383_0005_m_000000_0: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (32663142)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 178, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Shuffling to disk since <:NUM:> is greater than maxSingleShuffleLimit (<:NUM:>)\", \"cluster_count\": 178}\n",
      "Input (6150): fetcher#4 about to shuffle output of map attempt_1445094324383_0005_m_000000_0 decomp: 216988123 len: 216988127 to DISK\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 179, \"cluster_size\": 1, \"template_mined\": \"fetcher#<:NUM:> about to shuffle output of map attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> decomp: <:NUM:> len: <:NUM:> to DISK\", \"cluster_count\": 179}\n",
      "Input (6151): Ignoring obsolete output of KILLED map-task: 'attempt_1445094324383_0005_m_000006_1'\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 180, \"cluster_size\": 1, \"template_mined\": \"Ignoring obsolete output of KILLED map-task: 'attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>'\", \"cluster_count\": 180}\n",
      "Input (6424): Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@49c051a1\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 172, \"cluster_size\": 2, \"template_mined\": \"Using ShuffleConsumerPlugin: <:*:>\", \"cluster_count\": 180}\n",
      "Input (6430): for url=13562/mapOutput?job=job_1445094324383_0004&reduce=0&map=attempt_1445094324383_0004_m_000001_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 181, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 181}\n",
      "Input (6534): Instantiated MRClientService at MININT-FNANLI5.fareast.corp.microsoft.com/10.86.169.121:61543\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 61, \"cluster_size\": 3, \"template_mined\": \"Instantiated MRClientService at <:*:>\", \"cluster_count\": 181}\n",
      "Input (6546): Extract jar:file:/D:/hadoop-2.6.0-localbox/share/hadoop/yarn/hadoop-yarn-common-2.6.0-SNAPSHOT.jar!/webapps/mapreduce to C:\\Users\\msrabi\\AppData\\Local\\Temp\\Jetty_0_0_0_0_61550_mapreduce____.mrek2k\\webapp\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 69, \"cluster_size\": 3, \"template_mined\": \"Extract jar:file:/D:/hadoop-<:NUM:>.<:NUM:>.<:NUM:>-localbox/share/hadoop/yarn/hadoop-yarn-common-<:NUM:>.<:NUM:>.<:NUM:>-SNAPSHOT.jar!/webapps/mapreduce to <:*:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> mapreduce <:*:>\", \"cluster_count\": 181}\n",
      "Input (7469): attempt_1445182159119_0001_r_000000_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 154, \"cluster_size\": 48, \"template_mined\": \"attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> <:*:> <:*:> <:*:> <:*:>\", \"cluster_count\": 181}\n",
      "Input (7478): Diagnostics report from attempt_1445182159119_0001_r_000000_0: Container killed by the ApplicationMaster.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 160, \"cluster_size\": 23, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: Container killed by the ApplicationMaster.\", \"cluster_count\": 181}\n",
      "Input (7508): History url is http://MININT-FNANLI5.fareast.corp.microsoft.com:19888/jobhistory/job/job_1445182159119_0001\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 139, \"cluster_size\": 2, \"template_mined\": \"History url is <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 181}\n",
      "Input (8504): Could not delete hdfs://msra-sa-41:9000/pageout/out5/_temporary/1/_temporary/attempt_1445062781478_0020_m_000001_1\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 163, \"cluster_size\": 4, \"template_mined\": \"Could not delete <:*:> temporary/<:NUM:>/ temporary/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 181}\n",
      "Input (9881): Diagnostics report from attempt_1445076437777_0002_m_000006_0:\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 182, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>:\", \"cluster_count\": 182}\n",
      "Processing line: 10000, rate 7591.4 lines/sec, 182 clusters so far.\n",
      "Input (10972): Slow ReadProcessor read fields took 54719ms (threshold=30000ms); ack: seqno: -2 status: SUCCESS status: ERROR downstreamAckTimeNanos: 0, targets: [10.86.164.15:50010, 10.86.169.121:50010]\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 183, \"cluster_size\": 1, \"template_mined\": \"Slow ReadProcessor read fields took 54719ms (threshold=30000ms); ack: seqno: <:NUM:> status: SUCCESS status: ERROR downstreamAckTimeNanos: <:NUM:>, targets: [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>]\", \"cluster_count\": 183}\n",
      "Input (10977): In stop, writing event TASK_FINISHED\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 184, \"cluster_size\": 1, \"template_mined\": \"In stop, writing event TASK FINISHED\", \"cluster_count\": 184}\n",
      "Input (10978): In stop, writing event JOB_FINISHED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 184, \"cluster_size\": 2, \"template_mined\": \"In stop, writing event <:*:> FINISHED\", \"cluster_count\": 184}\n",
      "Input (14012): Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MININT-75DGDAM1/10.86.165.66\"; destination host is: \"minint-fnanli5.fareast.corp.microsoft.com\":52839;\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 169, \"cluster_size\": 2, \"template_mined\": \"Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: \\\"minint-fnanli5.fareast.corp.microsoft.com\\\":<:NUM:>;\", \"cluster_count\": 184}\n",
      "Input (17015): Read 172334808 bytes from map-output for attempt_1445094324383_0001_m_000009_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 185, \"cluster_size\": 1, \"template_mined\": \"Read <:NUM:> bytes from map-output for attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 185}\n",
      "Input (17016): MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 5943ms\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 186, \"cluster_size\": 1, \"template_mined\": \"MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:> freed by fetcher#<:NUM:> in 5943ms\", \"cluster_count\": 186}\n",
      "Input (17019): for url=13562/mapOutput?job=job_1445094324383_0001&reduce=0&map=attempt_1445094324383_0001_m_000001_0,attempt_1445094324383_0001_m_000008_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 187, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 187}\n",
      "Input (17023): MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 8239ms\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 186, \"cluster_size\": 2, \"template_mined\": \"MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:> freed by fetcher#<:NUM:> in <:*:>\", \"cluster_count\": 187}\n",
      "Input (17026): for url=13562/mapOutput?job=job_1445094324383_0001&reduce=0&map=attempt_1445094324383_0001_m_000003_0,attempt_1445094324383_0001_m_000006_0,attempt_1445094324383_0001_m_000005_0,attempt_1445094324383_0001_m_000004_0,attempt_1445094324383_0001_m_000007_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 188, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 188}\n",
      "Input (17055): EventFetcher is interrupted.. Returning\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 189, \"cluster_size\": 1, \"template_mined\": \"EventFetcher is interrupted.. Returning\", \"cluster_count\": 189}\n",
      "Input (17057): finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 190, \"cluster_size\": 1, \"template_mined\": \"finalMerge called with <:NUM:> in-memory map-outputs and <:NUM:> on-disk map-outputs\", \"cluster_count\": 190}\n",
      "Input (17058): Merging 10 files, 2125289789 bytes from disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 191, \"cluster_size\": 1, \"template_mined\": \"Merging <:NUM:> files, <:NUM:> bytes from disk\", \"cluster_count\": 191}\n",
      "Input (17059): Merging 0 segments, 0 bytes from memory into reduce\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 192, \"cluster_size\": 1, \"template_mined\": \"Merging <:NUM:> segments, <:NUM:> bytes from memory into reduce\", \"cluster_count\": 192}\n",
      "Input (17062): mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 193, \"cluster_size\": 1, \"template_mined\": \"mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\", \"cluster_count\": 193}\n",
      "Input (17063): Task:attempt_1445094324383_0001_r_000000_0 is done. And is in the process of committing\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 26, \"cluster_size\": 36, \"template_mined\": \"Task:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> is done. And is in the process of committing\", \"cluster_count\": 193}\n",
      "Input (17064): Task attempt_1445094324383_0001_r_000000_0 is allowed to commit now\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 194, \"cluster_size\": 1, \"template_mined\": \"Task attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> is allowed to commit now\", \"cluster_count\": 194}\n",
      "Input (17065): Saved output of task 'attempt_1445094324383_0001_r_000000_0' to hdfs://msra-sa-41:9000/out/out4/_temporary/1/task_1445094324383_0001_r_000000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 195, \"cluster_size\": 1, \"template_mined\": \"Saved output of task 'attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>' to hdfs://msra-sa-<:NUM:>:<:NUM:>/out/out4/ temporary/<:NUM:>/task <:NUM:> <:NUM:> r <:NUM:>\", \"cluster_count\": 195}\n",
      "Input (17066): Task 'attempt_1445094324383_0001_r_000000_0' done.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 27, \"cluster_size\": 36, \"template_mined\": \"Task 'attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>' done.\", \"cluster_count\": 195}\n",
      "Input (18036): for url=13562/mapOutput?job=job_1445087491445_0007&reduce=0&map=attempt_1445087491445_0007_m_000006_0,attempt_1445087491445_0007_m_000007_0,attempt_1445087491445_0007_m_000008_0,attempt_1445087491445_0007_m_000009_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 196, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 196}\n",
      "Input (18075): Exception in createBlockOutputStream\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 197, \"cluster_size\": 1, \"template_mined\": \"Exception in createBlockOutputStream\", \"cluster_count\": 197}\n",
      "Input (18076): Bad connect ack with firstBadLink as 10.86.169.121:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 198, \"cluster_size\": 1, \"template_mined\": \"Bad connect ack with firstBadLink as <:IP:>:<:NUM:>\", \"cluster_count\": 198}\n",
      "Input (18080): Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073743056_2272\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 199, \"cluster_size\": 1, \"template_mined\": \"Abandoning BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:>\", \"cluster_count\": 199}\n",
      "Input (18081): Excluding datanode 10.86.169.121:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 200, \"cluster_size\": 1, \"template_mined\": \"Excluding datanode <:IP:>:<:NUM:>\", \"cluster_count\": 200}\n",
      "Input (18084): Saved output of task 'attempt_1445087491445_0007_r_000000_1000' to hdfs://msra-sa-41:9000/out/out1/_temporary/2/task_1445087491445_0007_r_000000\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 195, \"cluster_size\": 2, \"template_mined\": \"Saved output of task 'attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>' to <:*:> temporary/<:NUM:>/task <:NUM:> <:NUM:> r <:NUM:>\", \"cluster_count\": 200}\n",
      "Processing line: 20000, rate 11991.7 lines/sec, 200 clusters so far.\n",
      "Input (21068): Killing taskAttempt:attempt_1445094324383_0002_r_000000_0 because it is running on unusable node:04DN8IQ.fareast.corp.microsoft.com:55452\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 167, \"cluster_size\": 4, \"template_mined\": \"Killing taskAttempt:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> because it is running on unusable <:*:>\", \"cluster_count\": 200}\n",
      "Input (21072): Diagnostics report from attempt_1445094324383_0002_r_000000_0: Container released on a *lost* node\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 168, \"cluster_size\": 4, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: Container released on a *lost* node\", \"cluster_count\": 200}\n",
      "Input (21076): attempt_1445094324383_0002_r_000000_0 TaskAttempt Transitioned from KILL_CONTAINER_CLEANUP to KILL_TASK_CLEANUP\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 162, \"cluster_size\": 24, \"template_mined\": \"attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from KILL CONTAINER CLEANUP to KILL TASK CLEANUP\", \"cluster_count\": 200}\n",
      "Input (21731): Could not delete hdfs://msra-sa-41:9000/out/out1/_temporary/1/_temporary/attempt_1445094324383_0002_r_000000_1\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 163, \"cluster_size\": 24, \"template_mined\": \"Could not delete <:*:> temporary/<:NUM:>/ temporary/attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 200}\n",
      "Input (21986): Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#4\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 175, \"cluster_size\": 14, \"template_mined\": \"Assigning <:*:> with <:NUM:> to fetcher#<:NUM:>\", \"cluster_count\": 200}\n",
      "Input (21988): assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#4\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 176, \"cluster_size\": 14, \"template_mined\": \"assigned <:NUM:> of <:NUM:> to <:*:> to fetcher#<:NUM:>\", \"cluster_count\": 200}\n",
      "Input (21999): 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 857ms\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 201, \"cluster_size\": 1, \"template_mined\": \"04DN8IQ.fareast.corp.microsoft.com:<:NUM:> freed by fetcher#<:NUM:> in 857ms\", \"cluster_count\": 201}\n",
      "Input (22037): 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 603ms\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 201, \"cluster_size\": 2, \"template_mined\": \"04DN8IQ.fareast.corp.microsoft.com:<:NUM:> freed by fetcher#<:NUM:> in <:*:>\", \"cluster_count\": 201}\n",
      "Input (24095): Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MININT-FNANLI5/127.0.0.1\"; destination host is: \"10.190.173.170\":29630;\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 169, \"cluster_size\": 4, \"template_mined\": \"Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\", \"cluster_count\": 201}\n",
      "Input (24122): Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 165, \"cluster_size\": 151, \"template_mined\": \"Retrying connect to server: <:*:> Already tried <:NUM:> time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=<:NUM:>, sleepTime=<:NUM:> MILLISECONDS)\", \"cluster_count\": 201}\n",
      "Input (24132): Communication exception: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to 10.190.173.170:29630 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 202, \"cluster_size\": 1, \"template_mined\": \"Communication exception: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to <:IP:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 202}\n",
      "Input (24145): java.net.NoRouteToHostException: No route to host: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 203, \"cluster_size\": 1, \"template_mined\": \"java.net.NoRouteToHostException: No route to host: no further information\", \"cluster_count\": 203}\n",
      "Input (24194): Process Thread Dump: Communication exception\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 204, \"cluster_size\": 1, \"template_mined\": \"Process Thread Dump: Communication exception\", \"cluster_count\": 204}\n",
      "Input (24197): WAITING\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 205, \"cluster_size\": 1, \"template_mined\": \"WAITING\", \"cluster_count\": 205}\n",
      "Input (24198): 0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 206, \"cluster_size\": 1, \"template_mined\": \"<:NUM:>\", \"cluster_count\": 206}\n",
      "Input (24207): TIMED_WAITING\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 207, \"cluster_size\": 1, \"template_mined\": \"TIMED WAITING\", \"cluster_count\": 207}\n",
      "Input (24217): RUNNABLE\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 208, \"cluster_size\": 1, \"template_mined\": \"RUNNABLE\", \"cluster_count\": 208}\n",
      "Input (24326): Last retry, killing attempt_1445144423722_0022_m_000004_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 209, \"cluster_size\": 1, \"template_mined\": \"Last retry, killing attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 209}\n",
      "Input (24420): for url=13562/mapOutput?job=job_1445144423722_0022&reduce=0&map=attempt_1445144423722_0022_m_000002_0,attempt_1445144423722_0022_m_000000_0,attempt_1445144423722_0022_m_000001_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 210, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 210}\n",
      "Input (25583): Runnning cleanup for the task\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 211, \"cluster_size\": 1, \"template_mined\": \"Runnning cleanup for the task\", \"cluster_count\": 211}\n",
      "Input (28556): Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 212, \"cluster_size\": 1, \"template_mined\": \"Address change detected. Old: msra-sa-<:NUM:>/<:IP:>:<:NUM:> New: msra-sa-<:NUM:>:<:NUM:>\", \"cluster_count\": 212}\n",
      "Input (28557): Failed to renew lease for [DFSClient_NONMAPREDUCE_483047941_1] for 46 seconds.  Will retry shortly ...\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 213, \"cluster_size\": 1, \"template_mined\": \"Failed to renew lease for [DFSClient NONMAPREDUCE <:NUM:> <:NUM:>] for <:NUM:> seconds. Will retry shortly ...\", \"cluster_count\": 213}\n",
      "Input (28558): No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 214, \"cluster_size\": 1, \"template_mined\": \"No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 214}\n",
      "Input (29238): ERROR IN CONTACTING RM.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 215, \"cluster_size\": 1, \"template_mined\": \"ERROR IN CONTACTING RM.\", \"cluster_count\": 215}\n",
      "Input (29239): Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MININT-FNANLI5/127.0.0.1\"; destination host is: \"msra-sa-41\":8030;\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 216, \"cluster_size\": 1, \"template_mined\": \"Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \\\"MININT-FNANLI5/<:IP:>\\\"; destination host is: \\\"msra-sa-<:NUM:>\\\":<:NUM:>;\", \"cluster_count\": 216}\n",
      "Input (29352): Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \"MININT-FNANLI5/127.0.0.1\"; destination host is: \"msra-sa-41\":8030;\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 217, \"cluster_size\": 1, \"template_mined\": \"Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \\\"MININT-FNANLI5/<:IP:>\\\"; destination host is: \\\"msra-sa-<:NUM:>\\\":<:NUM:>;\", \"cluster_count\": 217}\n",
      "Input (29370): java.io.IOException: Couldn't set up IO streams\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 218, \"cluster_size\": 1, \"template_mined\": \"java.io.IOException: Couldn't set up IO streams\", \"cluster_count\": 218}\n",
      "Input (29376): java.nio.channels.UnresolvedAddressException\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 219, \"cluster_size\": 1, \"template_mined\": \"java.nio.channels.UnresolvedAddressException\", \"cluster_count\": 219}\n",
      "Processing line: 30000, rate 15856.5 lines/sec, 219 clusters so far.\n",
      "Input (30801): Ignoring obsolete output of FAILED map-task: 'attempt_1445182159119_0002_m_000007_0'\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 180, \"cluster_size\": 3, \"template_mined\": \"Ignoring obsolete output of <:*:> map-task: 'attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>'\", \"cluster_count\": 219}\n",
      "Input (30847): Exception in getting events\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 220, \"cluster_size\": 1, \"template_mined\": \"Exception in getting events\", \"cluster_count\": 220}\n",
      "Input (30848): Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MSRA-SA-41/10.190.173.170\"; destination host is: \"minint-fnanli5.fareast.corp.microsoft.com\":52839;\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 216, \"cluster_size\": 2, \"template_mined\": \"Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\", \"cluster_count\": 220}\n",
      "Input (31728): MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 36889ms\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 201, \"cluster_size\": 10, \"template_mined\": \"<:*:> freed by fetcher#<:NUM:> in <:*:>\", \"cluster_count\": 220}\n",
      "Input (37555): TaskAttempt killed because it ran on unusable node 04DN8IQ.fareast.corp.microsoft.com:55452. AttemptId:attempt_1445087491445_0004_m_000002_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 221, \"cluster_size\": 1, \"template_mined\": \"TaskAttempt killed because it ran on unusable node 04DN8IQ.fareast.corp.microsoft.com:<:NUM:>. AttemptId:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 221}\n",
      "Input (38182): TaskAttempt killed because it ran on unusable node MININT-FNANLI5.fareast.corp.microsoft.com:55629. AttemptId:attempt_1445087491445_0004_m_000005_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 221, \"cluster_size\": 7, \"template_mined\": \"TaskAttempt killed because it ran on unusable node <:*:> AttemptId:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 221}\n",
      "Input (38193): Ramping down all scheduled reduces:0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 222, \"cluster_size\": 1, \"template_mined\": \"Ramping down all scheduled reduces:<:NUM:>\", \"cluster_count\": 222}\n",
      "Input (38194): Going to preempt 1 due to lack of space for maps\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 223, \"cluster_size\": 1, \"template_mined\": \"Going to preempt <:NUM:> due to lack of space for maps\", \"cluster_count\": 223}\n",
      "Input (38195): Preempting attempt_1445087491445_0004_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 224, \"cluster_size\": 1, \"template_mined\": \"Preempting attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 224}\n",
      "Input (38222): Reduce preemption successful attempt_1445087491445_0004_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 225, \"cluster_size\": 1, \"template_mined\": \"Reduce preemption successful attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 225}\n",
      "Processing line: 40000, rate 16942.7 lines/sec, 225 clusters so far.\n",
      "Input (40973): Merging 4 intermediate segments out of a total of 13\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 226, \"cluster_size\": 1, \"template_mined\": \"Merging <:NUM:> intermediate segments out of a total of <:NUM:>\", \"cluster_count\": 226}\n",
      "Input (41035): Communication exception: java.net.ConnectException: Call From MSRA-SA-39/172.22.149.145 to minint-fnanli5.fareast.corp.microsoft.com:49594 failed on connection exception: java.net.ConnectException: Connection timed out: no further information; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 227, \"cluster_size\": 1, \"template_mined\": \"Communication exception: java.net.ConnectException: Call From MSRA-SA-<:NUM:>/<:IP:> to minint-fnanli5.fareast.corp.microsoft.com:<:NUM:> failed on connection exception: java.net.ConnectException: Connection timed out: no further information; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused\", \"cluster_count\": 227}\n",
      "Input (41048): java.net.ConnectException: Connection timed out: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 228, \"cluster_size\": 1, \"template_mined\": \"java.net.ConnectException: Connection timed out: no further information\", \"cluster_count\": 228}\n",
      "Input (41210): Stopping ReduceTask metrics system...\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 35, \"cluster_size\": 56, \"template_mined\": \"Stopping <:*:> metrics system...\", \"cluster_count\": 228}\n",
      "Input (41211): ReduceTask metrics system stopped.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 171, \"cluster_size\": 18, \"template_mined\": \"ReduceTask metrics system <:*:>\", \"cluster_count\": 228}\n",
      "Input (41212): ReduceTask metrics system shutdown complete.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 229, \"cluster_size\": 1, \"template_mined\": \"ReduceTask metrics system shutdown complete.\", \"cluster_count\": 229}\n",
      "Input (49153): Slow ReadProcessor read fields took 48944ms (threshold=30000ms); ack: seqno: -2 status: SUCCESS status: ERROR downstreamAckTimeNanos: 0, targets: [172.22.149.145:50010, 10.86.169.121:50010]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 183, \"cluster_size\": 2, \"template_mined\": \"Slow ReadProcessor read fields took <:*:> (threshold=30000ms); ack: seqno: <:NUM:> status: SUCCESS status: ERROR downstreamAckTimeNanos: <:NUM:>, targets: [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>]\", \"cluster_count\": 229}\n",
      "Processing line: 50000, rate 19349.8 lines/sec, 229 clusters so far.\n",
      "Input (51306): Releasing unassigned and invalid container Container: [ContainerId: container_1445076437777_0005_01_000011, NodeId: MININT-75DGDAM1.fareast.corp.microsoft.com:53425, NodeHttpAddress: MININT-75DGDAM1.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.86.165.66:53425 }, ]. RM may have assignment issues\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 230, \"cluster_size\": 1, \"template_mined\": \"Releasing unassigned and invalid container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: MININT-75DGDAM1.fareast.corp.microsoft.com:<:NUM:>, NodeHttpAddress: MININT-75DGDAM1.fareast.corp.microsoft.com:<:NUM:>, Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ]. RM may have assignment issues\", \"cluster_count\": 230}\n",
      "Input (53107): Connection retry failed with 4 attempts in 180 seconds\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 231, \"cluster_size\": 1, \"template_mined\": \"Connection retry failed with <:NUM:> attempts in <:NUM:> seconds\", \"cluster_count\": 231}\n",
      "Input (53108): Failed to connect to MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 map outputs\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 232, \"cluster_size\": 1, \"template_mined\": \"Failed to connect to MININT-FNANLI5.fareast.corp.microsoft.com:<:NUM:> with <:NUM:> map outputs\", \"cluster_count\": 232}\n",
      "Input (53109): Connection timed out: connect\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 233, \"cluster_size\": 1, \"template_mined\": \"Connection timed out: connect\", \"cluster_count\": 233}\n",
      "Input (53130): Reporting fetch failure for attempt_1445087491445_0004_m_000005_0 to jobtracker.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 234, \"cluster_size\": 1, \"template_mined\": \"Reporting fetch failure for attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> to jobtracker.\", \"cluster_count\": 234}\n",
      "Input (55432): Task: attempt_1445182159119_0002_m_000007_0 - exited : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt_1445182159119_0002_m_000007_0/file.out\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 235, \"cluster_size\": 1, \"template_mined\": \"Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\", \"cluster_count\": 235}\n",
      "Input (55448): Diagnostics report from attempt_1445182159119_0002_m_000007_0: Error: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt_1445182159119_0002_m_000007_0/file.out\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 236, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\", \"cluster_count\": 236}\n",
      "Input (55484): attempt_1445182159119_0002_m_000007_0 TaskAttempt Transitioned from FAIL_CONTAINER_CLEANUP to FAIL_TASK_CLEANUP\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 162, \"cluster_size\": 72, \"template_mined\": \"attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> CONTAINER CLEANUP to <:*:> TASK CLEANUP\", \"cluster_count\": 236}\n",
      "Input (55488): 1 failures on node 04DN8IQ.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 237, \"cluster_size\": 1, \"template_mined\": \"<:NUM:> failures on node 04DN8IQ.fareast.corp.microsoft.com\", \"cluster_count\": 237}\n",
      "Input (57955): Task: attempt_1445182159119_0015_m_000005_0 - exited : java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 238, \"cluster_size\": 1, \"template_mined\": \"Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 238}\n",
      "Input (57968): Diagnostics report from attempt_1445182159119_0015_m_000005_0: Error: java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 239, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 239}\n",
      "Input (58126): Task: attempt_1445182159119_0015_m_000005_0 - exited : java.io.IOException: Spill failed\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 240, \"cluster_size\": 1, \"template_mined\": \"Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.io.IOException: Spill failed\", \"cluster_count\": 240}\n",
      "Input (58148): org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 241, \"cluster_size\": 1, \"template_mined\": \"org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 241}\n",
      "Input (58158): java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 242, \"cluster_size\": 1, \"template_mined\": \"java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 242}\n",
      "Input (58164): Diagnostics report from attempt_1445182159119_0015_m_000005_0: Error: java.io.IOException: Spill failed\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 243, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: java.io.IOException: Spill failed\", \"cluster_count\": 243}\n",
      "Input (58371): Added attempt_1445182159119_0015_m_000006_1 to list of failed maps\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 244, \"cluster_size\": 1, \"template_mined\": \"Added attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> to list of failed maps\", \"cluster_count\": 244}\n",
      "Input (58391): Assigning container Container: [ContainerId: container_1445182159119_0015_01_000012, NodeId: MSRA-SA-41.fareast.corp.microsoft.com:10769, NodeHttpAddress: MSRA-SA-41.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 5, Token: Token { kind: ContainerToken, service: 10.190.173.170:10769 }, ] to fast fail map\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 245, \"cluster_size\": 1, \"template_mined\": \"Assigning container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>, NodeHttpAddress: MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>, Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] to fast fail map\", \"cluster_count\": 245}\n",
      "Input (58392): Assigned from earlierFailedMaps\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 246, \"cluster_size\": 1, \"template_mined\": \"Assigned from earlierFailedMaps\", \"cluster_count\": 246}\n",
      "Processing line: 60000, rate 15459.5 lines/sec, 246 clusters so far.\n",
      "Input (61066): Unable to parse prior job history, aborting recovery\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 247, \"cluster_size\": 1, \"template_mined\": \"Unable to parse prior job history, aborting recovery\", \"cluster_count\": 247}\n",
      "Input (61067): Incompatible event log version: null\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 248, \"cluster_size\": 1, \"template_mined\": \"Incompatible event log version: null\", \"cluster_count\": 248}\n",
      "Input (61082): Could not parse the old history file. Will not have old AMinfos\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 249, \"cluster_size\": 1, \"template_mined\": \"Could not parse the old history file. Will not have old AMinfos\", \"cluster_count\": 249}\n",
      "Processing line: 70000, rate 16194.5 lines/sec, 249 clusters so far.\n",
      "Input (71736): Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MININT-75DGDAM1/10.86.165.66\"; destination host is: \"04dn8iq.fareast.corp.microsoft.com\":49470;\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 250, \"cluster_size\": 1, \"template_mined\": \"Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \\\"MININT-75DGDAM1/<:IP:>\\\"; destination host is: \\\"04dn8iq.fareast.corp.microsoft.com\\\":<:NUM:>;\", \"cluster_count\": 250}\n",
      "Processing line: 80000, rate 22441.5 lines/sec, 250 clusters so far.\n",
      "Input (81569): No route to host: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 251, \"cluster_size\": 1, \"template_mined\": \"No route to host: no further information\", \"cluster_count\": 251}\n",
      "Input (81597): Failed to connect to /172.22.149.145:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 252, \"cluster_size\": 1, \"template_mined\": \"Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information\", \"cluster_count\": 252}\n",
      "Input (81684): Could not obtain BP-1347369012-10.190.173.170-1444972147527:blk_1073742989_2201 from any node: java.io.IOException: No live nodes contain block BP-1347369012-10.190.173.170-1444972147527:blk_1073742989_2201 after checking nodes = [172.22.149.145:50010, 10.190.173.170:50010], ignoredNodes = null No live nodes contain current block Block locations: 172.22.149.145:50010 10.190.173.170:50010 Dead nodes:  10.190.173.170:50010 172.22.149.145:50010. Will get new block locations from namenode and retry...\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 253, \"cluster_size\": 1, \"template_mined\": \"Could not obtain BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> from any node: java.io.IOException: No live nodes contain block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> after checking nodes = [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>], ignoredNodes = null No live nodes contain current block Block locations: <:IP:>:<:NUM:> <:IP:>:<:NUM:> Dead nodes: <:IP:>:<:NUM:> <:IP:>:<:NUM:>. Will get new block locations from namenode and retry...\", \"cluster_count\": 253}\n",
      "Input (81685): DFS chooseDataNode: got # 1 IOException, will wait for 1857.517062515084 msec.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 254, \"cluster_size\": 1, \"template_mined\": \"DFS chooseDataNode: got # <:NUM:> IOException, will wait for <:NUM:>.<:NUM:> msec.\", \"cluster_count\": 254}\n",
      "Input (81803): Could not obtain BP-1347369012-10.190.173.170-1444972147527:blk_1073742989_2201 from any node: java.io.IOException: No live nodes contain block BP-1347369012-10.190.173.170-1444972147527:blk_1073742989_2201 after checking nodes = [172.22.149.145:50010, 10.190.173.170:50010], ignoredNodes = null No live nodes contain current block Block locations: 172.22.149.145:50010 10.190.173.170:50010 Dead nodes:  10.190.173.170:50010 172.22.149.145:50010 10.86.169.121:50010. Will get new block locations from namenode and retry...\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 255, \"cluster_size\": 1, \"template_mined\": \"Could not obtain BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> from any node: java.io.IOException: No live nodes contain block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> after checking nodes = [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>], ignoredNodes = null No live nodes contain current block Block locations: <:IP:>:<:NUM:> <:IP:>:<:NUM:> Dead nodes: <:IP:>:<:NUM:> <:IP:>:<:NUM:> <:IP:>:<:NUM:>. Will get new block locations from namenode and retry...\", \"cluster_count\": 255}\n",
      "Input (81806): DFS Read\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 256, \"cluster_size\": 1, \"template_mined\": \"DFS Read\", \"cluster_count\": 256}\n",
      "Input (83813): Releasing unassigned and invalid container Container: [ContainerId: container_1445062781478_0013_01_000012, NodeId: MININT-FNANLI5.fareast.corp.microsoft.com:64642, NodeHttpAddress: MININT-FNANLI5.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.86.169.121:64642 }, ]. RM may have assignment issues\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 230, \"cluster_size\": 2, \"template_mined\": \"Releasing unassigned and invalid container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ]. RM may have assignment issues\", \"cluster_count\": 256}\n",
      "Processing line: 90000, rate 17285.5 lines/sec, 256 clusters so far.\n",
      "Input (93956): DataStreamer Exception\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 257, \"cluster_size\": 1, \"template_mined\": \"DataStreamer Exception\", \"cluster_count\": 257}\n",
      "Processing line: 100000, rate 28163.4 lines/sec, 257 clusters so far.\n",
      "Processing line: 110000, rate 45357.7 lines/sec, 257 clusters so far.\n",
      "Input (112621): Could not contact RM after 360000 milliseconds.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 258, \"cluster_size\": 1, \"template_mined\": \"Could not contact RM after <:NUM:> milliseconds.\", \"cluster_count\": 258}\n",
      "Input (112622): Error communicating with RM: Could not contact RM after 360000 milliseconds.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 259, \"cluster_size\": 1, \"template_mined\": \"Error communicating with RM: Could not contact RM after <:NUM:> milliseconds.\", \"cluster_count\": 259}\n",
      "Input (112629): Error writing History Event: org.apache.hadoop.mapreduce.jobhistory.JobUnsuccessfulCompletionEvent@13bb724a\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 260, \"cluster_size\": 1, \"template_mined\": \"Error writing History Event: org.apache.hadoop.mapreduce.jobhistory.JobUnsuccessfulCompletionEvent@13bb724a\", \"cluster_count\": 260}\n",
      "Input (112663): Thread Thread[eventHandlingThread,5,main] threw an Exception.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 261, \"cluster_size\": 1, \"template_mined\": \"Thread Thread[eventHandlingThread,<:NUM:>,main] threw an Exception.\", \"cluster_count\": 261}\n",
      "Input (112664): java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 262, \"cluster_size\": 1, \"template_mined\": \"java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 262}\n",
      "Input (112708): Found jobId job_1445175094696_0003 to have not been closed. Will close\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 263, \"cluster_size\": 1, \"template_mined\": \"Found jobId job <:NUM:> <:NUM:> to have not been closed. Will close\", \"cluster_count\": 263}\n",
      "Input (112709): Error writing History Event: org.apache.hadoop.mapreduce.jobhistory.JobUnsuccessfulCompletionEvent@2890300b\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 260, \"cluster_size\": 2, \"template_mined\": \"Error writing History Event: <:*:>\", \"cluster_count\": 263}\n",
      "Input (112743): Service JobHistoryEventHandler failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 264, \"cluster_size\": 1, \"template_mined\": \"Service JobHistoryEventHandler failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 264}\n",
      "Input (112790): When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 265, \"cluster_size\": 1, \"template_mined\": \"When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 265}\n",
      "Input (112839): cleanup failed for container container_1445175094696_0003_01_000012 : java.lang.IllegalArgumentException: java.net.UnknownHostException: MSRA-SA-39.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 266, \"cluster_size\": 1, \"template_mined\": \"cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: MSRA-SA-<:NUM:>.fareast.corp.microsoft.com\", \"cluster_count\": 266}\n",
      "Input (112863): java.net.UnknownHostException: MSRA-SA-39.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 267, \"cluster_size\": 1, \"template_mined\": \"java.net.UnknownHostException: MSRA-SA-<:NUM:>.fareast.corp.microsoft.com\", \"cluster_count\": 267}\n",
      "Input (112866): Diagnostics report from attempt_1445175094696_0003_r_000000_0: cleanup failed for container container_1445175094696_0003_01_000012 : java.lang.IllegalArgumentException: java.net.UnknownHostException: MSRA-SA-39.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 268, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>: cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: MSRA-SA-<:NUM:>.fareast.corp.microsoft.com\", \"cluster_count\": 268}\n",
      "Input (112936): Exception while unregistering\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 269, \"cluster_size\": 1, \"template_mined\": \"Exception while unregistering\", \"cluster_count\": 269}\n",
      "Input (112984): Skipping cleaning up the staging dir. assuming AM will be retried.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 270, \"cluster_size\": 1, \"template_mined\": \"Skipping cleaning up the staging dir. assuming AM will be retried.\", \"cluster_count\": 270}\n",
      "Input (112989): Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 264, \"cluster_size\": 2, \"template_mined\": \"Service <:*:> failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 270}\n",
      "Input (113036): Graceful stop failed\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 271, \"cluster_size\": 1, \"template_mined\": \"Graceful stop failed\", \"cluster_count\": 271}\n",
      "Processing line: 120000, rate 30903.9 lines/sec, 271 clusters so far.\n",
      "Processing line: 130000, rate 41334.3 lines/sec, 271 clusters so far.\n",
      "Processing line: 140000, rate 37887.0 lines/sec, 271 clusters so far.\n",
      "Processing line: 150000, rate 44423.0 lines/sec, 271 clusters so far.\n",
      "Processing line: 160000, rate 39395.2 lines/sec, 271 clusters so far.\n",
      "Processing line: 170000, rate 42733.6 lines/sec, 271 clusters so far.\n",
      "Processing line: 180000, rate 41061.8 lines/sec, 271 clusters so far.\n",
      "Processing line: 190000, rate 41728.1 lines/sec, 271 clusters so far.\n",
      "Input (190822): Failed on local exception: java.net.SocketException: Permission denied: no further information; Host Details : local host is: \"MININT-FNANLI5/127.0.0.1\"; destination host is: \"msra-sa-41\":9000;\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 217, \"cluster_size\": 174, \"template_mined\": \"Failed on local exception: <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> Host Details : local host is: \\\"MININT-FNANLI5/<:IP:>\\\"; destination host is: \\\"msra-sa-<:NUM:>\\\":<:NUM:>;\", \"cluster_count\": 271}\n",
      "Input (190841): java.net.SocketException: Permission denied: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 272, \"cluster_size\": 1, \"template_mined\": \"java.net.SocketException: Permission denied: no further information\", \"cluster_count\": 272}\n",
      "Input (195348): Diagnostics report from attempt_1445087491445_0009_r_000000_0: AttemptID:attempt_1445087491445_0009_r_000000_0 Timed out after 600 secs\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 166, \"cluster_size\": 5, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: AttemptID:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> Timed out after <:NUM:> secs\", \"cluster_count\": 272}\n",
      "Processing line: 200000, rate 20340.0 lines/sec, 272 clusters so far.\n",
      "Input (200543): MapCompletionEvents reques\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 273, \"cluster_size\": 1, \"template_mined\": \"MapCompletionEvents reques\", \"cluster_count\": 273}\n",
      "Processing line: 210000, rate 17670.4 lines/sec, 273 clusters so far.\n",
      "Processing line: 220000, rate 19102.5 lines/sec, 273 clusters so far.\n",
      "Input (221630): Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MININT-FNANLI5/10.86.169.121\"; destination host is: \"04dn8iq.fareast.corp.microsoft.com\":49470;\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 250, \"cluster_size\": 2, \"template_mined\": \"Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: \\\"04dn8iq.fareast.corp.microsoft.com\\\":<:NUM:>;\", \"cluster_count\": 273}\n",
      "Input (224638): WordCount and PageRank. Each application has been run for several times, simulating both normal and abnormal cases with injected specific failures.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 274, \"cluster_size\": 1, \"template_mined\": \"WordCount and PageRank. Each application has been run for several times, simulating both normal and abnormal cases with injected specific failures.\", \"cluster_count\": 274}\n",
      "Input (225454): Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MSRA-SA-41/10.190.173.170\"; destination host is: \"minint-fnanli5.fareast.corp.microsoft.com\":62270;\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 250, \"cluster_size\": 3, \"template_mined\": \"Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\", \"cluster_count\": 274}\n",
      "Processing line: 230000, rate 20161.4 lines/sec, 274 clusters so far.\n",
      "Input (231113): Task attempt_1445182159119_0004_m_000004_0 failed : org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 275, \"cluster_size\": 1, \"template_mined\": \"Task attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> failed : org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 275}\n",
      "Input (233135): Progress of TaskAttempt attempt_1445087491445_0004_m_000003_1 is : 9.742042E-4\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 114, \"cluster_size\": 34147, \"template_mined\": \"Progress of TaskAttempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> is : <:*:>\", \"cluster_count\": 275}\n",
      "Input (233779): Error communicating with RM: Resource Manager doesn't recognize AttemptId: application_1445087491445_0004\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 276, \"cluster_size\": 1, \"template_mined\": \"Error communicating with RM: Resource Manager doesn't recognize AttemptId: application <:NUM:> <:NUM:>\", \"cluster_count\": 276}\n",
      "Input (233780): Resource Manager doesn't recognize AttemptId: application_1445087491445_0004\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 277, \"cluster_size\": 1, \"template_mined\": \"Resource Manager doesn't recognize AttemptId: application <:NUM:> <:NUM:>\", \"cluster_count\": 277}\n",
      "Input (233785): org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException: Application attempt appattempt_1445087491445_0004_000001 doesn't exist in ApplicationMasterService cache.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 278, \"cluster_size\": 1, \"template_mined\": \"org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException: Application attempt appattempt <:NUM:> <:NUM:> <:NUM:> doesn't exist in ApplicationMasterService cache.\", \"cluster_count\": 278}\n",
      "Input (233814): org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException): Application attempt appattempt_1445087491445_0004_000001 doesn't exist in ApplicationMasterService cache.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 279, \"cluster_size\": 1, \"template_mined\": \"org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException): Application attempt appattempt <:NUM:> <:NUM:> <:NUM:> doesn't exist in ApplicationMasterService cache.\", \"cluster_count\": 279}\n",
      "Input (233834): Notify RMCommunicator isAMLastRetry: false\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 128, \"cluster_size\": 75, \"template_mined\": \"Notify <:*:> isAMLastRetry: <:*:>\", \"cluster_count\": 279}\n",
      "Input (233835): RMCommunicator notified that shouldUnregistered is: false\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 129, \"cluster_size\": 38, \"template_mined\": \"RMCommunicator notified that shouldUnregistered is: <:*:>\", \"cluster_count\": 279}\n",
      "Input (233837): JobHistoryEventHandler notified that forceJobCompletion is false\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 130, \"cluster_size\": 38, \"template_mined\": \"JobHistoryEventHandler notified that forceJobCompletion is <:*:>\", \"cluster_count\": 279}\n",
      "Processing line: 240000, rate 19207.0 lines/sec, 279 clusters so far.\n",
      "Processing line: 250000, rate 39597.7 lines/sec, 279 clusters so far.\n",
      "Input (255041): Error closing writer for JobID: job_1445144423722_0023\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 280, \"cluster_size\": 1, \"template_mined\": \"Error closing writer for JobID: job <:NUM:> <:NUM:>\", \"cluster_count\": 280}\n",
      "Input (255105): Service JobHistoryEventHandler failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 281, \"cluster_size\": 1, \"template_mined\": \"Service JobHistoryEventHandler failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\", \"cluster_count\": 281}\n",
      "Input (255106): java.nio.channels.ClosedChannelException\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 282, \"cluster_size\": 1, \"template_mined\": \"java.nio.channels.ClosedChannelException\", \"cluster_count\": 282}\n",
      "Input (255131): When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 283, \"cluster_size\": 1, \"template_mined\": \"When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\", \"cluster_count\": 283}\n",
      "Input (255159): cleanup failed for container container_1445144423722_0023_01_000002 : java.lang.IllegalArgumentException: java.net.UnknownHostException: 04DN8IQ.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 266, \"cluster_size\": 2, \"template_mined\": \"cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: <:*:>\", \"cluster_count\": 283}\n",
      "Input (255183): java.net.UnknownHostException: 04DN8IQ.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 267, \"cluster_size\": 3, \"template_mined\": \"java.net.UnknownHostException: <:*:>\", \"cluster_count\": 283}\n",
      "Input (255187): Diagnostics report from attempt_1445144423722_0023_m_000000_0: cleanup failed for container container_1445144423722_0023_01_000002 : java.lang.IllegalArgumentException: java.net.UnknownHostException: 04DN8IQ.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 268, \"cluster_size\": 2, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: <:*:>\", \"cluster_count\": 283}\n",
      "Input (255491): Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 281, \"cluster_size\": 2, \"template_mined\": \"Service <:*:> failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\", \"cluster_count\": 283}\n",
      "Processing line: 260000, rate 33824.1 lines/sec, 283 clusters so far.\n",
      "Processing line: 270000, rate 37700.4 lines/sec, 283 clusters so far.\n",
      "Processing line: 280000, rate 47453.9 lines/sec, 283 clusters so far.\n",
      "Processing line: 290000, rate 45309.3 lines/sec, 283 clusters so far.\n",
      "Input (295221): Exception running child : java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 284, \"cluster_size\": 1, \"template_mined\": \"Exception running child : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 284}\n",
      "Input (295280): Exception cleaning up: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 285, \"cluster_size\": 1, \"template_mined\": \"Exception cleaning up: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 285}\n",
      "Input (299046): Task: attempt_1445182159119_0014_r_000000_0 - exited : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 286, \"cluster_size\": 1, \"template_mined\": \"Task: attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> - exited : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\", \"cluster_count\": 286}\n",
      "Input (299076): Diagnostics report from attempt_1445182159119_0014_r_000000_0: Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 287, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>: Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\", \"cluster_count\": 287}\n",
      "Processing line: 300000, rate 27424.8 lines/sec, 287 clusters so far.\n",
      "Input (301332): Task: attempt_1445182159119_0003_m_000000_0 - failed due to FSError: java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 288, \"cluster_size\": 1, \"template_mined\": \"Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - failed due to FSError: java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 288}\n",
      "Input (301333): Diagnostics report from attempt_1445182159119_0003_m_000000_0: FSError: java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 239, \"cluster_size\": 9, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: <:*:> java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 288}\n",
      "Input (301474): Assigning container Container: [ContainerId: container_1445182159119_0003_01_000016, NodeId: 04DN8IQ.fareast.corp.microsoft.com:64260, NodeHttpAddress: 04DN8IQ.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 5, Token: Token { kind: ContainerToken, service: 10.86.164.138:64260 }, ] to fast fail map\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 245, \"cluster_size\": 4, \"template_mined\": \"Assigning container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] to fast fail map\", \"cluster_count\": 288}\n",
      "Processing line: 310000, rate 18728.7 lines/sec, 288 clusters so far.\n",
      "Input (313342): for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000002_0,attempt_1445087491445_0002_m_000003_1,attempt_1445087491445_0002_m_000004_0,attempt_1445087491445_0002_m_000005_0,attempt_1445087491445_0002_m_000008_0,attempt_1445087491445_0002_m_000009_0,attempt_1445087491445_0002_m_000011_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 289, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 289}\n",
      "Input (317282): Task: attempt_1445144423722_0020_m_000002_0 - exited : java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 290, \"cluster_size\": 1, \"template_mined\": \"Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 290}\n",
      "Input (317339): Diagnostics report from attempt_1445144423722_0020_m_000002_0: Error: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 291, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 291}\n",
      "Input (317532): Task cleanup failed for attempt attempt_1445144423722_0020_m_000002_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 292, \"cluster_size\": 1, \"template_mined\": \"Task cleanup failed for attempt attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 292}\n",
      "Input (317651): 1 failures on node MININT-FNANLI5.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 237, \"cluster_size\": 9, \"template_mined\": \"<:NUM:> failures on node <:*:>\", \"cluster_count\": 292}\n",
      "Processing line: 320000, rate 26965.7 lines/sec, 292 clusters so far.\n",
      "Processing line: 330000, rate 41024.5 lines/sec, 292 clusters so far.\n",
      "Input (332788): In stop, writing event MAP_ATTEMPT_FAILED\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 293, \"cluster_size\": 1, \"template_mined\": \"In stop, writing event MAP ATTEMPT FAILED\", \"cluster_count\": 293}\n",
      "Input (332789): Service JobHistoryEventHandler failed in state STOPPED; cause: org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 294, \"cluster_size\": 1, \"template_mined\": \"Service JobHistoryEventHandler failed in state STOPPED; cause: org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\", \"cluster_count\": 294}\n",
      "Input (332790): Attempt to process a enum when a union was expected.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 295, \"cluster_size\": 1, \"template_mined\": \"Attempt to process a enum when a union was expected.\", \"cluster_count\": 295}\n",
      "Input (332812): When stopping the service JobHistoryEventHandler : org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 296, \"cluster_size\": 1, \"template_mined\": \"When stopping the service JobHistoryEventHandler : org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\", \"cluster_count\": 296}\n",
      "Input (333349): Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state STOPPED; cause: org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 294, \"cluster_size\": 2, \"template_mined\": \"Service <:*:> failed in state STOPPED; cause: org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\", \"cluster_count\": 296}\n",
      "Processing line: 340000, rate 33635.8 lines/sec, 296 clusters so far.\n",
      "Processing line: 350000, rate 38467.0 lines/sec, 296 clusters so far.\n",
      "Processing line: 360000, rate 38024.8 lines/sec, 296 clusters so far.\n",
      "Processing line: 370000, rate 41480.9 lines/sec, 296 clusters so far.\n",
      "Processing line: 380000, rate 23016.7 lines/sec, 296 clusters so far.\n",
      "Input (382915): Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#3\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 297, \"cluster_size\": 1, \"template_mined\": \"Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\", \"cluster_count\": 297}\n",
      "Input (386489): Shuffle failed : local error on this node: 04DN8IQ/10.86.164.138\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 298, \"cluster_size\": 1, \"template_mined\": \"Shuffle failed : local error on this node: 04DN8IQ/<:IP:>\", \"cluster_count\": 298}\n",
      "Input (388352): Ignoring exception during close for org.apache.hadoop.mapred.MapTask$NewOutputCollector@6a6585ee\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 299, \"cluster_size\": 1, \"template_mined\": \"Ignoring exception during close for org.apache.hadoop.mapred.MapTask$NewOutputCollector@6a6585ee\", \"cluster_count\": 299}\n",
      "Input (388364): Exception running child : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt_1445182159119_0002_m_000007_0/file.out\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 300, \"cluster_size\": 1, \"template_mined\": \"Exception running child : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\", \"cluster_count\": 300}\n",
      "Processing line: 390000, rate 17012.6 lines/sec, 300 clusters so far.\n",
      "Input (394043): IPC Server handler 29 on 58622, call statusUpdate(attempt_1445094324383_0003_m_000000_0, org.apache.hadoop.mapred.MapTaskStatus@cdcdbf7), rpc version=2, client version=19, methodsFingerPrint=937413979 from 10.86.169.121:52490 Call#68 Retry#0: output error\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 301, \"cluster_size\": 1, \"template_mined\": \"IPC Server handler <:NUM:> on <:NUM:>, call statusUpdate(attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>, org.apache.hadoop.mapred.MapTaskStatus@cdcdbf7), rpc version=<:NUM:>, client version=<:NUM:>, methodsFingerPrint=<:NUM:> from <:IP:>:<:NUM:> Call#<:NUM:> Retry#<:NUM:>: output error\", \"cluster_count\": 301}\n",
      "Input (394044): IPC Server handler 29 on 58622 caught an exception\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 302, \"cluster_size\": 1, \"template_mined\": \"IPC Server handler <:NUM:> on <:NUM:> caught an exception\", \"cluster_count\": 302}\n",
      "--- Done processing file in 16.55 sec. Total of 395363 lines, rate 23887.2 lines/sec, 302 clusters\n",
      "ID=28    : size=201731    : \n",
      "ID=114   : size=45882     : Progress of TaskAttempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> is : <:*:>\n",
      "ID=113   : size=18140     : MapCompletionEvents request from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>. startIndex <:NUM:> maxEvents <:NUM:>\n",
      "ID=212   : size=5795      : Address change detected. Old: msra-sa-<:NUM:>/<:IP:>:<:NUM:> New: msra-sa-<:NUM:>:<:NUM:>\n",
      "ID=12    : size=5542      : (EQUATOR) <:NUM:> kvi <:NUM:>(<:NUM:>)\n",
      "ID=18    : size=5339      : Spilling map output\n",
      "ID=19    : size=5339      : bufstart = <:NUM:>; bufend = <:NUM:>; bufvoid = <:NUM:>\n",
      "ID=20    : size=5339      : kvstart = <:NUM:>(<:NUM:>); kvend = <:NUM:>(<:NUM:>); length = <:NUM:>/<:NUM:>\n",
      "ID=203   : size=5330      : java.net.NoRouteToHostException: No route to host: no further information\n",
      "ID=214   : size=5303      : No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=213   : size=5300      : Failed to renew lease for [DFSClient NONMAPREDUCE <:NUM:> <:NUM:>] for <:NUM:> seconds. Will retry shortly ...\n",
      "ID=21    : size=5204      : Finished spill <:NUM:>\n",
      "ID=22    : size=4579      : (RESET) equator <:NUM:> kv <:NUM:>(<:NUM:>) kvi <:NUM:>(<:NUM:>)\n",
      "ID=93    : size=4415      : Recalculating schedule, headroom=<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "ID=148   : size=3841      : Reduce slow start threshold not met. completedMapsForReduceSlowstart <:NUM:>\n",
      "ID=84    : size=3297      : Resolved <:*:> to /default-rack\n",
      "ID=86    : size=2921      : attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> to <:*:>\n",
      "ID=164   : size=2517      : Retrying connect to server: <:*:> Already tried <:NUM:> time(s); maxRetries=<:NUM:>\n",
      "ID=88    : size=2074      : task <:NUM:> <:NUM:> <:*:> <:NUM:> Task Transitioned from <:*:> to <:*:>\n",
      "ID=107   : size=1757      : Opening proxy : <:*:>\n",
      "ID=105   : size=1744      : Processing the event EventType: CONTAINER REMOTE <:*:> for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> taskAttempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=154   : size=1611      : attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> <:*:> <:*:> <:*:> <:*:>\n",
      "ID=96    : size=1249      : After Scheduling: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\n",
      "ID=3     : size=1140      : MapTask metrics system <:*:>\n",
      "ID=165   : size=1048      : Retrying connect to server: <:*:> Already tried <:NUM:> time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=<:NUM:>, sleepTime=<:NUM:> MILLISECONDS)\n",
      "ID=85    : size=1033      : TaskAttempt: [attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>] using containerId: [container <:NUM:> <:NUM:> <:NUM:> <:NUM:> on NM: <:*:>\n",
      "ID=1     : size=978       : loaded properties from hadoop-metrics2.properties\n",
      "ID=2     : size=978       : Scheduled snapshot period at <:NUM:> second(s).\n",
      "ID=4     : size=978       : Executing with tokens:\n",
      "ID=110   : size=962       : Auth successful for job <:NUM:> <:NUM:> (auth:SIMPLE)\n",
      "ID=5     : size=909       : Kind: mapreduce.job, Service: job <:NUM:> <:NUM:>, Ident: <:*:>\n",
      "ID=6     : size=909       : Sleeping for 0ms before retrying again. Got null now.\n",
      "ID=7     : size=907       : mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application <:NUM:> <:NUM:>\n",
      "ID=8     : size=907       : session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "ID=100   : size=906       : Assigned container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> to attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=106   : size=906       : Launching attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=108   : size=906       : Shuffle port returned by ContainerManager for attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> : <:NUM:>\n",
      "ID=9     : size=905       : ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "ID=10    : size=905       : Using ResourceCalculatorProcessTree : <:*:>\n",
      "ID=109   : size=905       : ATTEMPT START task <:NUM:> <:NUM:> <:*:> <:NUM:>\n",
      "ID=111   : size=902       : JVM with ID : jvm <:NUM:> <:NUM:> <:*:> <:NUM:> asked for a task\n",
      "ID=112   : size=902       : JVM with ID: jvm <:NUM:> <:NUM:> <:*:> <:NUM:> given task: attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=97    : size=899       : getResources() for application <:NUM:> <:NUM:>: ask=<:NUM:> release= <:NUM:> newContainers=<:NUM:> finishedContainers=<:NUM:> resourcelimit=<memory:<:NUM:>, vCores:<:NUM:>> knownNMs=<:NUM:>\n",
      "ID=125   : size=851       : KILLING attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=11    : size=834       : Processing split: <:*:>\n",
      "ID=13    : size=834       : mapreduce.task.io.sort.mb: <:NUM:>\n",
      "ID=14    : size=834       : soft limit at <:NUM:>\n",
      "ID=15    : size=834       : bufstart = <:NUM:>; bufvoid = <:NUM:>\n",
      "ID=16    : size=834       : kvstart = <:NUM:>; length = <:NUM:>\n",
      "ID=17    : size=834       : Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "ID=150   : size=826       : Received completed container container <:NUM:> <:NUM:> <:NUM:> <:NUM:>\n",
      "ID=92    : size=797       : Before Scheduling: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\n",
      "ID=160   : size=733       : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: Container killed by the ApplicationMaster.\n",
      "ID=87    : size=701       : Task succeeded with attempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=90    : size=701       : Num completed Tasks: <:NUM:>\n",
      "ID=24    : size=678       : Merging <:NUM:> sorted segments\n",
      "ID=25    : size=678       : Down to the last merge-pass, with <:NUM:> segments left of total size: <:NUM:> bytes\n",
      "ID=178   : size=667       : attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Shuffling to disk since <:NUM:> is greater than maxSingleShuffleLimit (<:NUM:>)\n",
      "ID=179   : size=664       : fetcher#<:NUM:> about to shuffle output of map attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> decomp: <:NUM:> len: <:NUM:> to DISK\n",
      "ID=185   : size=649       : Read <:NUM:> bytes from map-output for attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "ID=23    : size=644       : Starting flush of map output\n",
      "ID=26    : size=625       : Task:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> is done. And is in the process of committing\n",
      "ID=42    : size=621       : Registering class <:*:> for class <:*:>\n",
      "ID=27    : size=616       : Task 'attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>' done.\n",
      "ID=98    : size=606       : Got allocated containers <:NUM:>\n",
      "ID=123   : size=605       : Done acknowledgement from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=158   : size=531       : completedMapPercent <:NUM:>.<:NUM:> totalResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> finalMapResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> finalReduceResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> netScheduledMapResource:<memory:<:NUM:>, vCores:<:NUM:>> netScheduledReduceResource:<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "ID=217   : size=487       : Failed on local exception: <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> Host Details : local host is: \"MININT-FNANLI5/<:IP:>\"; destination host is: \"msra-sa-<:NUM:>\":<:NUM:>;\n",
      "ID=215   : size=480       : ERROR IN CONTACTING RM.\n",
      "ID=218   : size=479       : java.io.IOException: Couldn't set up IO streams\n",
      "ID=219   : size=479       : java.nio.channels.UnresolvedAddressException\n",
      "ID=175   : size=472       : Assigning <:*:> with <:NUM:> to fetcher#<:NUM:>\n",
      "ID=176   : size=472       : assigned <:NUM:> of <:NUM:> to <:*:> to fetcher#<:NUM:>\n",
      "ID=174   : size=414       : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>: Got <:NUM:> new map-outputs\n",
      "ID=186   : size=383       : MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:> freed by fetcher#<:NUM:> in <:*:>\n",
      "ID=181   : size=381       : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=82    : size=335       : Processing the event EventType: <:*:> <:*:>\n",
      "ID=35    : size=317       : Stopping <:*:> metrics system...\n",
      "ID=54    : size=306       : job <:NUM:> <:*:> Transitioned from <:*:> to <:*:>\n",
      "ID=36    : size=302       : MapTask metrics system shutdown complete.\n",
      "ID=155   : size=255       : DefaultSpeculator.addSpeculativeAttempt -- we are speculating task <:NUM:> <:NUM:> m <:NUM:>\n",
      "ID=156   : size=255       : We launched <:NUM:> speculations. Sleeping <:NUM:> milliseconds.\n",
      "ID=43    : size=226       : Default file system [hdfs://msra-sa-<:NUM:>:<:NUM:>]\n",
      "ID=153   : size=218       : An existing connection was forcibly closed by the remote host\n",
      "ID=152   : size=217       : Socket Reader #<:NUM:> for port <:NUM:>: readAndProcess from client <:IP:> threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]\n",
      "ID=162   : size=216       : attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> CONTAINER CLEANUP to <:*:> TASK CLEANUP\n",
      "ID=163   : size=216       : Could not delete <:*:> temporary/<:NUM:>/ temporary/attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=157   : size=194       : Scheduling a redundant attempt for task task <:NUM:> <:NUM:> m <:NUM:>\n",
      "ID=161   : size=182       : Issuing kill to other attempt attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "ID=136   : size=141       : Moved tmp to done: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "ID=56    : size=138       : Using callQueue class java.util.concurrent.LinkedBlockingQueue\n",
      "ID=57    : size=138       : Starting Socket Reader #<:NUM:> for port <:NUM:>\n",
      "ID=59    : size=138       : IPC Server listener on <:NUM:>: starting\n",
      "ID=60    : size=138       : IPC Server Responder: starting\n",
      "ID=65    : size=138       : Added filter AM PROXY FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context <:*:>\n",
      "ID=66    : size=138       : adding path spec: <:*:>\n",
      "ID=101   : size=138       : The <:*:> file on the remote FS is <:*:> <:NUM:> <:*:>\n",
      "ID=206   : size=126       : <:NUM:>\n",
      "ID=120   : size=119       : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:>\n",
      "ID=128   : size=104       : Notify <:*:> isAMLastRetry: <:*:>\n",
      "ID=222   : size=100       : Ramping down all scheduled reduces:<:NUM:>\n",
      "ID=223   : size=100       : Going to preempt <:NUM:> due to lack of space for maps\n",
      "ID=47    : size=96        : Read from history task task <:NUM:> <:NUM:> m <:NUM:>\n",
      "ID=83    : size=96        : Recovering task task <:NUM:> <:NUM:> m <:NUM:> from prior app attempt, status was SUCCEEDED\n",
      "ID=134   : size=94        : Copied to done location: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "ID=171   : size=86        : ReduceTask metrics system <:*:>\n",
      "ID=180   : size=84        : Ignoring obsolete output of <:*:> map-task: 'attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>'\n",
      "ID=99    : size=73        : Assigned to reduce\n",
      "ID=201   : size=72        : <:*:> freed by fetcher#<:NUM:> in <:*:>\n",
      "ID=172   : size=71        : Using ShuffleConsumerPlugin: <:*:>\n",
      "ID=173   : size=71        : MergerManager: memoryLimit=<:NUM:>, maxSingleShuffleLimit=<:NUM:>, mergeThreshold=<:NUM:>, ioSortFactor=<:NUM:>, memToMemMergeOutputsThreshold=<:NUM:>\n",
      "ID=37    : size=69        : Created MRAppMaster for application appattempt <:NUM:> <:NUM:> <:NUM:>\n",
      "ID=38    : size=69        : Kind: YARN AM RM TOKEN, Service: , Ident: (appAttemptId { application id { id: <:NUM:> cluster timestamp: <:NUM:> } attemptId: <:NUM:> } keyId: <:NUM:>)\n",
      "ID=39    : size=69        : Using mapred newApiCommitter.\n",
      "ID=40    : size=69        : OutputCommitter set in config null\n",
      "ID=41    : size=69        : OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "ID=44    : size=69        : Emitting job history data to the timeline server is not enabled\n",
      "ID=49    : size=69        : MRAppMaster metrics system started\n",
      "ID=50    : size=69        : Adding job token for job <:NUM:> <:NUM:> to jobTokenSecretManager\n",
      "ID=51    : size=69        : Not uberizing job <:NUM:> <:NUM:> because: not enabled; too many maps; too much input;\n",
      "ID=52    : size=69        : Input size for job job <:NUM:> <:NUM:> = <:NUM:>. Number of splits = <:NUM:>\n",
      "ID=53    : size=69        : Number of reduces for job job <:NUM:> <:NUM:> = <:NUM:>\n",
      "ID=55    : size=69        : MRAppMaster launching normal, non-uberized, multi-container job job <:NUM:> <:NUM:>.\n",
      "ID=58    : size=69        : Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server\n",
      "ID=61    : size=69        : Instantiated MRClientService at <:*:>\n",
      "ID=62    : size=69        : Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n",
      "ID=63    : size=69        : Http request log for http.requests.mapreduce is not defined\n",
      "ID=64    : size=69        : Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n",
      "ID=67    : size=69        : Jetty bound to port <:NUM:>\n",
      "ID=68    : size=69        : jetty-<:NUM:>.<:NUM:>.<:NUM:>\n",
      "ID=69    : size=69        : Extract jar:file:/D:/hadoop-<:NUM:>.<:NUM:>.<:NUM:>-localbox/share/hadoop/yarn/hadoop-yarn-common-<:NUM:>.<:NUM:>.<:NUM:>-SNAPSHOT.jar!/webapps/mapreduce to <:*:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> mapreduce <:*:>\n",
      "ID=70    : size=69        : Started HttpServer2$SelectChannelConnectorWithSafeStartup@<:IP:>:<:NUM:>\n",
      "ID=71    : size=69        : Web app /mapreduce started at <:NUM:>\n",
      "ID=72    : size=69        : Registered webapp guice modules\n",
      "ID=73    : size=69        : JOB CREATE job <:NUM:> <:NUM:>\n",
      "ID=74    : size=69        : nodeBlacklistingEnabled:true\n",
      "ID=75    : size=69        : maxTaskFailuresPerNode is <:NUM:>\n",
      "ID=76    : size=69        : blacklistDisablePercent is <:NUM:>\n",
      "ID=77    : size=69        : Connecting to ResourceManager at <:*:>\n",
      "ID=78    : size=69        : maxContainerCapability: <memory:<:NUM:>, vCores:<:NUM:>>\n",
      "ID=79    : size=69        : queue: default\n",
      "ID=80    : size=69        : Upper limit on the thread pool size is <:NUM:>\n",
      "ID=81    : size=69        : yarn.client.max-cached-nodemanagers-proxies : <:NUM:>\n",
      "ID=89    : size=69        : Event Writer setup for JobId: job <:NUM:> <:NUM:>, File: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist\n",
      "ID=91    : size=69        : reduceResourceRequest:<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "ID=102   : size=69        : Adding #<:NUM:> tokens and #<:NUM:> secret keys for NM use for launching container\n",
      "ID=103   : size=69        : Size of containertokens dob is <:NUM:>\n",
      "ID=104   : size=69        : Putting shuffle token in serviceData\n",
      "ID=94    : size=65        : Reduce slow start threshold reached. Scheduling reduces.\n",
      "ID=147   : size=65        : mapResourceRequest:<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "ID=190   : size=56        : finalMerge called with <:NUM:> in-memory map-outputs and <:NUM:> on-disk map-outputs\n",
      "ID=191   : size=56        : Merging <:NUM:> files, <:NUM:> bytes from disk\n",
      "ID=192   : size=56        : Merging <:NUM:> segments, <:NUM:> bytes from memory into reduce\n",
      "ID=193   : size=56        : mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "ID=167   : size=55        : Killing taskAttempt:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> because it is running on unusable <:*:>\n",
      "ID=168   : size=55        : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: Container released on a *lost* node\n",
      "ID=170   : size=55        : java.io.IOException: An existing connection was forcibly closed by the remote host\n",
      "ID=189   : size=54        : EventFetcher is interrupted.. Returning\n",
      "ID=129   : size=52        : RMCommunicator notified that shouldUnregistered is: <:*:>\n",
      "ID=130   : size=52        : JobHistoryEventHandler notified that forceJobCompletion is <:*:>\n",
      "ID=131   : size=52        : Calling stop for all the services\n",
      "ID=132   : size=52        : Stopping JobHistoryEventHandler. Size of the outstanding queue size is <:NUM:>\n",
      "ID=127   : size=50        : We are finishing cleanly so this is the last retry\n",
      "ID=137   : size=48        : Stopped JobHistoryEventHandler. super.stop()\n",
      "ID=138   : size=48        : Setting job diagnostics to\n",
      "ID=139   : size=48        : History url is <:*:> <:NUM:> <:NUM:>\n",
      "ID=141   : size=48        : Final Stats: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\n",
      "ID=143   : size=48        : Stopping server on <:NUM:>\n",
      "ID=144   : size=48        : Stopping IPC Server listener on <:NUM:>\n",
      "ID=145   : size=48        : TaskHeartbeatHandler thread interrupted\n",
      "ID=146   : size=48        : Stopping IPC Server Responder\n",
      "ID=194   : size=48        : Task attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> is allowed to commit now\n",
      "ID=195   : size=48        : Saved output of task 'attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>' to <:*:> temporary/<:NUM:>/task <:NUM:> <:NUM:> r <:NUM:>\n",
      "ID=118   : size=47        : Commit-pending state update from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "ID=119   : size=47        : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> TaskAttempt Transitioned from RUNNING to COMMIT PENDING\n",
      "ID=121   : size=47        : Commit go/no-go request from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "ID=122   : size=47        : Result of canCommit for attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>:true\n",
      "ID=124   : size=47        : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> TaskAttempt Transitioned from COMMIT PENDING to SUCCESS CONTAINER CLEANUP\n",
      "ID=126   : size=47        : Calling handler for JobFinishedEvent\n",
      "ID=133   : size=47        : Copying hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist to hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "ID=135   : size=47        : Copying hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:> conf.xml to hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "ID=140   : size=45        : Waiting for application to be successfully unregistered.\n",
      "ID=142   : size=45        : Deleting staging directory hdfs://msra-sa-<:NUM:>:<:NUM:> /tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>\n",
      "ID=169   : size=44        : Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\n",
      "ID=95    : size=43        : All maps assigned. Ramping up all remaining reduces:<:NUM:>\n",
      "ID=159   : size=31        : Ramping up <:NUM:>\n",
      "ID=197   : size=30        : Exception in createBlockOutputStream\n",
      "ID=198   : size=30        : Bad connect ack with firstBadLink as <:IP:>:<:NUM:>\n",
      "ID=199   : size=30        : Abandoning BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:>\n",
      "ID=200   : size=30        : Excluding datanode <:IP:>:<:NUM:>\n",
      "ID=187   : size=29        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=208   : size=25        : RUNNABLE\n",
      "ID=221   : size=25        : TaskAttempt killed because it ran on unusable node <:*:> AttemptId:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "ID=267   : size=24        : java.net.UnknownHostException: <:*:>\n",
      "ID=210   : size=21        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=207   : size=20        : TIMED WAITING\n",
      "ID=251   : size=20        : No route to host: no further information\n",
      "ID=46    : size=19        : Previous history file is at hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist\n",
      "ID=151   : size=19        : Container complete event for unknown container id container <:NUM:> <:NUM:> <:NUM:> <:NUM:>\n",
      "ID=182   : size=19        : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>:\n",
      "ID=205   : size=18        : WAITING\n",
      "ID=242   : size=18        : java.io.IOException: There is not enough space on the disk\n",
      "ID=241   : size=17        : org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\n",
      "ID=29    : size=16        : I/O error constructing remote block reader.\n",
      "ID=239   : size=16        : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: <:*:> java.io.IOException: There is not enough space on the disk\n",
      "ID=45    : size=15        : Recovery is enabled. Will try to recover from previous life on best effort basis.\n",
      "ID=196   : size=15        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=229   : size=15        : ReduceTask metrics system shutdown complete.\n",
      "ID=115   : size=14        : DFSOutputStream ResponseProcessor exception for block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:>\n",
      "ID=116   : size=14        : Bad response ERROR for block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> from datanode <:IP:>:<:NUM:>\n",
      "ID=117   : size=14        : Error Recovery for block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> in pipeline <:IP:>:<:NUM:>, <:IP:>:<:NUM:>: bad datanode <:IP:>:<:NUM:>\n",
      "ID=262   : size=14        : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=149   : size=12        : Cannot assign container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] for a map as either container memory less than required <memory:<:NUM:>, vCores:<:NUM:>> or no pending map tasks - maps.isEmpty=true\n",
      "ID=188   : size=12        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=266   : size=12        : cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: <:*:>\n",
      "ID=268   : size=12        : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: <:*:>\n",
      "ID=48    : size=11        : Read completed tasks from history <:NUM:>\n",
      "ID=30    : size=10        : Connection timed out: no further information\n",
      "ID=202   : size=10        : Communication exception: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to <:IP:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=237   : size=10        : <:NUM:> failures on node <:*:>\n",
      "ID=244   : size=10        : Added attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> to list of failed maps\n",
      "ID=252   : size=10        : Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information\n",
      "ID=183   : size=9         : Slow ReadProcessor read fields took <:*:> (threshold=30000ms); ack: seqno: <:NUM:> status: SUCCESS status: ERROR downstreamAckTimeNanos: <:NUM:>, targets: [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>]\n",
      "ID=177   : size=8         : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=245   : size=8         : Assigning container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] to fast fail map\n",
      "ID=246   : size=8         : Assigned from earlierFailedMaps\n",
      "ID=248   : size=8         : Incompatible event log version: null\n",
      "ID=272   : size=8         : java.net.SocketException: Permission denied: no further information\n",
      "ID=282   : size=8         : java.nio.channels.ClosedChannelException\n",
      "ID=166   : size=7         : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: AttemptID:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> Timed out after <:NUM:> secs\n",
      "ID=216   : size=7         : Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\n",
      "ID=226   : size=7         : Merging <:NUM:> intermediate segments out of a total of <:NUM:>\n",
      "ID=230   : size=7         : Releasing unassigned and invalid container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ]. RM may have assignment issues\n",
      "ID=32    : size=6         : Successfully connected to /<:IP:>:<:NUM:> for BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:>\n",
      "ID=211   : size=6         : Runnning cleanup for the task\n",
      "ID=238   : size=6         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.io.IOException: There is not enough space on the disk\n",
      "ID=243   : size=6         : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: java.io.IOException: Spill failed\n",
      "ID=254   : size=6         : DFS chooseDataNode: got # <:NUM:> IOException, will wait for <:NUM:>.<:NUM:> msec.\n",
      "ID=258   : size=6         : Could not contact RM after <:NUM:> milliseconds.\n",
      "ID=31    : size=5         : Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information\n",
      "ID=184   : size=5         : In stop, writing event <:*:> FINISHED\n",
      "ID=204   : size=5         : Process Thread Dump: Communication exception\n",
      "ID=209   : size=5         : Last retry, killing attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "ID=253   : size=5         : Could not obtain BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> from any node: java.io.IOException: No live nodes contain block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> after checking nodes = [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>], ignoredNodes = null No live nodes contain current block Block locations: <:IP:>:<:NUM:> <:IP:>:<:NUM:> Dead nodes: <:IP:>:<:NUM:> <:IP:>:<:NUM:>. Will get new block locations from namenode and retry...\n",
      "ID=247   : size=4         : Unable to parse prior job history, aborting recovery\n",
      "ID=249   : size=4         : Could not parse the old history file. Will not have old AMinfos\n",
      "ID=250   : size=4         : Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\n",
      "ID=260   : size=4         : Error writing History Event: <:*:>\n",
      "ID=287   : size=4         : Diagnostics report from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>: Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\n",
      "ID=291   : size=4         : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=295   : size=4         : Attempt to process a enum when a union was expected.\n",
      "ID=220   : size=3         : Exception in getting events\n",
      "ID=240   : size=3         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.io.IOException: Spill failed\n",
      "ID=256   : size=3         : DFS Read\n",
      "ID=257   : size=3         : DataStreamer Exception\n",
      "ID=259   : size=3         : Error communicating with RM: Could not contact RM after <:NUM:> milliseconds.\n",
      "ID=261   : size=3         : Thread Thread[eventHandlingThread,<:NUM:>,main] threw an Exception.\n",
      "ID=269   : size=3         : Exception while unregistering\n",
      "ID=270   : size=3         : Skipping cleaning up the staging dir. assuming AM will be retried.\n",
      "ID=271   : size=3         : Graceful stop failed\n",
      "ID=289   : size=3         : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=33    : size=2         : Connection refused: no further information\n",
      "ID=236   : size=2         : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\n",
      "ID=263   : size=2         : Found jobId job <:NUM:> <:NUM:> to have not been closed. Will close\n",
      "ID=264   : size=2         : Service <:*:> failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=276   : size=2         : Error communicating with RM: Resource Manager doesn't recognize AttemptId: application <:NUM:> <:NUM:>\n",
      "ID=277   : size=2         : Resource Manager doesn't recognize AttemptId: application <:NUM:> <:NUM:>\n",
      "ID=278   : size=2         : org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException: Application attempt appattempt <:NUM:> <:NUM:> <:NUM:> doesn't exist in ApplicationMasterService cache.\n",
      "ID=279   : size=2         : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException): Application attempt appattempt <:NUM:> <:NUM:> <:NUM:> doesn't exist in ApplicationMasterService cache.\n",
      "ID=281   : size=2         : Service <:*:> failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n",
      "ID=284   : size=2         : Exception running child : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=285   : size=2         : Exception cleaning up: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=286   : size=2         : Task: attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> - exited : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\n",
      "ID=288   : size=2         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - failed due to FSError: java.io.IOException: There is not enough space on the disk\n",
      "ID=290   : size=2         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=292   : size=2         : Task cleanup failed for attempt attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "ID=294   : size=2         : Service <:*:> failed in state STOPPED; cause: org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\n",
      "ID=297   : size=2         : Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\n",
      "ID=298   : size=2         : Shuffle failed : local error on this node: 04DN8IQ/<:IP:>\n",
      "ID=34    : size=1         : Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.ConnectException: Connection refused: no further information\n",
      "ID=224   : size=1         : Preempting attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "ID=225   : size=1         : Reduce preemption successful attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "ID=227   : size=1         : Communication exception: java.net.ConnectException: Call From MSRA-SA-<:NUM:>/<:IP:> to minint-fnanli5.fareast.corp.microsoft.com:<:NUM:> failed on connection exception: java.net.ConnectException: Connection timed out: no further information; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused\n",
      "ID=228   : size=1         : java.net.ConnectException: Connection timed out: no further information\n",
      "ID=231   : size=1         : Connection retry failed with <:NUM:> attempts in <:NUM:> seconds\n",
      "ID=232   : size=1         : Failed to connect to MININT-FNANLI5.fareast.corp.microsoft.com:<:NUM:> with <:NUM:> map outputs\n",
      "ID=233   : size=1         : Connection timed out: connect\n",
      "ID=234   : size=1         : Reporting fetch failure for attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> to jobtracker.\n",
      "ID=235   : size=1         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\n",
      "ID=255   : size=1         : Could not obtain BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> from any node: java.io.IOException: No live nodes contain block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> after checking nodes = [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>], ignoredNodes = null No live nodes contain current block Block locations: <:IP:>:<:NUM:> <:IP:>:<:NUM:> Dead nodes: <:IP:>:<:NUM:> <:IP:>:<:NUM:> <:IP:>:<:NUM:>. Will get new block locations from namenode and retry...\n",
      "ID=265   : size=1         : When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=273   : size=1         : MapCompletionEvents reques\n",
      "ID=274   : size=1         : WordCount and PageRank. Each application has been run for several times, simulating both normal and abnormal cases with injected specific failures.\n",
      "ID=275   : size=1         : Task attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> failed : org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\n",
      "ID=280   : size=1         : Error closing writer for JobID: job <:NUM:> <:NUM:>\n",
      "ID=283   : size=1         : When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n",
      "ID=293   : size=1         : In stop, writing event MAP ATTEMPT FAILED\n",
      "ID=296   : size=1         : When stopping the service JobHistoryEventHandler : org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\n",
      "ID=299   : size=1         : Ignoring exception during close for org.apache.hadoop.mapred.MapTask$NewOutputCollector@6a6585ee\n",
      "ID=300   : size=1         : Exception running child : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\n",
      "ID=301   : size=1         : IPC Server handler <:NUM:> on <:NUM:>, call statusUpdate(attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>, org.apache.hadoop.mapred.MapTaskStatus@cdcdbf7), rpc version=<:NUM:>, client version=<:NUM:>, methodsFingerPrint=<:NUM:> from <:IP:>:<:NUM:> Call#<:NUM:> Retry#<:NUM:>: output error\n",
      "ID=302   : size=1         : IPC Server handler <:NUM:> on <:NUM:> caught an exception\n",
      "Prefix Tree:\n",
      "<root>\n",
      "\t<L=4>\n",
      "\t\t\"loaded\" (cluster_count=1)\n",
      "\t\t\tID=1     : size=978       : loaded properties from hadoop-metrics2.properties\n",
      "\t\t\"MapTask\" (cluster_count=1)\n",
      "\t\t\tID=3     : size=1140      : MapTask metrics system <:*:>\n",
      "\t\t\"Using\" (cluster_count=2)\n",
      "\t\t\tID=10    : size=905       : Using ResourceCalculatorProcessTree : <:*:>\n",
      "\t\t\tID=56    : size=138       : Using callQueue class java.util.concurrent.LinkedBlockingQueue\n",
      "\t\t\"(EQUATOR)\" (cluster_count=1)\n",
      "\t\t\tID=12    : size=5542      : (EQUATOR) <:NUM:> kvi <:NUM:>(<:NUM:>)\n",
      "\t\t\"soft\" (cluster_count=1)\n",
      "\t\t\tID=14    : size=834       : soft limit at <:NUM:>\n",
      "\t\t\"Merging\" (cluster_count=1)\n",
      "\t\t\tID=24    : size=678       : Merging <:NUM:> sorted segments\n",
      "\t\t\"Stopping\" (cluster_count=3)\n",
      "\t\t\tID=35    : size=317       : Stopping <:*:> metrics system...\n",
      "\t\t\tID=143   : size=48        : Stopping server on <:NUM:>\n",
      "\t\t\tID=146   : size=48        : Stopping IPC Server Responder\n",
      "\t\t\"Default\" (cluster_count=1)\n",
      "\t\t\tID=43    : size=226       : Default file system [hdfs://msra-sa-<:NUM:>:<:NUM:>]\n",
      "\t\t\"MRAppMaster\" (cluster_count=1)\n",
      "\t\t\tID=49    : size=69        : MRAppMaster metrics system started\n",
      "\t\t\"IPC\" (cluster_count=1)\n",
      "\t\t\tID=60    : size=138       : IPC Server Responder: starting\n",
      "\t\t\"Instantiated\" (cluster_count=1)\n",
      "\t\t\tID=61    : size=69        : Instantiated MRClientService at <:*:>\n",
      "\t\t\"adding\" (cluster_count=1)\n",
      "\t\t\tID=66    : size=138       : adding path spec: <:*:>\n",
      "\t\t\"Registered\" (cluster_count=1)\n",
      "\t\t\tID=72    : size=69        : Registered webapp guice modules\n",
      "\t\t\"Resolved\" (cluster_count=1)\n",
      "\t\t\tID=84    : size=3297      : Resolved <:*:> to /default-rack\n",
      "\t\t\"Num\" (cluster_count=1)\n",
      "\t\t\tID=90    : size=701       : Num completed Tasks: <:NUM:>\n",
      "\t\t\"Recalculating\" (cluster_count=1)\n",
      "\t\t\tID=93    : size=4415      : Recalculating schedule, headroom=<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "\t\t\"Got\" (cluster_count=1)\n",
      "\t\t\tID=98    : size=606       : Got allocated containers <:NUM:>\n",
      "\t\t\"Opening\" (cluster_count=1)\n",
      "\t\t\tID=107   : size=1757      : Opening proxy : <:*:>\n",
      "\t\t\"Calling\" (cluster_count=1)\n",
      "\t\t\tID=126   : size=47        : Calling handler for JobFinishedEvent\n",
      "\t\t\"Notify\" (cluster_count=1)\n",
      "\t\t\tID=128   : size=104       : Notify <:*:> isAMLastRetry: <:*:>\n",
      "\t\t\"Setting\" (cluster_count=1)\n",
      "\t\t\tID=138   : size=48        : Setting job diagnostics to\n",
      "\t\t\"ReduceTask\" (cluster_count=1)\n",
      "\t\t\tID=171   : size=86        : ReduceTask metrics system <:*:>\n",
      "\t\t\"EventFetcher\" (cluster_count=1)\n",
      "\t\t\tID=189   : size=54        : EventFetcher is interrupted.. Returning\n",
      "\t\t\"Abandoning\" (cluster_count=1)\n",
      "\t\t\tID=199   : size=30        : Abandoning BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:>\n",
      "\t\t\"ERROR\" (cluster_count=1)\n",
      "\t\t\tID=215   : size=480       : ERROR IN CONTACTING RM.\n",
      "\t\t\"Exception\" (cluster_count=1)\n",
      "\t\t\tID=220   : size=3         : Exception in getting events\n",
      "\t\t\"Connection\" (cluster_count=1)\n",
      "\t\t\tID=233   : size=1         : Connection timed out: connect\n",
      "\t<L=6>\n",
      "\t\t\"Scheduled\" (cluster_count=1)\n",
      "\t\t\tID=2     : size=978       : Scheduled snapshot period at <:NUM:> second(s).\n",
      "\t\t\"mapreduce.cluster.local.dir\" (cluster_count=1)\n",
      "\t\t\tID=7     : size=907       : mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application <:NUM:> <:NUM:>\n",
      "\t\t\"session.id\" (cluster_count=1)\n",
      "\t\t\tID=8     : size=907       : session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "\t\t\"bufstart\" (cluster_count=1)\n",
      "\t\t\tID=15    : size=834       : bufstart = <:NUM:>; bufvoid = <:NUM:>\n",
      "\t\t\"kvstart\" (cluster_count=1)\n",
      "\t\t\tID=16    : size=834       : kvstart = <:NUM:>; length = <:NUM:>\n",
      "\t\t\"Map\" (cluster_count=1)\n",
      "\t\t\tID=17    : size=834       : Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "\t\t\"I/O\" (cluster_count=1)\n",
      "\t\t\tID=29    : size=16        : I/O error constructing remote block reader.\n",
      "\t\t\"Connection\" (cluster_count=1)\n",
      "\t\t\tID=30    : size=10        : Connection timed out: no further information\n",
      "\t\t\"Registering\" (cluster_count=1)\n",
      "\t\t\tID=42    : size=621       : Registering class <:*:> for class <:*:>\n",
      "\t\t\"Read\" (cluster_count=1)\n",
      "\t\t\tID=48    : size=11        : Read completed tasks from history <:NUM:>\n",
      "\t\t\"Adding\" (cluster_count=1)\n",
      "\t\t\tID=58    : size=69        : Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server\n",
      "\t\t\"IPC\" (cluster_count=1)\n",
      "\t\t\tID=59    : size=138       : IPC Server listener on <:NUM:>: starting\n",
      "\t\t\"Web\" (cluster_count=1)\n",
      "\t\t\tID=71    : size=69        : Web app /mapreduce started at <:NUM:>\n",
      "\t\t\"Processing\" (cluster_count=1)\n",
      "\t\t\tID=82    : size=335       : Processing the event EventType: <:*:> <:*:>\n",
      "\t\t\"Size\" (cluster_count=1)\n",
      "\t\t\tID=103   : size=69        : Size of containertokens dob is <:NUM:>\n",
      "\t\t\"RMCommunicator\" (cluster_count=1)\n",
      "\t\t\tID=129   : size=52        : RMCommunicator notified that shouldUnregistered is: <:*:>\n",
      "\t\t\"JobHistoryEventHandler\" (cluster_count=1)\n",
      "\t\t\tID=130   : size=52        : JobHistoryEventHandler notified that forceJobCompletion is <:*:>\n",
      "\t\t\"Calling\" (cluster_count=1)\n",
      "\t\t\tID=131   : size=52        : Calling stop for all the services\n",
      "\t\t\"History\" (cluster_count=1)\n",
      "\t\t\tID=139   : size=48        : History url is <:*:> <:NUM:> <:NUM:>\n",
      "\t\t\"Stopping\" (cluster_count=1)\n",
      "\t\t\tID=144   : size=48        : Stopping IPC Server listener on <:NUM:>\n",
      "\t\t\"MergerManager:\" (cluster_count=1)\n",
      "\t\t\tID=173   : size=71        : MergerManager: memoryLimit=<:NUM:>, maxSingleShuffleLimit=<:NUM:>, mergeThreshold=<:NUM:>, ioSortFactor=<:NUM:>, memToMemMergeOutputsThreshold=<:NUM:>\n",
      "\t\t\"Assigning\" (cluster_count=1)\n",
      "\t\t\tID=175   : size=472       : Assigning <:*:> with <:NUM:> to fetcher#<:NUM:>\n",
      "\t\t\"In\" (cluster_count=1)\n",
      "\t\t\tID=184   : size=5         : In stop, writing event <:*:> FINISHED\n",
      "\t\t\"MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>\" (cluster_count=1)\n",
      "\t\t\tID=186   : size=383       : MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:> freed by fetcher#<:NUM:> in <:*:>\n",
      "\t\t\"mapred.skip.on\" (cluster_count=1)\n",
      "\t\t\tID=193   : size=56        : mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "\t\t\"<:*:>\" (cluster_count=1)\n",
      "\t\t\tID=201   : size=72        : <:*:> freed by fetcher#<:NUM:> in <:*:>\n",
      "\t\t\"java.io.IOException:\" (cluster_count=1)\n",
      "\t\t\tID=218   : size=479       : java.io.IOException: Couldn't set up IO streams\n",
      "\t\t\"java.net.SocketException:\" (cluster_count=1)\n",
      "\t\t\tID=272   : size=8         : java.net.SocketException: Permission denied: no further information\n",
      "\t\t\"Ignoring\" (cluster_count=1)\n",
      "\t\t\tID=299   : size=1         : Ignoring exception during close for org.apache.hadoop.mapred.MapTask$NewOutputCollector@6a6585ee\n",
      "\t<L=3>\n",
      "\t\t\"Executing\" (cluster_count=1)\n",
      "\t\t\tID=4     : size=978       : Executing with tokens:\n",
      "\t\t\"Processing\" (cluster_count=1)\n",
      "\t\t\tID=11    : size=834       : Processing split: <:*:>\n",
      "\t\t\"Spilling\" (cluster_count=1)\n",
      "\t\t\tID=18    : size=5339      : Spilling map output\n",
      "\t\t\"Finished\" (cluster_count=1)\n",
      "\t\t\tID=21    : size=5204      : Finished spill <:NUM:>\n",
      "\t\t\"Using\" (cluster_count=2)\n",
      "\t\t\tID=39    : size=69        : Using mapred newApiCommitter.\n",
      "\t\t\tID=172   : size=71        : Using ShuffleConsumerPlugin: <:*:>\n",
      "\t\t\"OutputCommitter\" (cluster_count=1)\n",
      "\t\t\tID=41    : size=69        : OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "\t\t\"maxTaskFailuresPerNode\" (cluster_count=1)\n",
      "\t\t\tID=75    : size=69        : maxTaskFailuresPerNode is <:NUM:>\n",
      "\t\t\"blacklistDisablePercent\" (cluster_count=1)\n",
      "\t\t\tID=76    : size=69        : blacklistDisablePercent is <:NUM:>\n",
      "\t\t\"maxContainerCapability:\" (cluster_count=1)\n",
      "\t\t\tID=78    : size=69        : maxContainerCapability: <memory:<:NUM:>, vCores:<:NUM:>>\n",
      "\t\t\"yarn.client.max-cached-nodemanagers-proxies\" (cluster_count=1)\n",
      "\t\t\tID=81    : size=69        : yarn.client.max-cached-nodemanagers-proxies : <:NUM:>\n",
      "\t\t\"Assigned\" (cluster_count=2)\n",
      "\t\t\tID=99    : size=73        : Assigned to reduce\n",
      "\t\t\tID=246   : size=8         : Assigned from earlierFailedMaps\n",
      "\t\t\"Stopped\" (cluster_count=1)\n",
      "\t\t\tID=137   : size=48        : Stopped JobHistoryEventHandler. super.stop()\n",
      "\t\t\"TaskHeartbeatHandler\" (cluster_count=1)\n",
      "\t\t\tID=145   : size=48        : TaskHeartbeatHandler thread interrupted\n",
      "\t\t\"Ramping\" (cluster_count=1)\n",
      "\t\t\tID=159   : size=31        : Ramping up <:NUM:>\n",
      "\t\t\"Exception\" (cluster_count=2)\n",
      "\t\t\tID=197   : size=30        : Exception in createBlockOutputStream\n",
      "\t\t\tID=269   : size=3         : Exception while unregistering\n",
      "\t\t\"Excluding\" (cluster_count=1)\n",
      "\t\t\tID=200   : size=30        : Excluding datanode <:IP:>:<:NUM:>\n",
      "\t\t\"Graceful\" (cluster_count=1)\n",
      "\t\t\tID=271   : size=3         : Graceful stop failed\n",
      "\t<L=8>\n",
      "\t\t\"Kind:\" (cluster_count=1)\n",
      "\t\t\tID=5     : size=909       : Kind: mapreduce.job, Service: job <:NUM:> <:NUM:>, Ident: <:*:>\n",
      "\t\t\"Task\" (cluster_count=1)\n",
      "\t\t\tID=27    : size=616       : Task 'attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>' done.\n",
      "\t\t\"Successfully\" (cluster_count=1)\n",
      "\t\t\tID=32    : size=6         : Successfully connected to /<:IP:>:<:NUM:> for BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:>\n",
      "\t\t\"Created\" (cluster_count=1)\n",
      "\t\t\tID=37    : size=69        : Created MRAppMaster for application appattempt <:NUM:> <:NUM:> <:NUM:>\n",
      "\t\t\"job\" (cluster_count=1)\n",
      "\t\t\tID=54    : size=306       : job <:NUM:> <:*:> Transitioned from <:*:> to <:*:>\n",
      "\t\t\"Http\" (cluster_count=1)\n",
      "\t\t\tID=63    : size=69        : Http request log for http.requests.mapreduce is not defined\n",
      "\t\t\"All\" (cluster_count=1)\n",
      "\t\t\tID=95    : size=43        : All maps assigned. Ramping up all remaining reduces:<:NUM:>\n",
      "\t\t\"DFSOutputStream\" (cluster_count=1)\n",
      "\t\t\tID=115   : size=14        : DFSOutputStream ResponseProcessor exception for block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:>\n",
      "\t\t\"Reduce\" (cluster_count=1)\n",
      "\t\t\tID=148   : size=3841      : Reduce slow start threshold not met. completedMapsForReduceSlowstart <:NUM:>\n",
      "\t\t\"Received\" (cluster_count=1)\n",
      "\t\t\tID=150   : size=826       : Received completed container container <:NUM:> <:NUM:> <:NUM:> <:NUM:>\n",
      "\t\t\"assigned\" (cluster_count=1)\n",
      "\t\t\tID=176   : size=472       : assigned <:NUM:> of <:NUM:> to <:*:> to fetcher#<:NUM:>\n",
      "\t\t\"java.net.NoRouteToHostException:\" (cluster_count=1)\n",
      "\t\t\tID=203   : size=5330      : java.net.NoRouteToHostException: No route to host: no further information\n",
      "\t\t\"Unable\" (cluster_count=1)\n",
      "\t\t\tID=247   : size=4         : Unable to parse prior job history, aborting recovery\n",
      "\t\t\"Resource\" (cluster_count=1)\n",
      "\t\t\tID=277   : size=2         : Resource Manager doesn't recognize AttemptId: application <:NUM:> <:NUM:>\n",
      "\t\t\"Error\" (cluster_count=1)\n",
      "\t\t\tID=280   : size=1         : Error closing writer for JobID: job <:NUM:> <:NUM:>\n",
      "\t\t\"When\" (cluster_count=1)\n",
      "\t\t\tID=283   : size=1         : When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n",
      "\t<L=9>\n",
      "\t\t\"Sleeping\" (cluster_count=1)\n",
      "\t\t\tID=6     : size=909       : Sleeping for 0ms before retrying again. Got null now.\n",
      "\t\t\"bufstart\" (cluster_count=1)\n",
      "\t\t\tID=19    : size=5339      : bufstart = <:NUM:>; bufend = <:NUM:>; bufvoid = <:NUM:>\n",
      "\t\t\"kvstart\" (cluster_count=1)\n",
      "\t\t\tID=20    : size=5339      : kvstart = <:NUM:>(<:NUM:>); kvend = <:NUM:>(<:NUM:>); length = <:NUM:>/<:NUM:>\n",
      "\t\t\"Read\" (cluster_count=1)\n",
      "\t\t\tID=47    : size=96        : Read from history task task <:NUM:> <:NUM:> m <:NUM:>\n",
      "\t\t\"Adding\" (cluster_count=1)\n",
      "\t\t\tID=50    : size=69        : Adding job token for job <:NUM:> <:NUM:> to jobTokenSecretManager\n",
      "\t\t\"MRAppMaster\" (cluster_count=1)\n",
      "\t\t\tID=55    : size=69        : MRAppMaster launching normal, non-uberized, multi-container job job <:NUM:> <:NUM:>.\n",
      "\t\t\"Added\" (cluster_count=1)\n",
      "\t\t\tID=65    : size=138       : Added filter AM PROXY FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context <:*:>\n",
      "\t\t\"Upper\" (cluster_count=1)\n",
      "\t\t\tID=80    : size=69        : Upper limit on the thread pool size is <:NUM:>\n",
      "\t\t\"Done\" (cluster_count=1)\n",
      "\t\t\tID=123   : size=605       : Done acknowledgement from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "\t\t\"Copying\" (cluster_count=1)\n",
      "\t\t\tID=133   : size=47        : Copying hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist to hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=182   : size=19        : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>:\n",
      "\t\t\"Merging\" (cluster_count=1)\n",
      "\t\t\tID=192   : size=56        : Merging <:NUM:> segments, <:NUM:> bytes from memory into reduce\n",
      "\t\t\"Last\" (cluster_count=1)\n",
      "\t\t\tID=209   : size=5         : Last retry, killing attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "\t\t\"Reduce\" (cluster_count=1)\n",
      "\t\t\tID=225   : size=1         : Reduce preemption successful attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "\t\t\"Connection\" (cluster_count=1)\n",
      "\t\t\tID=231   : size=1         : Connection retry failed with <:NUM:> attempts in <:NUM:> seconds\n",
      "\t\t\"Failed\" (cluster_count=1)\n",
      "\t\t\tID=232   : size=1         : Failed to connect to MININT-FNANLI5.fareast.corp.microsoft.com:<:NUM:> with <:NUM:> map outputs\n",
      "\t\t\"java.io.IOException:\" (cluster_count=1)\n",
      "\t\t\tID=242   : size=18        : java.io.IOException: There is not enough space on the disk\n",
      "\t\t\"Service\" (cluster_count=1)\n",
      "\t\t\tID=281   : size=2         : Service <:*:> failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n",
      "\t\t\"Shuffle\" (cluster_count=1)\n",
      "\t\t\tID=298   : size=2         : Shuffle failed : local error on this node: 04DN8IQ/<:IP:>\n",
      "\t\t\"IPC\" (cluster_count=1)\n",
      "\t\t\tID=302   : size=1         : IPC Server handler <:NUM:> on <:NUM:> caught an exception\n",
      "\t<L=7>\n",
      "\t\t\"ProcfsBasedProcessTree\" (cluster_count=1)\n",
      "\t\t\tID=9     : size=905       : ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "\t\t\"(RESET)\" (cluster_count=1)\n",
      "\t\t\tID=22    : size=4579      : (RESET) equator <:NUM:> kv <:NUM:>(<:NUM:>) kvi <:NUM:>(<:NUM:>)\n",
      "\t\t\"Starting\" (cluster_count=1)\n",
      "\t\t\tID=57    : size=138       : Starting Socket Reader #<:NUM:> for port <:NUM:>\n",
      "\t\t\"Reduce\" (cluster_count=1)\n",
      "\t\t\tID=94    : size=65        : Reduce slow start threshold reached. Scheduling reduces.\n",
      "\t\t\"Launching\" (cluster_count=1)\n",
      "\t\t\tID=106   : size=906       : Launching attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "\t\t\"ATTEMPT\" (cluster_count=1)\n",
      "\t\t\tID=109   : size=905       : ATTEMPT START task <:NUM:> <:NUM:> <:*:> <:NUM:>\n",
      "\t\t\"Auth\" (cluster_count=1)\n",
      "\t\t\tID=110   : size=962       : Auth successful for job <:NUM:> <:NUM:> (auth:SIMPLE)\n",
      "\t\t\"KILLING\" (cluster_count=1)\n",
      "\t\t\tID=125   : size=851       : KILLING attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "\t\t\"Waiting\" (cluster_count=1)\n",
      "\t\t\tID=140   : size=45        : Waiting for application to be successfully unregistered.\n",
      "\t\t\"Deleting\" (cluster_count=1)\n",
      "\t\t\tID=142   : size=45        : Deleting staging directory hdfs://msra-sa-<:NUM:>:<:NUM:> /tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>\n",
      "\t\t\"We\" (cluster_count=1)\n",
      "\t\t\tID=156   : size=255       : We launched <:NUM:> speculations. Sleeping <:NUM:> milliseconds.\n",
      "\t\t\"Merging\" (cluster_count=1)\n",
      "\t\t\tID=191   : size=56        : Merging <:NUM:> files, <:NUM:> bytes from disk\n",
      "\t\t\"Bad\" (cluster_count=1)\n",
      "\t\t\tID=198   : size=30        : Bad connect ack with firstBadLink as <:IP:>:<:NUM:>\n",
      "\t\t\"Address\" (cluster_count=1)\n",
      "\t\t\tID=212   : size=5795      : Address change detected. Old: msra-sa-<:NUM:>/<:IP:>:<:NUM:> New: msra-sa-<:NUM:>:<:NUM:>\n",
      "\t\t\"Preempting\" (cluster_count=1)\n",
      "\t\t\tID=224   : size=1         : Preempting attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "\t\t\"java.net.ConnectException:\" (cluster_count=1)\n",
      "\t\t\tID=228   : size=1         : java.net.ConnectException: Connection timed out: no further information\n",
      "\t\t\"No\" (cluster_count=1)\n",
      "\t\t\tID=251   : size=20        : No route to host: no further information\n",
      "\t\t\"Could\" (cluster_count=1)\n",
      "\t\t\tID=258   : size=6         : Could not contact RM after <:NUM:> milliseconds.\n",
      "\t\t\"In\" (cluster_count=1)\n",
      "\t\t\tID=293   : size=1         : In stop, writing event MAP ATTEMPT FAILED\n",
      "\t<L=2>\n",
      "\t\t\"mapreduce.task.io.sort.mb:\" (cluster_count=1)\n",
      "\t\t\tID=13    : size=834       : mapreduce.task.io.sort.mb: <:NUM:>\n",
      "\t\t\"Started\" (cluster_count=1)\n",
      "\t\t\tID=70    : size=69        : Started HttpServer2$SelectChannelConnectorWithSafeStartup@<:IP:>:<:NUM:>\n",
      "\t\t\"queue:\" (cluster_count=1)\n",
      "\t\t\tID=79    : size=69        : queue: default\n",
      "\t\t\"reduceResourceRequest:<memory:<:NUM:>,\" (cluster_count=1)\n",
      "\t\t\tID=91    : size=69        : reduceResourceRequest:<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "\t\t\"mapResourceRequest:<memory:<:NUM:>,\" (cluster_count=1)\n",
      "\t\t\tID=147   : size=65        : mapResourceRequest:<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "\t\t\"TIMED\" (cluster_count=1)\n",
      "\t\t\tID=207   : size=20        : TIMED WAITING\n",
      "\t\t\"DFS\" (cluster_count=1)\n",
      "\t\t\tID=256   : size=3         : DFS Read\n",
      "\t\t\"DataStreamer\" (cluster_count=1)\n",
      "\t\t\tID=257   : size=3         : DataStreamer Exception\n",
      "\t\t\"java.net.UnknownHostException:\" (cluster_count=1)\n",
      "\t\t\tID=267   : size=24        : java.net.UnknownHostException: <:*:>\n",
      "\t\t\"MapCompletionEvents\" (cluster_count=1)\n",
      "\t\t\tID=273   : size=1         : MapCompletionEvents reques\n",
      "\t<L=5>\n",
      "\t\t\"Starting\" (cluster_count=1)\n",
      "\t\t\tID=23    : size=644       : Starting flush of map output\n",
      "\t\t\"Connection\" (cluster_count=1)\n",
      "\t\t\tID=33    : size=2         : Connection refused: no further information\n",
      "\t\t\"MapTask\" (cluster_count=1)\n",
      "\t\t\tID=36    : size=302       : MapTask metrics system shutdown complete.\n",
      "\t\t\"OutputCommitter\" (cluster_count=1)\n",
      "\t\t\tID=40    : size=69        : OutputCommitter set in config null\n",
      "\t\t\"Logging\" (cluster_count=1)\n",
      "\t\t\tID=62    : size=69        : Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n",
      "\t\t\"Added\" (cluster_count=1)\n",
      "\t\t\tID=64    : size=69        : Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n",
      "\t\t\"Jetty\" (cluster_count=1)\n",
      "\t\t\tID=67    : size=69        : Jetty bound to port <:NUM:>\n",
      "\t\t\"JOB\" (cluster_count=1)\n",
      "\t\t\tID=73    : size=69        : JOB CREATE job <:NUM:> <:NUM:>\n",
      "\t\t\"Connecting\" (cluster_count=1)\n",
      "\t\t\tID=77    : size=69        : Connecting to ResourceManager at <:*:>\n",
      "\t\t\"Putting\" (cluster_count=1)\n",
      "\t\t\tID=104   : size=69        : Putting shuffle token in serviceData\n",
      "\t\t\"Copied\" (cluster_count=1)\n",
      "\t\t\tID=134   : size=94        : Copied to done location: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "\t\t\"Moved\" (cluster_count=1)\n",
      "\t\t\tID=136   : size=141       : Moved tmp to done: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "\t\t\"Process\" (cluster_count=1)\n",
      "\t\t\tID=204   : size=5         : Process Thread Dump: Communication exception\n",
      "\t\t\"Runnning\" (cluster_count=1)\n",
      "\t\t\tID=211   : size=6         : Runnning cleanup for the task\n",
      "\t\t\"Ramping\" (cluster_count=1)\n",
      "\t\t\tID=222   : size=100       : Ramping down all scheduled reduces:<:NUM:>\n",
      "\t\t\"ReduceTask\" (cluster_count=1)\n",
      "\t\t\tID=229   : size=15        : ReduceTask metrics system shutdown complete.\n",
      "\t\t\"<:NUM:>\" (cluster_count=1)\n",
      "\t\t\tID=237   : size=10        : <:NUM:> failures on node <:*:>\n",
      "\t\t\"Incompatible\" (cluster_count=1)\n",
      "\t\t\tID=248   : size=8         : Incompatible event log version: null\n",
      "\t\t\"Error\" (cluster_count=1)\n",
      "\t\t\tID=260   : size=4         : Error writing History Event: <:*:>\n",
      "\t\t\"Thread\" (cluster_count=1)\n",
      "\t\t\tID=261   : size=3         : Thread Thread[eventHandlingThread,<:NUM:>,main] threw an Exception.\n",
      "\t<L=14>\n",
      "\t\t\"Down\" (cluster_count=1)\n",
      "\t\t\tID=25    : size=678       : Down to the last merge-pass, with <:NUM:> segments left of total size: <:NUM:> bytes\n",
      "\t\t\"Recovery\" (cluster_count=1)\n",
      "\t\t\tID=45    : size=15        : Recovery is enabled. Will try to recover from previous life on best effort basis.\n",
      "\t\t\"Not\" (cluster_count=1)\n",
      "\t\t\tID=51    : size=69        : Not uberizing job <:NUM:> <:NUM:> because: not enabled; too many maps; too much input;\n",
      "\t\t\"Input\" (cluster_count=1)\n",
      "\t\t\tID=52    : size=69        : Input size for job job <:NUM:> <:NUM:> = <:NUM:>. Number of splits = <:NUM:>\n",
      "\t\t\"Recovering\" (cluster_count=1)\n",
      "\t\t\tID=83    : size=96        : Recovering task task <:NUM:> <:NUM:> m <:NUM:> from prior app attempt, status was SUCCEEDED\n",
      "\t\t\"Assigned\" (cluster_count=1)\n",
      "\t\t\tID=100   : size=906       : Assigned container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> to attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "\t\t\"Shuffle\" (cluster_count=1)\n",
      "\t\t\tID=108   : size=906       : Shuffle port returned by ContainerManager for attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> : <:NUM:>\n",
      "\t\t\"Error\" (cluster_count=1)\n",
      "\t\t\tID=117   : size=14        : Error Recovery for block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> in pipeline <:IP:>:<:NUM:>, <:IP:>:<:NUM:>: bad datanode <:IP:>:<:NUM:>\n",
      "\t\t\"attempt\" (cluster_count=2)\n",
      "\t\t\tID=120   : size=119       : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:>\n",
      "\t\t\tID=154   : size=1611      : attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> <:*:> <:*:> <:*:> <:*:>\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=160   : size=733       : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: Container killed by the ApplicationMaster.\n",
      "\t\t\"Killing\" (cluster_count=1)\n",
      "\t\t\tID=167   : size=55        : Killing taskAttempt:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> because it is running on unusable <:*:>\n",
      "\t\t\"for\" (cluster_count=1)\n",
      "\t\t\tID=181   : size=381       : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "\t<L=15>\n",
      "\t\t\"Task:attempt\" (cluster_count=1)\n",
      "\t\t\tID=26    : size=625       : Task:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> is done. And is in the process of committing\n",
      "\t\t\"Event\" (cluster_count=1)\n",
      "\t\t\tID=89    : size=69        : Event Writer setup for JobId: job <:NUM:> <:NUM:>, File: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist\n",
      "\t\t\"attempt\" (cluster_count=1)\n",
      "\t\t\tID=124   : size=47        : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> TaskAttempt Transitioned from COMMIT PENDING to SUCCESS CONTAINER CLEANUP\n",
      "\t\t\"Retrying\" (cluster_count=1)\n",
      "\t\t\tID=165   : size=1048      : Retrying connect to server: <:*:> Already tried <:NUM:> time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=<:NUM:>, sleepTime=<:NUM:> MILLISECONDS)\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=168   : size=55        : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: Container released on a *lost* node\n",
      "\t\t\"TaskAttempt\" (cluster_count=1)\n",
      "\t\t\tID=221   : size=25        : TaskAttempt killed because it ran on unusable node <:*:> AttemptId:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "\t<L=0> (cluster_count=1)\n",
      "\t\tID=28    : size=201731    : \n",
      "\t<L=19>\n",
      "\t\t\"Failed\" (cluster_count=1)\n",
      "\t\t\tID=31    : size=5         : Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information\n",
      "\t\t\"fetcher#<:NUM:>\" (cluster_count=1)\n",
      "\t\t\tID=179   : size=664       : fetcher#<:NUM:> about to shuffle output of map attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> decomp: <:NUM:> len: <:NUM:> to DISK\n",
      "\t\t\"Slow\" (cluster_count=1)\n",
      "\t\t\tID=183   : size=9         : Slow ReadProcessor read fields took <:*:> (threshold=30000ms); ack: seqno: <:NUM:> status: SUCCESS status: ERROR downstreamAckTimeNanos: <:NUM:>, targets: [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>]\n",
      "\t\t\"for\" (cluster_count=1)\n",
      "\t\t\tID=187   : size=29        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "\t\t\"Task:\" (cluster_count=1)\n",
      "\t\t\tID=238   : size=6         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.io.IOException: There is not enough space on the disk\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=239   : size=16        : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: <:*:> java.io.IOException: There is not enough space on the disk\n",
      "\t\t\"Task\" (cluster_count=1)\n",
      "\t\t\tID=275   : size=1         : Task attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> failed : org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\n",
      "\t\t\"Exception\" (cluster_count=1)\n",
      "\t\t\tID=300   : size=1         : Exception running child : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\n",
      "\t<L=18>\n",
      "\t\t\"Failed\" (cluster_count=1)\n",
      "\t\t\tID=34    : size=1         : Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.ConnectException: Connection refused: no further information\n",
      "\t\t\"Service\" (cluster_count=1)\n",
      "\t\t\tID=294   : size=2         : Service <:*:> failed in state STOPPED; cause: org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\n",
      "\t<L=24>\n",
      "\t\t\"Kind:\" (cluster_count=1)\n",
      "\t\t\tID=38    : size=69        : Kind: YARN AM RM TOKEN, Service: , Ident: (appAttemptId { application id { id: <:NUM:> cluster timestamp: <:NUM:> } attemptId: <:NUM:> } keyId: <:NUM:>)\n",
      "\t\t\"for\" (cluster_count=1)\n",
      "\t\t\tID=210   : size=21        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "\t\t\"Communication\" (cluster_count=1)\n",
      "\t\t\tID=227   : size=1         : Communication exception: java.net.ConnectException: Call From MSRA-SA-<:NUM:>/<:IP:> to minint-fnanli5.fareast.corp.microsoft.com:<:NUM:> failed on connection exception: java.net.ConnectException: Connection timed out: no further information; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused\n",
      "\t<L=11>\n",
      "\t\t\"Emitting\" (cluster_count=1)\n",
      "\t\t\tID=44    : size=69        : Emitting job history data to the timeline server is not enabled\n",
      "\t\t\"Previous\" (cluster_count=1)\n",
      "\t\t\tID=46    : size=19        : Previous history file is at hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist\n",
      "\t\t\"Extract\" (cluster_count=1)\n",
      "\t\t\tID=69    : size=69        : Extract jar:file:/D:/hadoop-<:NUM:>.<:NUM:>.<:NUM:>-localbox/share/hadoop/yarn/hadoop-yarn-common-<:NUM:>.<:NUM:>.<:NUM:>-SNAPSHOT.jar!/webapps/mapreduce to <:*:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> mapreduce <:*:>\n",
      "\t\t\"task\" (cluster_count=1)\n",
      "\t\t\tID=88    : size=2074      : task <:NUM:> <:NUM:> <:*:> <:NUM:> Task Transitioned from <:*:> to <:*:>\n",
      "\t\t\"The\" (cluster_count=1)\n",
      "\t\t\tID=101   : size=138       : The <:*:> file on the remote FS is <:*:> <:NUM:> <:*:>\n",
      "\t\t\"Bad\" (cluster_count=1)\n",
      "\t\t\tID=116   : size=14        : Bad response ERROR for block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> from datanode <:IP:>:<:NUM:>\n",
      "\t\t\"Scheduling\" (cluster_count=1)\n",
      "\t\t\tID=157   : size=194       : Scheduling a redundant attempt for task task <:NUM:> <:NUM:> m <:NUM:>\n",
      "\t\t\"Issuing\" (cluster_count=1)\n",
      "\t\t\tID=161   : size=182       : Issuing kill to other attempt attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "\t\t\"Could\" (cluster_count=1)\n",
      "\t\t\tID=163   : size=216       : Could not delete <:*:> temporary/<:NUM:>/ temporary/attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "\t\t\"java.io.IOException:\" (cluster_count=1)\n",
      "\t\t\tID=170   : size=55        : java.io.IOException: An existing connection was forcibly closed by the remote host\n",
      "\t\t\"Going\" (cluster_count=1)\n",
      "\t\t\tID=223   : size=100       : Going to preempt <:NUM:> due to lack of space for maps\n",
      "\t\t\"DFS\" (cluster_count=1)\n",
      "\t\t\tID=254   : size=6         : DFS chooseDataNode: got # <:NUM:> IOException, will wait for <:NUM:>.<:NUM:> msec.\n",
      "\t\t\"Error\" (cluster_count=1)\n",
      "\t\t\tID=259   : size=3         : Error communicating with RM: Could not contact RM after <:NUM:> milliseconds.\n",
      "\t\t\"Skipping\" (cluster_count=1)\n",
      "\t\t\tID=270   : size=3         : Skipping cleaning up the staging dir. assuming AM will be retried.\n",
      "\t\t\"Task\" (cluster_count=1)\n",
      "\t\t\tID=292   : size=2         : Task cleanup failed for attempt attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "\t<L=10>\n",
      "\t\t\"Number\" (cluster_count=1)\n",
      "\t\t\tID=53    : size=69        : Number of reduces for job job <:NUM:> <:NUM:> = <:NUM:>\n",
      "\t\t\"Task\" (cluster_count=1)\n",
      "\t\t\tID=87    : size=701       : Task succeeded with attempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "\t\t\"Commit-pending\" (cluster_count=1)\n",
      "\t\t\tID=118   : size=47        : Commit-pending state update from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "\t\t\"Commit\" (cluster_count=1)\n",
      "\t\t\tID=121   : size=47        : Commit go/no-go request from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "\t\t\"Result\" (cluster_count=1)\n",
      "\t\t\tID=122   : size=47        : Result of canCommit for attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>:true\n",
      "\t\t\"We\" (cluster_count=1)\n",
      "\t\t\tID=127   : size=50        : We are finishing cleanly so this is the last retry\n",
      "\t\t\"Stopping\" (cluster_count=1)\n",
      "\t\t\tID=132   : size=52        : Stopping JobHistoryEventHandler. Size of the outstanding queue size is <:NUM:>\n",
      "\t\t\"Copying\" (cluster_count=1)\n",
      "\t\t\tID=135   : size=47        : Copying hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:> conf.xml to hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "\t\t\"An\" (cluster_count=1)\n",
      "\t\t\tID=153   : size=218       : An existing connection was forcibly closed by the remote host\n",
      "\t\t\"DefaultSpeculator.addSpeculativeAttempt\" (cluster_count=1)\n",
      "\t\t\tID=155   : size=255       : DefaultSpeculator.addSpeculativeAttempt -- we are speculating task <:NUM:> <:NUM:> m <:NUM:>\n",
      "\t\t\"Retrying\" (cluster_count=1)\n",
      "\t\t\tID=164   : size=2517      : Retrying connect to server: <:*:> Already tried <:NUM:> time(s); maxRetries=<:NUM:>\n",
      "\t\t\"attempt\" (cluster_count=1)\n",
      "\t\t\tID=174   : size=414       : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>: Got <:NUM:> new map-outputs\n",
      "\t\t\"finalMerge\" (cluster_count=1)\n",
      "\t\t\tID=190   : size=56        : finalMerge called with <:NUM:> in-memory map-outputs and <:NUM:> on-disk map-outputs\n",
      "\t\t\"Merging\" (cluster_count=1)\n",
      "\t\t\tID=226   : size=7         : Merging <:NUM:> intermediate segments out of a total of <:NUM:>\n",
      "\t\t\"org.apache.hadoop.fs.FSError:\" (cluster_count=1)\n",
      "\t\t\tID=241   : size=17        : org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\n",
      "\t\t\"Attempt\" (cluster_count=1)\n",
      "\t\t\tID=295   : size=4         : Attempt to process a enum when a union was expected.\n",
      "\t\t\"Exception\" (cluster_count=1)\n",
      "\t\t\tID=297   : size=2         : Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\n",
      "\t<L=1> (cluster_count=7)\n",
      "\t\tID=68    : size=69        : jetty-<:NUM:>.<:NUM:>.<:NUM:>\n",
      "\t\tID=74    : size=69        : nodeBlacklistingEnabled:true\n",
      "\t\tID=205   : size=18        : WAITING\n",
      "\t\tID=206   : size=126       : <:NUM:>\n",
      "\t\tID=208   : size=25        : RUNNABLE\n",
      "\t<L=17>\n",
      "\t\t\"TaskAttempt:\" (cluster_count=1)\n",
      "\t\t\tID=85    : size=1033      : TaskAttempt: [attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>] using containerId: [container <:NUM:> <:NUM:> <:NUM:> <:NUM:> on NM: <:*:>\n",
      "\t\t\"Saved\" (cluster_count=1)\n",
      "\t\t\tID=195   : size=48        : Saved output of task 'attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>' to <:*:> temporary/<:NUM:>/task <:NUM:> <:NUM:> r <:NUM:>\n",
      "\t\t\"When\" (cluster_count=1)\n",
      "\t\t\tID=296   : size=1         : When stopping the service JobHistoryEventHandler : org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\n",
      "\t<L=12>\n",
      "\t\t\"attempt\" (cluster_count=1)\n",
      "\t\t\tID=86    : size=2921      : attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> to <:*:>\n",
      "\t\t\"Progress\" (cluster_count=1)\n",
      "\t\t\tID=114   : size=45882     : Progress of TaskAttempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> is : <:*:>\n",
      "\t\t\"Container\" (cluster_count=1)\n",
      "\t\t\tID=151   : size=19        : Container complete event for unknown container id container <:NUM:> <:NUM:> <:NUM:> <:NUM:>\n",
      "\t\t\"completedMapPercent\" (cluster_count=1)\n",
      "\t\t\tID=158   : size=531       : completedMapPercent <:NUM:>.<:NUM:> totalResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> finalMapResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> finalReduceResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> netScheduledMapResource:<memory:<:NUM:>, vCores:<:NUM:>> netScheduledReduceResource:<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "\t\t\"Ignoring\" (cluster_count=1)\n",
      "\t\t\tID=180   : size=84        : Ignoring obsolete output of <:*:> map-task: 'attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>'\n",
      "\t\t\"Read\" (cluster_count=1)\n",
      "\t\t\tID=185   : size=649       : Read <:NUM:> bytes from map-output for attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "\t\t\"Task\" (cluster_count=1)\n",
      "\t\t\tID=194   : size=48        : Task attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> is allowed to commit now\n",
      "\t\t\"Reporting\" (cluster_count=1)\n",
      "\t\t\tID=234   : size=1         : Reporting fetch failure for attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> to jobtracker.\n",
      "\t\t\"Added\" (cluster_count=1)\n",
      "\t\t\tID=244   : size=10        : Added attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> to list of failed maps\n",
      "\t\t\"Could\" (cluster_count=1)\n",
      "\t\t\tID=249   : size=4         : Could not parse the old history file. Will not have old AMinfos\n",
      "\t\t\"Found\" (cluster_count=1)\n",
      "\t\t\tID=263   : size=2         : Found jobId job <:NUM:> <:NUM:> to have not been closed. Will close\n",
      "\t\t\"Error\" (cluster_count=1)\n",
      "\t\t\tID=276   : size=2         : Error communicating with RM: Resource Manager doesn't recognize AttemptId: application <:NUM:> <:NUM:>\n",
      "\t\t\"org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException:\" (cluster_count=1)\n",
      "\t\t\tID=278   : size=2         : org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException: Application attempt appattempt <:NUM:> <:NUM:> <:NUM:> doesn't exist in ApplicationMasterService cache.\n",
      "\t\t\"org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException):\" (cluster_count=1)\n",
      "\t\t\tID=279   : size=2         : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException): Application attempt appattempt <:NUM:> <:NUM:> <:NUM:> doesn't exist in ApplicationMasterService cache.\n",
      "\t<L=13>\n",
      "\t\t\"Before\" (cluster_count=1)\n",
      "\t\t\tID=92    : size=797       : Before Scheduling: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\n",
      "\t\t\"After\" (cluster_count=1)\n",
      "\t\t\tID=96    : size=1249      : After Scheduling: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\n",
      "\t\t\"getResources()\" (cluster_count=1)\n",
      "\t\t\tID=97    : size=899       : getResources() for application <:NUM:> <:NUM:>: ask=<:NUM:> release= <:NUM:> newContainers=<:NUM:> finishedContainers=<:NUM:> resourcelimit=<memory:<:NUM:>, vCores:<:NUM:>> knownNMs=<:NUM:>\n",
      "\t\t\"Adding\" (cluster_count=1)\n",
      "\t\t\tID=102   : size=69        : Adding #<:NUM:> tokens and #<:NUM:> secret keys for NM use for launching container\n",
      "\t\t\"JVM\" (cluster_count=1)\n",
      "\t\t\tID=111   : size=902       : JVM with ID : jvm <:NUM:> <:NUM:> <:*:> <:NUM:> asked for a task\n",
      "\t\t\"MapCompletionEvents\" (cluster_count=1)\n",
      "\t\t\tID=113   : size=18140     : MapCompletionEvents request from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>. startIndex <:NUM:> maxEvents <:NUM:>\n",
      "\t\t\"attempt\" (cluster_count=1)\n",
      "\t\t\tID=119   : size=47        : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> TaskAttempt Transitioned from RUNNING to COMMIT PENDING\n",
      "\t\t\"Final\" (cluster_count=1)\n",
      "\t\t\tID=141   : size=48        : Final Stats: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\n",
      "\t\t\"Task:\" (cluster_count=1)\n",
      "\t\t\tID=240   : size=3         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.io.IOException: Spill failed\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=243   : size=6         : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: java.io.IOException: Spill failed\n",
      "\t\t\"cleanup\" (cluster_count=1)\n",
      "\t\t\tID=266   : size=12        : cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: <:*:>\n",
      "\t<L=21>\n",
      "\t\t\"Processing\" (cluster_count=1)\n",
      "\t\t\tID=105   : size=1744      : Processing the event EventType: CONTAINER REMOTE <:*:> for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> taskAttempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "\t\t\"Failed\" (cluster_count=1)\n",
      "\t\t\tID=217   : size=487       : Failed on local exception: <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> Host Details : local host is: \"MININT-FNANLI5/<:IP:>\"; destination host is: \"msra-sa-<:NUM:>\":<:NUM:>;\n",
      "\t\t\"WordCount\" (cluster_count=1)\n",
      "\t\t\tID=274   : size=1         : WordCount and PageRank. Each application has been run for several times, simulating both normal and abnormal cases with injected specific failures.\n",
      "\t\t\"Task:\" (cluster_count=1)\n",
      "\t\t\tID=288   : size=2         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - failed due to FSError: java.io.IOException: There is not enough space on the disk\n",
      "\t<L=16>\n",
      "\t\t\"JVM\" (cluster_count=1)\n",
      "\t\t\tID=112   : size=902       : JVM with ID: jvm <:NUM:> <:NUM:> <:*:> <:NUM:> given task: attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "\t\t\"attempt\" (cluster_count=2)\n",
      "\t\t\tID=162   : size=216       : attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> CONTAINER CLEANUP to <:*:> TASK CLEANUP\n",
      "\t\t\tID=178   : size=667       : attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Shuffling to disk since <:NUM:> is greater than maxSingleShuffleLimit (<:NUM:>)\n",
      "\t\t\"Failed\" (cluster_count=1)\n",
      "\t\t\tID=213   : size=5300      : Failed to renew lease for [DFSClient NONMAPREDUCE <:NUM:> <:NUM:>] for <:NUM:> seconds. Will retry shortly ...\n",
      "\t\t\"Task:\" (cluster_count=1)\n",
      "\t\t\tID=286   : size=2         : Task: attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> - exited : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=287   : size=4         : Diagnostics report from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>: Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\n",
      "\t<L=47>\n",
      "\t\t\"Cannot\" (cluster_count=1)\n",
      "\t\t\tID=149   : size=12        : Cannot assign container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] for a map as either container memory less than required <memory:<:NUM:>, vCores:<:NUM:>> or no pending map tasks - maps.isEmpty=true\n",
      "\t<L=23>\n",
      "\t\t\"Socket\" (cluster_count=1)\n",
      "\t\t\tID=152   : size=217       : Socket Reader #<:NUM:> for port <:NUM:>: readAndProcess from client <:IP:> threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]\n",
      "\t<L=20>\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=166   : size=7         : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: AttemptID:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> Timed out after <:NUM:> secs\n",
      "\t\t\"Failed\" (cluster_count=1)\n",
      "\t\t\tID=252   : size=10        : Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information\n",
      "\t<L=29>\n",
      "\t\t\"Communication\" (cluster_count=2)\n",
      "\t\t\tID=169   : size=44        : Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\n",
      "\t\t\tID=202   : size=10        : Communication exception: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to <:IP:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "\t\t\"for\" (cluster_count=1)\n",
      "\t\t\tID=196   : size=15        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "\t<L=39>\n",
      "\t\t\"for\" (cluster_count=1)\n",
      "\t\t\tID=177   : size=8         : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "\t<L=34>\n",
      "\t\t\"for\" (cluster_count=1)\n",
      "\t\t\tID=188   : size=12        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "\t\t\"When\" (cluster_count=1)\n",
      "\t\t\tID=265   : size=1         : When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "\t<L=26>\n",
      "\t\t\"No\" (cluster_count=1)\n",
      "\t\t\tID=214   : size=5303      : No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "\t\t\"Failed\" (cluster_count=1)\n",
      "\t\t\tID=216   : size=7         : Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\n",
      "\t<L=35>\n",
      "\t\t\"Releasing\" (cluster_count=1)\n",
      "\t\t\tID=230   : size=7         : Releasing unassigned and invalid container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ]. RM may have assignment issues\n",
      "\t\t\"Service\" (cluster_count=1)\n",
      "\t\t\tID=264   : size=2         : Service <:*:> failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "\t<L=25>\n",
      "\t\t\"Task:\" (cluster_count=1)\n",
      "\t\t\tID=235   : size=1         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=236   : size=2         : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\n",
      "\t\t\"IPC\" (cluster_count=1)\n",
      "\t\t\tID=301   : size=1         : IPC Server handler <:NUM:> on <:NUM:>, call statusUpdate(attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>, org.apache.hadoop.mapred.MapTaskStatus@cdcdbf7), rpc version=<:NUM:>, client version=<:NUM:>, methodsFingerPrint=<:NUM:> from <:IP:>:<:NUM:> Call#<:NUM:> Retry#<:NUM:>: output error\n",
      "\t<L=31>\n",
      "\t\t\"Assigning\" (cluster_count=1)\n",
      "\t\t\tID=245   : size=8         : Assigning container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] to fast fail map\n",
      "\t\t\"Failure\" (cluster_count=1)\n",
      "\t\t\tID=250   : size=4         : Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\n",
      "\t\t\"Exception\" (cluster_count=1)\n",
      "\t\t\tID=284   : size=2         : Exception running child : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "\t<L=50>\n",
      "\t\t\"Could\" (cluster_count=1)\n",
      "\t\t\tID=253   : size=5         : Could not obtain BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> from any node: java.io.IOException: No live nodes contain block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> after checking nodes = [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>], ignoredNodes = null No live nodes contain current block Block locations: <:IP:>:<:NUM:> <:IP:>:<:NUM:> Dead nodes: <:IP:>:<:NUM:> <:IP:>:<:NUM:>. Will get new block locations from namenode and retry...\n",
      "\t<L=51>\n",
      "\t\t\"Could\" (cluster_count=1)\n",
      "\t\t\tID=255   : size=1         : Could not obtain BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> from any node: java.io.IOException: No live nodes contain block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> after checking nodes = [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>], ignoredNodes = null No live nodes contain current block Block locations: <:IP:>:<:NUM:> <:IP:>:<:NUM:> Dead nodes: <:IP:>:<:NUM:> <:IP:>:<:NUM:> <:IP:>:<:NUM:>. Will get new block locations from namenode and retry...\n",
      "\t<L=27>\n",
      "\t\t\"java.net.NoRouteToHostException:\" (cluster_count=1)\n",
      "\t\t\tID=262   : size=14        : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "\t<L=22>\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=268   : size=12        : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: <:*:>\n",
      "\t<L=30>\n",
      "\t\t\"Exception\" (cluster_count=1)\n",
      "\t\t\tID=285   : size=2         : Exception cleaning up: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "\t<L=44>\n",
      "\t\t\"for\" (cluster_count=1)\n",
      "\t\t\tID=289   : size=3         : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "\t<L=37>\n",
      "\t\t\"Task:\" (cluster_count=1)\n",
      "\t\t\tID=290   : size=2         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=291   : size=4         : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "total          : took    14.37 s (100.00%),    395,363 samples,   36.35 ms / 1000 samples,       27,508.28 hz\n",
      "mask           : took     7.35 s ( 51.14%),    395,363 samples,   18.59 ms / 1000 samples,       53,790.49 hz\n",
      "drain          : took     4.90 s ( 34.06%),    395,363 samples,   12.38 ms / 1000 samples,       80,753.37 hz\n",
      "tree_search    : took     1.66 s ( 11.52%),    395,363 samples,    4.19 ms / 1000 samples,      238,688.06 hz\n",
      "cluster_exist  : took     1.35 s (  9.38%),    395,061 samples,    3.41 ms / 1000 samples,      293,192.41 hz\n",
      "create_cluster : took     0.01 s (  0.05%),        302 samples,   25.41 ms / 1000 samples,       39,356.22 hz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from os.path import dirname\n",
    "\n",
    "config = TemplateMinerConfig()\n",
    "config.load(\"/Users/shuming/Documents/GitHub/loglizer/data/drain3.ini\")\n",
    "config.profiling_enabled = True\n",
    "template_miner = TemplateMiner(config=config)\n",
    "f = open(\"/Users/shuming/Downloads/merged_hadoop/merged.log\")             \n",
    "lines = f.readlines()              \n",
    "\n",
    "line_count = 0\n",
    "start_time = time.time()\n",
    "batch_start_time = start_time\n",
    "batch_size = 10000\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(message)s')\n",
    "\n",
    "for line in lines:\n",
    "    line = line.rstrip()\n",
    "    line = line.partition(\": \")[2]\n",
    "    result = template_miner.add_log_message(line)\n",
    "    line_count += 1\n",
    "    if line_count % batch_size == 0:\n",
    "        time_took = time.time() - batch_start_time\n",
    "        rate = batch_size / time_took\n",
    "        logger.info(f\"Processing line: {line_count}, rate {rate:.1f} lines/sec, \"\n",
    "                    f\"{len(template_miner.drain.clusters)} clusters so far.\")\n",
    "        batch_start_time = time.time()\n",
    "    if result[\"change_type\"] != \"none\":\n",
    "        result_json = json.dumps(result)\n",
    "        logger.info(f\"Input ({line_count}): \" + line)\n",
    "        logger.info(\"Result: \" + result_json)\n",
    "\n",
    "time_took = time.time() - start_time\n",
    "rate = line_count / time_took\n",
    "logger.info(f\"--- Done processing file in {time_took:.2f} sec. Total of {line_count} lines, rate {rate:.1f} lines/sec, \"\n",
    "            f\"{len(template_miner.drain.clusters)} clusters\")\n",
    "\n",
    "sorted_clusters = sorted(template_miner.drain.clusters, key=lambda it: it.size, reverse=True)\n",
    "for cluster in sorted_clusters:\n",
    "    logger.info(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal logs of Hadoop log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "def57a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_log = ['1445087491445_0005','1445087491445_0007', \n",
    "'1445175094696_0005','1445062781478_0011','1445062781478_0016','1445062781478_0019'\n",
    "'1445076437777_0002','1445076437777_0005','1445144423722_0021','1445144423722_0024'\n",
    "'1445182159119_0012']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30f89434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read from history task task <:NUM:> <:NUM:> m <:NUM:>\n",
      "47\n",
      "1445076437777_0005\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445087491445_0005\n",
      "1445076437777_0005\n",
      "1445087491445_0007\n",
      "1445094324383_0002\n",
      "1445087491445_0004\n",
      "1445087491445_0004\n",
      "1445087491445_0010\n",
      "1445094324383_0001\n",
      "1445087491445_0004\n",
      "1445094324383_0001\n",
      "1445182159119_0011\n",
      "1445062781478_0019\n",
      "1445087491445_0006\n",
      "1445182159119_0011\n",
      "1445087491445_0001\n",
      "1445144423722_0022\n",
      "1445182159119_0016\n",
      "1445076437777_0001\n",
      "1445076437777_0001\n",
      "1445144423722_0022\n",
      "1445087491445_0001\n",
      "1445182159119_0002\n",
      "1445182159119_0002\n",
      "1445182159119_0014\n",
      "1445144423722_0020\n",
      "1445094324383_0005\n",
      "1445144423722_0023\n",
      "1445144423722_0023\n",
      "1445094324383_0005\n",
      "1445087491445_0003\n",
      "1445087491445_0002\n",
      "1445076437777_0002\n",
      "1445182159119_0015\n",
      "1445182159119_0015\n",
      "1445094324383_0004\n",
      "1445087491445_0002\n",
      "1445062781478_0020\n",
      "1445182159119_0001\n",
      "1445062781478_0020\n",
      "1445062781478_0020\n",
      "1445076437777_0002\n",
      "1445144423722_0021\n",
      "1445182159119_0015\n",
      "1445087491445_0002\n",
      "1445094324383_0004\n",
      "1445087491445_0002\n",
      "1445182159119_0015\n",
      "1445087491445_0003\n",
      "1445144423722_0023\n",
      "1445087491445_0003\n",
      "1445094324383_0005\n",
      "1445094324383_0005\n",
      "1445144423722_0023\n",
      "1445144423722_0020\n",
      "1445144423722_0020\n",
      "1445182159119_0002\n",
      "1445182159119_0002\n",
      "1445182159119_0016\n",
      "1445144423722_0022\n",
      "1445076437777_0001\n",
      "1445087491445_0001\n",
      "1445087491445_0001\n",
      "1445076437777_0001\n",
      "1445144423722_0022\n",
      "1445182159119_0005\n",
      "1445062781478_0019\n",
      "1445182159119_0011\n",
      "1445182159119_0011\n",
      "1445087491445_0006\n",
      "1445062781478_0019\n",
      "1445094324383_0001\n",
      "1445087491445_0004\n",
      "1445094324383_0001\n",
      "1445087491445_0010\n",
      "1445087491445_0004\n",
      "1445062781478_0018\n",
      "1445087491445_0004\n",
      "1445094324383_0002\n",
      "1445087491445_0007\n",
      "\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445182159119_0012\n",
      "1445076437777_0005\n",
      "1445087491445_0005\n",
      "1445182159119_0012\n",
      "1445076437777_0005\n",
      "1445087491445_0005\n",
      "1445087491445_0005\n",
      "1445076437777_0005\n",
      "1445076437777_0004\n",
      "1445094324383_0002\n",
      "1445062781478_0018\n",
      "1445087491445_0004\n",
      "1445094324383_0001\n",
      "1445062781478_0018\n",
      "1445094324383_0001\n",
      "1445087491445_0004\n",
      "1445182159119_0004\n",
      "1445182159119_0011\n",
      "1445062781478_0019\n",
      "1445087491445_0006\n",
      "1445182159119_0011\n",
      "1445182159119_0005\n",
      "1445087491445_0001\n",
      "1445076437777_0001\n",
      "1445182159119_0016\n",
      "1445144423722_0022\n",
      "1445182159119_0016\n",
      "1445144423722_0022\n",
      "1445076437777_0001\n",
      "1445087491445_0001\n",
      "1445182159119_0002\n",
      "1445182159119_0002\n",
      "1445144423722_0020\n",
      "1445144423722_0020\n",
      "1445182159119_0003\n",
      "1445094324383_0005\n",
      "1445182159119_0014\n",
      "1445076437777_0003\n",
      "1445144423722_0023\n",
      "1445144423722_0023\n",
      "1445094324383_0005\n",
      "1445144423722_0021\n",
      "1445182159119_0015\n",
      "1445076437777_0002\n",
      "1445182159119_0015\n",
      "1445087491445_0002\n",
      "1445062781478_0020\n",
      "1445182159119_0001\n",
      "1445182159119_0001\n",
      "1445062781478_0020\n",
      "1445062781478_0020\n",
      "1445182159119_0015\n",
      "1445087491445_0002\n",
      "1445087491445_0002\n",
      "1445076437777_0002\n",
      "1445182159119_0015\n",
      "1445144423722_0021\n",
      "1445144423722_0023\n",
      "1445094324383_0005\n",
      "1445094324383_0005\n",
      "1445144423722_0023\n",
      "1445076437777_0003\n",
      "1445182159119_0014\n",
      "1445182159119_0003\n",
      "1445144423722_0020\n",
      "1445144423722_0020\n",
      "1445182159119_0002\n",
      "1445182159119_0002\n",
      "1445076437777_0001\n",
      "1445144423722_0022\n",
      "1445182159119_0016\n",
      "1445087491445_0001\n",
      "1445087491445_0001\n",
      "1445144423722_0022\n",
      "1445182159119_0016\n",
      "1445076437777_0001\n",
      "1445182159119_0005\n",
      "1445182159119_0011\n",
      "1445182159119_0011\n",
      "1445062781478_0019\n",
      "1445182159119_0004\n",
      "1445087491445_0004\n",
      "1445094324383_0001\n",
      "1445062781478_0018\n",
      "1445094324383_0001\n",
      "1445087491445_0004\n",
      "1445094324383_0002\n",
      "1445076437777_0004\n",
      "1445062781478_0018\n",
      "1445087491445_0005\n",
      "1445076437777_0005\n",
      "1445076437777_0005\n",
      "1445182159119_0012\n",
      "1445087491445_0005\n",
      "1445087491445_0005\n",
      "1445182159119_0012\n",
      "1445076437777_0005\n",
      "1445076437777_0005\n",
      "1445182159119_0012\n",
      "1445087491445_0005\n",
      "1445094324383_0002\n",
      "1445182159119_0013\n",
      "1445087491445_0010\n",
      "1445094324383_0002\n",
      "1445062781478_0018\n",
      "1445182159119_0004\n",
      "1445094324383_0001\n",
      "1445144423722_0024\n",
      "1445062781478_0018\n",
      "1445087491445_0004\n",
      "1445182159119_0005\n",
      "1445182159119_0005\n",
      "1445062781478_0019\n",
      "1445087491445_0006\n",
      "1445182159119_0011\n",
      "1445182159119_0011\n",
      "1445062781478_0019\n",
      "1445087491445_0006\n",
      "1445182159119_0002\n",
      "1445182159119_0002\n",
      "1445076437777_0001\n",
      "1445144423722_0022\n",
      "1445087491445_0001\n",
      "1445182159119_0016\n",
      "1445144423722_0022\n",
      "1445182159119_0003\n",
      "1445144423722_0020\n",
      "1445182159119_0017\n",
      "1445144423722_0020\n",
      "1445182159119_0014\n",
      "1445087491445_0003\n",
      "1445144423722_0023\n",
      "1445094324383_0005\n",
      "1445076437777_0003\n",
      "1445144423722_0023\n",
      "1445144423722_0020\n",
      "1445062781478_0020\n",
      "1445062781478_0020\n",
      "1445182159119_0001\n",
      "1445144423722_0021\n",
      "1445182159119_0015\n",
      "1445076437777_0002\n",
      "1445087491445_0002\n",
      "1445087491445_0002\n",
      "1445076437777_0002\n",
      "1445144423722_0021\n",
      "1445182159119_0015\n",
      "1445087491445_0002\n",
      "1445182159119_0015\n",
      "1445144423722_0021\n",
      "1445076437777_0002\n",
      "1445076437777_0002\n",
      "1445182159119_0015\n",
      "1445144423722_0021\n",
      "1445087491445_0002\n",
      "1445062781478_0020\n",
      "1445182159119_0001\n",
      "1445182159119_0001\n",
      "1445062781478_0020\n",
      "1445094324383_0005\n",
      "1445144423722_0020\n",
      "1445144423722_0023\n",
      "1445076437777_0003\n",
      "1445144423722_0023\n",
      "1445094324383_0005\n",
      "1445087491445_0003\n",
      "1445182159119_0014\n",
      "1445144423722_0020\n",
      "1445182159119_0017\n",
      "1445144423722_0020\n",
      "1445087491445_0001\n",
      "1445076437777_0001\n",
      "1445144423722_0022\n",
      "1445182159119_0016\n",
      "1445144423722_0022\n",
      "1445076437777_0001\n",
      "1445087491445_0001\n",
      "1445182159119_0002\n",
      "1445182159119_0002\n",
      "1445182159119_0011\n",
      "1445087491445_0006\n",
      "1445062781478_0019\n",
      "1445087491445_0006\n",
      "1445062781478_0019\n",
      "1445182159119_0011\n",
      "1445182159119_0005\n",
      "1445062781478_0018\n",
      "1445144423722_0024\n",
      "1445182159119_0004\n",
      "1445062781478_0018\n",
      "1445182159119_0013\n",
      "1445094324383_0002\n",
      "1445094324383_0002\n",
      "1445182159119_0012\n",
      "1445076437777_0005\n",
      "1445087491445_0005\n",
      "1445182159119_0012\n",
      "1445087491445_0005\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445076437777_0005\n",
      "1445182159119_0012\n",
      "1445182159119_0012\n",
      "1445076437777_0005\n",
      "1445087491445_0005\n",
      "1445087491445_0010\n",
      "1445087491445_0010\n",
      "1445087491445_0004\n",
      "1445094324383_0002\n",
      "1445094324383_0002\n",
      "1445087491445_0004\n",
      "1445087491445_0007\n",
      "1445094324383_0001\n",
      "1445182159119_0005\n",
      "1445182159119_0005\n",
      "1445062781478_0019\n",
      "1445087491445_0006\n",
      "1445182159119_0011\n",
      "1445062781478_0019\n",
      "1445182159119_0002\n",
      "1445182159119_0002\n",
      "1445144423722_0022\n",
      "1445182159119_0016\n",
      "1445076437777_0001\n",
      "1445087491445_0001\n",
      "1445087491445_0001\n",
      "1445182159119_0016\n",
      "1445182159119_0014\n",
      "1445144423722_0020\n",
      "1445144423722_0020\n",
      "1445087491445_0003\n",
      "1445144423722_0023\n",
      "1445087491445_0003\n",
      "1445094324383_0005\n",
      "1445094324383_0005\n",
      "1445182159119_0001\n",
      "1445062781478_0020\n",
      "1445062781478_0020\n",
      "1445182159119_0001\n",
      "1445076437777_0002\n",
      "1445182159119_0015\n",
      "1445144423722_0021\n",
      "1445087491445_0002\n",
      "1445094324383_0004\n",
      "1445087491445_0002\n",
      "1445182159119_0015\n",
      "1445144423722_0021\n",
      "1445076437777_0002\n",
      "1445087491445_0002\n",
      "1445076437777_0002\n",
      "1445144423722_0021\n",
      "1445182159119_0015\n",
      "1445144423722_0021\n",
      "1445076437777_0002\n",
      "1445094324383_0004\n",
      "1445087491445_0002\n",
      "1445062781478_0020\n",
      "1445182159119_0001\n",
      "1445182159119_0001\n",
      "1445062781478_0020\n",
      "1445094324383_0005\n",
      "1445144423722_0023\n",
      "1445144423722_0023\n",
      "1445094324383_0005\n",
      "1445087491445_0003\n",
      "1445087491445_0003\n",
      "1445144423722_0020\n",
      "1445144423722_0020\n",
      "1445087491445_0001\n",
      "1445182159119_0016\n",
      "1445144423722_0022\n",
      "1445076437777_0001\n",
      "1445076437777_0001\n",
      "1445182159119_0016\n",
      "1445144423722_0022\n",
      "1445087491445_0001\n",
      "1445182159119_0002\n",
      "1445182159119_0011\n",
      "1445087491445_0006\n",
      "1445062781478_0019\n",
      "1445087491445_0006\n",
      "1445062781478_0019\n",
      "1445182159119_0011\n",
      "1445182159119_0005\n",
      "1445182159119_0005\n",
      "1445094324383_0001\n",
      "1445087491445_0007\n",
      "1445087491445_0004\n",
      "1445062781478_0018\n",
      "1445094324383_0002\n",
      "1445094324383_0002\n",
      "1445087491445_0004\n",
      "1445087491445_0010\n",
      "1445087491445_0010\n",
      "1445076437777_0005\n",
      "1445182159119_0012\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445087491445_0005\n",
      "1445182159119_0012\n",
      "1445062781478_0014\n",
      "1445175094696_0003\n",
      "1445175094696_0003\n",
      "1445087491445_0009\n",
      "1445087491445_0008\n",
      "1445062781478_0017\n",
      "1445087491445_0008\n",
      "1445062781478_0017\n",
      "1445175094696_0004\n",
      "1445182159119_0019\n",
      "1445182159119_0019\n",
      "1445062781478_0013\n",
      "1445062781478_0013\n",
      "1445062781478_0013\n",
      "1445062781478_0013\n",
      "1445182159119_0019\n",
      "1445175094696_0004\n",
      "1445182159119_0019\n",
      "1445062781478_0017\n",
      "1445087491445_0008\n",
      "1445062781478_0017\n",
      "1445087491445_0008\n",
      "1445087491445_0009\n",
      "1445175094696_0003\n",
      "1445175094696_0003\n",
      "1445062781478_0014\n",
      "1445062781478_0014\n",
      "1445175094696_0002\n",
      "1445062781478_0014\n",
      "1445182159119_0020\n",
      "1445175094696_0003\n",
      "1445175094696_0003\n",
      "1445062781478_0016\n",
      "1445087491445_0008\n",
      "1445062781478_0017\n",
      "1445087491445_0008\n",
      "1445062781478_0017\n",
      "1445062781478_0013\n",
      "1445175094696_0004\n",
      "1445182159119_0019\n",
      "1445182159119_0019\n",
      "1445175094696_0004\n",
      "1445062781478_0012\n",
      "1445062781478_0013\n",
      "1445182159119_0018\n",
      "1445182159119_0018\n",
      "1445062781478_0013\n",
      "1445182159119_0019\n",
      "1445062781478_0012\n",
      "1445175094696_0004\n",
      "1445182159119_0019\n",
      "1445062781478_0013\n",
      "1445062781478_0017\n",
      "1445087491445_0008\n",
      "1445062781478_0017\n",
      "1445087491445_0008\n",
      "1445062781478_0016\n",
      "1445175094696_0003\n",
      "1445175094696_0003\n",
      "1445182159119_0020\n",
      "1445062781478_0014\n",
      "1445175094696_0002\n",
      "1445062781478_0014\n",
      "1445062781478_0014\n",
      "1445062781478_0015\n",
      "1445175094696_0003\n",
      "1445175094696_0001\n",
      "1445087491445_0008\n",
      "1445062781478_0017\n",
      "1445062781478_0017\n",
      "1445182159119_0018\n",
      "1445062781478_0011\n",
      "1445182159119_0019\n",
      "1445062781478_0012\n",
      "1445175094696_0004\n",
      "1445175094696_0004\n",
      "1445182159119_0018\n",
      "1445062781478_0013\n",
      "1445175094696_0005\n",
      "1445175094696_0005\n",
      "1445062781478_0013\n",
      "1445175094696_0004\n",
      "1445182159119_0019\n",
      "1445175094696_0004\n",
      "1445062781478_0012\n",
      "1445062781478_0011\n",
      "1445182159119_0018\n",
      "1445062781478_0017\n",
      "1445062781478_0017\n",
      "1445175094696_0001\n",
      "1445175094696_0003\n",
      "1445062781478_0015\n",
      "1445062781478_0014\n",
      "1445062781478_0014\n",
      "1445062781478_0014\n",
      "1445062781478_0014\n",
      "1445175094696_0003\n",
      "1445087491445_0009\n",
      "1445087491445_0008\n",
      "1445062781478_0017\n",
      "1445182159119_0018\n",
      "1445182159119_0019\n",
      "1445175094696_0004\n",
      "1445175094696_0004\n",
      "1445182159119_0019\n",
      "1445062781478_0013\n",
      "1445175094696_0004\n",
      "1445182159119_0019\n",
      "1445175094696_0004\n",
      "1445062781478_0017\n",
      "1445087491445_0008\n",
      "1445062781478_0017\n",
      "1445087491445_0009\n",
      "1445175094696_0003\n",
      "1445175094696_0003\n",
      "1445062781478_0014\n",
      "1445062781478_0014\n",
      "1445175094696_0002\n",
      "1445062781478_0014\n",
      "1445175094696_0002\n",
      "1445062781478_0015\n",
      "1445062781478_0015\n",
      "1445182159119_0020\n",
      "1445062781478_0016\n",
      "1445175094696_0003\n",
      "1445087491445_0009\n",
      "1445175094696_0001\n",
      "1445175094696_0001\n",
      "1445062781478_0013\n",
      "1445062781478_0011\n",
      "1445182159119_0019\n",
      "1445062781478_0011\n",
      "1445175094696_0004\n",
      "1445062781478_0012\n",
      "1445062781478_0012\n",
      "1445175094696_0005\n",
      "1445182159119_0018\n",
      "1445182159119_0018\n",
      "1445175094696_0005\n",
      "1445182159119_0018\n",
      "1445175094696_0005\n",
      "1445175094696_0005\n",
      "1445182159119_0018\n",
      "1445062781478_0012\n",
      "1445062781478_0012\n",
      "1445175094696_0004\n",
      "1445182159119_0019\n",
      "1445062781478_0011\n",
      "1445062781478_0011\n",
      "1445062781478_0013\n",
      "1445062781478_0013\n",
      "1445175094696_0001\n",
      "1445175094696_0001\n",
      "1445087491445_0009\n",
      "1445087491445_0009\n",
      "1445062781478_0016\n",
      "1445175094696_0003\n",
      "1445182159119_0020\n",
      "1445062781478_0015\n",
      "1445175094696_0002\n",
      "1445087491445_0008\n",
      "1445062781478_0014\n",
      "1445175094696_0002\n",
      "1445175094696_0002\n",
      "1445175094696_0002\n",
      "1445062781478_0015\n",
      "1445062781478_0015\n",
      "1445062781478_0016\n",
      "1445087491445_0009\n",
      "1445175094696_0003\n",
      "1445062781478_0016\n",
      "1445087491445_0009\n",
      "1445175094696_0001\n",
      "1445175094696_0001\n",
      "1445062781478_0013\n",
      "1445062781478_0011\n",
      "1445062781478_0011\n",
      "1445062781478_0012\n",
      "1445175094696_0005\n",
      "1445182159119_0018\n",
      "1445175094696_0005\n",
      "1445182159119_0018\n",
      "1445175094696_0005\n",
      "1445175094696_0005\n",
      "1445182159119_0018\n",
      "1445062781478_0012\n",
      "1445062781478_0012\n",
      "1445062781478_0011\n",
      "1445062781478_0011\n",
      "1445182159119_0019\n",
      "1445062781478_0013\n",
      "1445062781478_0013\n",
      "1445175094696_0001\n",
      "1445175094696_0001\n",
      "1445087491445_0009\n",
      "1445087491445_0009\n",
      "1445062781478_0016\n",
      "1445182159119_0020\n",
      "1445062781478_0015\n",
      "1445062781478_0015\n",
      "1445175094696_0002\n",
      "1445087491445_0008\n",
      "1445175094696_0002\n",
      "1445175094696_0002\n",
      "1445182159119_0020\n",
      "1445182159119_0020\n",
      "1445062781478_0015\n",
      "1445062781478_0015\n",
      "1445062781478_0016\n",
      "1445087491445_0009\n",
      "1445062781478_0016\n",
      "1445175094696_0003\n",
      "1445087491445_0009\n",
      "1445175094696_0001\n",
      "1445087491445_0008\n",
      "1445175094696_0001\n",
      "1445062781478_0013\n",
      "1445062781478_0013\n",
      "1445062781478_0011\n",
      "1445062781478_0012\n",
      "1445062781478_0012\n",
      "1445182159119_0018\n",
      "1445182159119_0018\n",
      "1445182159119_0018\n",
      "1445182159119_0018\n",
      "1445175094696_0005\n",
      "1445062781478_0012\n",
      "1445062781478_0012\n",
      "1445062781478_0011\n",
      "1445062781478_0013\n",
      "1445062781478_0013\n",
      "1445087491445_0008\n",
      "1445175094696_0001\n",
      "1445087491445_0009\n",
      "1445062781478_0016\n",
      "1445087491445_0009\n",
      "1445062781478_0016\n",
      "1445062781478_0015\n",
      "1445182159119_0020\n",
      "1445182159119_0020\n",
      "1445175094696_0002\n",
      "1445175094696_0002\n",
      "1445175094696_0002\n",
      "1445175094696_0002\n",
      "1445182159119_0020\n",
      "1445182159119_0020\n",
      "1445062781478_0015\n",
      "1445062781478_0015\n",
      "1445175094696_0003\n",
      "1445062781478_0016\n",
      "1445087491445_0009\n",
      "1445062781478_0016\n",
      "1445087491445_0009\n",
      "1445175094696_0001\n",
      "1445175094696_0001\n",
      "1445062781478_0017\n",
      "1445062781478_0013\n",
      "1445062781478_0013\n",
      "1445062781478_0011\n",
      "1445182159119_0019\n",
      "1445062781478_0012\n",
      "1445062781478_0013\n",
      "1445175094696_0005\n",
      "1445182159119_0018\n",
      "1445175094696_0005\n",
      "1445182159119_0018\n",
      "1445182159119_0018\n",
      "1445062781478_0013\n",
      "1445062781478_0012\n",
      "1445182159119_0019\n",
      "1445062781478_0012\n",
      "1445062781478_0011\n",
      "1445062781478_0013\n",
      "1445062781478_0013\n",
      "1445062781478_0017\n",
      "1445175094696_0001\n",
      "1445087491445_0009\n",
      "1445062781478_0016\n",
      "1445087491445_0009\n",
      "1445062781478_0016\n",
      "1445175094696_0003\n",
      "1445062781478_0015\n",
      "1445182159119_0020\n",
      "1445182159119_0020\n",
      "1445175094696_0002\n",
      "1445175094696_0002\n",
      "1445087491445_0005\n",
      "1445182159119_0012\n",
      "1445094324383_0003\n",
      "1445094324383_0003\n",
      "1445087491445_0010\n",
      "1445087491445_0010\n",
      "1445076437777_0004\n",
      "1445182159119_0013\n",
      "1445087491445_0004\n",
      "1445182159119_0013\n",
      "1445182159119_0004\n",
      "1445182159119_0004\n",
      "1445144423722_0024\n",
      "1445087491445_0007\n",
      "1445087491445_0007\n",
      "1445062781478_0018\n",
      "1445144423722_0024\n",
      "1445182159119_0005\n",
      "1445062781478_0019\n",
      "1445182159119_0002\n",
      "abel.txt\n",
      "1445182159119_0016\n",
      "1445087491445_0002\n",
      "1445182159119_0003\n",
      "1445182159119_0003\n",
      "1445182159119_0017\n",
      "1445144423722_0020\n",
      "1445144423722_0023\n",
      "1445182159119_0017\n",
      "1445087491445_0003\n",
      "1445182159119_0014\n",
      "1445144423722_0020\n",
      "1445076437777_0003\n",
      "1445087491445_0003\n",
      "1445062781478_0020\n",
      "1445182159119_0001\n",
      "1445094324383_0004\n",
      "1445076437777_0002\n",
      "1445144423722_0021\n",
      "1445076437777_0001\n",
      "1445182159119_0015\n",
      "1445094324383_0004\n",
      "1445094324383_0004\n",
      "1445182159119_0015\n",
      "1445144423722_0021\n",
      "1445076437777_0002\n",
      "1445182159119_0001\n",
      "1445062781478_0020\n",
      "1445182159119_0002\n",
      "1445076437777_0003\n",
      "1445144423722_0020\n",
      "1445182159119_0014\n",
      "1445087491445_0003\n",
      "1445087491445_0003\n",
      "1445182159119_0014\n",
      "1445182159119_0017\n",
      "1445144423722_0023\n",
      "1445144423722_0020\n",
      "1445182159119_0017\n",
      "1445182159119_0003\n",
      "1445182159119_0003\n",
      "1445182159119_0016\n",
      "1445182159119_0002\n",
      "1445062781478_0019\n",
      "1445182159119_0005\n",
      "1445062781478_0018\n",
      "1445087491445_0007\n",
      "1445144423722_0024\n",
      "1445144423722_0024\n",
      "1445087491445_0007\n",
      "1445182159119_0004\n",
      "1445182159119_0013\n",
      "1445076437777_0004\n",
      "1445087491445_0004\n",
      "1445087491445_0010\n",
      "1445087491445_0010\n",
      "1445094324383_0003\n",
      "1445182159119_0012\n",
      "1445094324383_0003\n",
      "1445087491445_0005\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445087491445_0005\n",
      "1445087491445_0010\n",
      "1445087491445_0010\n",
      "1445182159119_0013\n",
      "1445076437777_0004\n",
      "1445087491445_0004\n",
      "1445087491445_0004\n",
      "1445094324383_0002\n",
      "1445182159119_0013\n",
      "1445182159119_0004\n",
      "1445182159119_0004\n",
      "1445144423722_0024\n",
      "1445087491445_0007\n",
      "1445062781478_0018\n",
      "1445087491445_0007\n",
      "1445062781478_0018\n",
      "1445144423722_0024\n",
      "1445087491445_0006\n",
      "1445087491445_0001\n",
      "1445087491445_0001\n",
      "1445182159119_0003\n",
      "1445182159119_0017\n",
      "1445144423722_0023\n",
      "1445182159119_0017\n",
      "1445087491445_0003\n",
      "1445144423722_0020\n",
      "1445182159119_0014\n",
      "1445076437777_0003\n",
      "1445076437777_0003\n",
      "1445144423722_0020\n",
      "1445087491445_0003\n",
      "1445094324383_0005\n",
      "1445094324383_0004\n",
      "1445076437777_0001\n",
      "1445087491445_0002\n",
      "1445094324383_0004\n",
      "1445076437777_0001\n",
      "1445094324383_0004\n",
      "1445087491445_0002\n",
      "1445094324383_0004\n",
      "1445182159119_0002\n",
      "1445087491445_0002\n",
      "1445182159119_0014\n",
      "1445144423722_0020\n",
      "1445076437777_0003\n",
      "1445094324383_0005\n",
      "1445087491445_0003\n",
      "1445087491445_0003\n",
      "1445076437777_0003\n",
      "1445182159119_0014\n",
      "1445144423722_0020\n",
      "1445182159119_0017\n",
      "1445182159119_0017\n",
      "1445182159119_0003\n",
      "1445182159119_0003\n",
      "1445087491445_0001\n",
      "1445087491445_0006\n",
      "1445062781478_0018\n",
      "1445087491445_0007\n",
      "1445144423722_0024\n",
      "1445144423722_0024\n",
      "1445062781478_0018\n",
      "1445182159119_0004\n",
      "1445182159119_0004\n",
      "1445094324383_0002\n",
      "1445087491445_0004\n",
      "1445182159119_0013\n",
      "1445076437777_0004\n",
      "1445182159119_0013\n",
      "1445087491445_0004\n",
      "1445087491445_0010\n",
      "1445087491445_0010\n",
      "1445094324383_0003\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445087491445_0005\n",
      "1445087491445_0004\n",
      "1445182159119_0013\n",
      "1445076437777_0004\n",
      "1445076437777_0004\n",
      "1445182159119_0013\n",
      "1445087491445_0004\n",
      "1445087491445_0010\n",
      "1445087491445_0010\n",
      "1445087491445_0004\n",
      "1445062781478_0018\n",
      "1445094324383_0001\n",
      "1445087491445_0007\n",
      "1445062781478_0018\n",
      "1445182159119_0004\n",
      "1445087491445_0002\n",
      "1445087491445_0001\n",
      "1445087491445_0001\n",
      "1445182159119_0017\n",
      "1445144423722_0023\n",
      "1445182159119_0003\n",
      "1445182159119_0003\n",
      "1445144423722_0020\n",
      "1445182159119_0014\n",
      "1445076437777_0003\n",
      "1445094324383_0005\n",
      "1445087491445_0003\n",
      "1445087491445_0003\n",
      "1445076437777_0003\n",
      "1445182159119_0014\n",
      "1445094324383_0005\n",
      "1445094324383_0004\n",
      "1445087491445_0002\n",
      "1445094324383_0004\n",
      "1445094324383_0004\n",
      "1445087491445_0002\n",
      "1445094324383_0004\n",
      "1445094324383_0005\n",
      "1445087491445_0003\n",
      "1445182159119_0014\n",
      "1445076437777_0003\n",
      "1445182159119_0014\n",
      "1445087491445_0003\n",
      "1445094324383_0005\n",
      "1445182159119_0003\n",
      "1445182159119_0003\n",
      "1445182159119_0017\n",
      "1445144423722_0023\n",
      "1445087491445_0001\n",
      "1445087491445_0001\n",
      "1445087491445_0002\n",
      "1445182159119_0004\n",
      "1445144423722_0024\n",
      "1445062781478_0018\n",
      "1445087491445_0007\n",
      "1445094324383_0001\n",
      "1445087491445_0004\n",
      "1445087491445_0010\n",
      "1445087491445_0010\n",
      "1445182159119_0013\n",
      "1445076437777_0004\n",
      "1445087491445_0004\n",
      "1445087491445_0004\n",
      "1445076437777_0004\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445087491445_0006\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445094324383_0003\n",
      "1445076437777_0005\n",
      "1445087491445_0005\n",
      "1445087491445_0004\n",
      "1445076437777_0004\n",
      "1445182159119_0013\n",
      "1445182159119_0013\n",
      "1445087491445_0004\n",
      "1445087491445_0010\n",
      "1445062781478_0018\n",
      "1445144423722_0024\n",
      "1445062781478_0018\n",
      "1445182159119_0004\n",
      "1445182159119_0004\n",
      "1445182159119_0011\n",
      "1445182159119_0016\n",
      "1445144423722_0022\n",
      "1445076437777_0001\n",
      "1445182159119_0002\n",
      "1445087491445_0002\n",
      "1445087491445_0001\n",
      "1445182159119_0002\n",
      "1445182159119_0017\n",
      "1445144423722_0020\n",
      "1445182159119_0003\n",
      "1445182159119_0003\n",
      "1445076437777_0003\n",
      "1445144423722_0023\n",
      "1445182159119_0014\n",
      "1445087491445_0003\n",
      "1445087491445_0003\n",
      "1445182159119_0014\n",
      "1445144423722_0020\n",
      "1445076437777_0003\n",
      "1445094324383_0005\n",
      "1445076437777_0001\n",
      "1445094324383_0004\n",
      "1445094324383_0004\n",
      "1445182159119_0015\n",
      "1445094324383_0004\n",
      "1445182159119_0015\n",
      "1445076437777_0001\n",
      "1445094324383_0004\n",
      "1445094324383_0005\n",
      "1445076437777_0003\n",
      "1445144423722_0020\n",
      "1445182159119_0014\n",
      "1445182159119_0014\n",
      "1445144423722_0023\n",
      "1445076437777_0003\n",
      "1445087491445_0003\n",
      "1445182159119_0003\n",
      "1445182159119_0003\n",
      "1445144423722_0020\n",
      "1445182159119_0017\n",
      "1445087491445_0001\n",
      "1445182159119_0002\n",
      "1445076437777_0001\n",
      "1445144423722_0022\n",
      "1445182159119_0016\n",
      "1445182159119_0011\n",
      "1445182159119_0004\n",
      "1445182159119_0004\n",
      "1445062781478_0018\n",
      "1445062781478_0018\n",
      "1445087491445_0007\n",
      "1445144423722_0024\n",
      "1445087491445_0010\n",
      "1445087491445_0010\n",
      "1445076437777_0004\n",
      "1445182159119_0013\n",
      "1445087491445_0004\n",
      "1445087491445_0004\n",
      "1445076437777_0004\n",
      "1445087491445_0005\n",
      "1445076437777_0005\n",
      "1445094324383_0003\n",
      "1445094324383_0003\n",
      "1445087491445_0006\n"
     ]
    }
   ],
   "source": [
    "match = template_miner.match(\"Read from history task task_1445087491445_0006_m_000005\")\n",
    "print(match.get_template())\n",
    "print(match.cluster_id)\n",
    "\n",
    "label = []\n",
    "events = []\n",
    "identifier = []\n",
    "\n",
    "# open files one by one\n",
    "meragefiledir = '/Users/shuming/Downloads/merged_hadoop/'\n",
    "filenames=os.listdir(meragefiledir)  \n",
    "   \n",
    "for filename in filenames: \n",
    "    sequence = []\n",
    "    filepath=meragefiledir+filename   \n",
    "    if filename[10:28] in normal_log:\n",
    "        label.append(0)\n",
    "    else:\n",
    "        label.append(1) \n",
    "    for line in open(filepath):     \n",
    "        match = template_miner.match(line)\n",
    "        sequence.append('E'+match.cluster_id)\n",
    "    identifier(filename)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
