{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0381c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c78a1bf",
   "metadata": {},
   "source": [
    "### mining log template model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "642cca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from os.path import dirname\n",
    "\n",
    "def mineModel(f, cfg):\n",
    "    config = TemplateMinerConfig()\n",
    "    config.load(cfg)\n",
    "    config.profiling_enabled = True\n",
    "    template_miner = TemplateMiner(config=config)   \n",
    "    lines = f.readlines()              \n",
    "\n",
    "    line_count = 0\n",
    "    start_time = time.time()\n",
    "    batch_start_time = start_time\n",
    "    batch_size = 10000\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(message)s')\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.rstrip()\n",
    "        line = line.partition(\": \")[2]\n",
    "        result = template_miner.add_log_message(line)\n",
    "        line_count += 1\n",
    "        if line_count % batch_size == 0:\n",
    "            time_took = time.time() - batch_start_time\n",
    "            rate = batch_size / time_took\n",
    "            logger.info(f\"Processing line: {line_count}, rate {rate:.1f} lines/sec, \"\n",
    "                        f\"{len(template_miner.drain.clusters)} clusters so far.\")\n",
    "            batch_start_time = time.time()\n",
    "        if result[\"change_type\"] != \"none\":\n",
    "            result_json = json.dumps(result)\n",
    "            logger.info(f\"Input ({line_count}): \" + line)\n",
    "            logger.info(\"Result: \" + result_json)\n",
    "\n",
    "    time_took = time.time() - start_time\n",
    "    rate = line_count / time_took\n",
    "    logger.info(f\"--- Done processing file in {time_took:.2f} sec. Total of {line_count} lines, rate {rate:.1f} lines/sec, \"\n",
    "                f\"{len(template_miner.drain.clusters)} clusters\")\n",
    "\n",
    "    sorted_clusters = sorted(template_miner.drain.clusters, key=lambda it: it.size, reverse=True)\n",
    "    for cluster in sorted_clusters:\n",
    "        logger.info(cluster)\n",
    "    \n",
    "    return template_miner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb7818",
   "metadata": {},
   "source": [
    "### Parse log files from Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2dd0a7",
   "metadata": {},
   "source": [
    "#### Move all files to a single directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b3c0ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shuming/Downloads/Hadoop/abnormal_label.txt\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000028.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000026.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000027.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000025.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000024.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000023.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000022.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000024.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000023.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000022.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000022.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000023.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000022.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000024.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000022.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000023.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000022.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000023.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_02_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_02_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_02_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_02_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000008.log\n",
      "copy complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "source_path = os.path.abspath(r'/Users/shuming/Downloads/Hadoop')     # source directory\n",
    "target_path = os.path.abspath(r'/Users/shuming/Downloads/merged_hadoop/')    # target directory\n",
    "\n",
    "if not os.path.exists(target_path):     # creat target directory if it doesn't exist \n",
    "    os.makedirs(target_path)\n",
    "\n",
    "if os.path.exists(source_path):    \n",
    "    \n",
    "    for root, dirs, files in os.walk(source_path):\n",
    "        for file in files:\n",
    "            src_file = os.path.join(root, file)\n",
    "            shutil.copy(src_file, target_path)\n",
    "            print(src_file)\n",
    "\n",
    "print('copy complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1d21e6",
   "metadata": {},
   "source": [
    "#### Merged all files into one log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad3f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os module consists of many functions about file and directory processing\n",
    "import os  \n",
    "meragefiledir = '/Users/shuming/Downloads/merged_hadoop/'\n",
    "filenames=os.listdir(meragefiledir)  \n",
    "# target file path\n",
    "file=open('/Users/shuming/Downloads/merged_hadoop/merged.log','w')  \n",
    "   \n",
    "for filename in filenames:  \n",
    "    if(filename != 'merged.log'):\n",
    "        filepath=meragefiledir+filename    \n",
    "        for line in open(filepath):  \n",
    "            file.writelines(line)  \n",
    "        file.write('\\n')  \n",
    " \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55c5fc4",
   "metadata": {},
   "source": [
    "#### Train Drain3 model with existing logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4b98610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (1): loaded properties from hadoop-metrics2.properties\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 1, \"cluster_size\": 1, \"template_mined\": \"loaded properties from hadoop-metrics2.properties\", \"cluster_count\": 1}\n",
      "Input (2): Scheduled snapshot period at 10 second(s).\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 2, \"cluster_size\": 1, \"template_mined\": \"Scheduled snapshot period at <:NUM:> second(s).\", \"cluster_count\": 2}\n",
      "Input (3): MapTask metrics system started\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 3, \"cluster_size\": 1, \"template_mined\": \"MapTask metrics system started\", \"cluster_count\": 3}\n",
      "Input (4): Executing with tokens:\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 4, \"cluster_size\": 1, \"template_mined\": \"Executing with tokens:\", \"cluster_count\": 4}\n",
      "Input (5): Kind: mapreduce.job, Service: job_1445076437777_0005, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@666adef3)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 5, \"cluster_size\": 1, \"template_mined\": \"Kind: mapreduce.job, Service: job <:NUM:> <:NUM:>, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@666adef3)\", \"cluster_count\": 5}\n",
      "Input (6): Sleeping for 0ms before retrying again. Got null now.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 6, \"cluster_size\": 1, \"template_mined\": \"Sleeping for 0ms before retrying again. Got null now.\", \"cluster_count\": 6}\n",
      "Input (7): mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445076437777_0005\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 7, \"cluster_size\": 1, \"template_mined\": \"mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application <:NUM:> <:NUM:>\", \"cluster_count\": 7}\n",
      "Input (8): session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 8, \"cluster_size\": 1, \"template_mined\": \"session.id is deprecated. Instead, use dfs.metrics.session-id\", \"cluster_count\": 8}\n",
      "Input (9): ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 9, \"cluster_size\": 1, \"template_mined\": \"ProcfsBasedProcessTree currently is supported only on Linux.\", \"cluster_count\": 9}\n",
      "Input (10):  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6270c836\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 10, \"cluster_size\": 1, \"template_mined\": \"Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6270c836\", \"cluster_count\": 10}\n",
      "Input (11): Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:268435456+134217728\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 11, \"cluster_size\": 1, \"template_mined\": \"Processing split: hdfs://msra-sa-<:NUM:>:<:NUM:>/pageinput2.txt:<:NUM:>+<:NUM:>\", \"cluster_count\": 11}\n",
      "Input (12): (EQUATOR) 0 kvi 26214396(104857584)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 12, \"cluster_size\": 1, \"template_mined\": \"(EQUATOR) <:NUM:> kvi <:NUM:>(<:NUM:>)\", \"cluster_count\": 12}\n",
      "Input (13): mapreduce.task.io.sort.mb: 100\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 13, \"cluster_size\": 1, \"template_mined\": \"mapreduce.task.io.sort.mb: <:NUM:>\", \"cluster_count\": 13}\n",
      "Input (14): soft limit at 83886080\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 14, \"cluster_size\": 1, \"template_mined\": \"soft limit at <:NUM:>\", \"cluster_count\": 14}\n",
      "Input (15): bufstart = 0; bufvoid = 104857600\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 15, \"cluster_size\": 1, \"template_mined\": \"bufstart = <:NUM:>; bufvoid = <:NUM:>\", \"cluster_count\": 15}\n",
      "Input (16): kvstart = 26214396; length = 6553600\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 16, \"cluster_size\": 1, \"template_mined\": \"kvstart = <:NUM:>; length = <:NUM:>\", \"cluster_count\": 16}\n",
      "Input (17): Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 17, \"cluster_size\": 1, \"template_mined\": \"Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\", \"cluster_count\": 17}\n",
      "Input (18): Spilling map output\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 18, \"cluster_size\": 1, \"template_mined\": \"Spilling map output\", \"cluster_count\": 18}\n",
      "Input (19): bufstart = 0; bufend = 48249276; bufvoid = 104857600\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 19, \"cluster_size\": 1, \"template_mined\": \"bufstart = <:NUM:>; bufend = <:NUM:>; bufvoid = <:NUM:>\", \"cluster_count\": 19}\n",
      "Input (20): kvstart = 26214396(104857584); kvend = 17305200(69220800); length = 8909197/6553600\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 20, \"cluster_size\": 1, \"template_mined\": \"kvstart = <:NUM:>(<:NUM:>); kvend = <:NUM:>(<:NUM:>); length = <:NUM:>/<:NUM:>\", \"cluster_count\": 20}\n",
      "Input (22): Finished spill 0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 21, \"cluster_size\": 1, \"template_mined\": \"Finished spill <:NUM:>\", \"cluster_count\": 21}\n",
      "Input (23): (RESET) equator 57318028 kv 14329500(57318000) kvi 12129788(48519152)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 22, \"cluster_size\": 1, \"template_mined\": \"(RESET) equator <:NUM:> kv <:NUM:>(<:NUM:>) kvi <:NUM:>(<:NUM:>)\", \"cluster_count\": 22}\n",
      "Input (60): Starting flush of map output\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 23, \"cluster_size\": 1, \"template_mined\": \"Starting flush of map output\", \"cluster_count\": 23}\n",
      "Input (65): Merging 8 sorted segments\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 24, \"cluster_size\": 1, \"template_mined\": \"Merging <:NUM:> sorted segments\", \"cluster_count\": 24}\n",
      "Input (66): Down to the last merge-pass, with 8 segments left of total size: 288688442 bytes\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 25, \"cluster_size\": 1, \"template_mined\": \"Down to the last merge-pass, with <:NUM:> segments left of total size: <:NUM:> bytes\", \"cluster_count\": 25}\n",
      "Input (67): Task:attempt_1445076437777_0005_m_000002_0 is done. And is in the process of committing\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 26, \"cluster_size\": 1, \"template_mined\": \"Task:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> is done. And is in the process of committing\", \"cluster_count\": 26}\n",
      "Input (68): Task 'attempt_1445076437777_0005_m_000002_0' done.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 27, \"cluster_size\": 1, \"template_mined\": \"Task 'attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>' done.\", \"cluster_count\": 27}\n",
      "Input (69): \n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 28, \"cluster_size\": 1, \"template_mined\": \"\", \"cluster_count\": 28}\n",
      "Input (74): Kind: mapreduce.job, Service: job_1445087491445_0005, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@7f0eb4b4)\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 5, \"cluster_size\": 2, \"template_mined\": \"Kind: mapreduce.job, Service: job <:NUM:> <:NUM:>, Ident: <:*:>\", \"cluster_count\": 28}\n",
      "Input (79):  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6ad3381f\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 10, \"cluster_size\": 2, \"template_mined\": \"Using ResourceCalculatorProcessTree : <:*:>\", \"cluster_count\": 28}\n",
      "Input (80): Processing split: hdfs://msra-sa-41:9000/wordcount2.txt:402653184+134217728\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 11, \"cluster_size\": 2, \"template_mined\": \"Processing split: <:*:>\", \"cluster_count\": 28}\n",
      "Input (122): I/O error constructing remote block reader.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 29, \"cluster_size\": 1, \"template_mined\": \"I/O error constructing remote block reader.\", \"cluster_count\": 29}\n",
      "Input (123): Connection timed out: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 30, \"cluster_size\": 1, \"template_mined\": \"Connection timed out: no further information\", \"cluster_count\": 30}\n",
      "Input (151): Failed to connect to /10.86.169.121:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 31, \"cluster_size\": 1, \"template_mined\": \"Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information\", \"cluster_count\": 31}\n",
      "Input (180): Successfully connected to /10.190.173.170:50010 for BP-1347369012-10.190.173.170-1444972147527:blk_1073742826_2022\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 32, \"cluster_size\": 1, \"template_mined\": \"Successfully connected to /<:IP:>:<:NUM:> for BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:>\", \"cluster_count\": 32}\n",
      "Input (271): Connection refused: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 33, \"cluster_size\": 1, \"template_mined\": \"Connection refused: no further information\", \"cluster_count\": 33}\n",
      "Input (296): Failed to connect to /10.86.169.121:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection refused: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 34, \"cluster_size\": 1, \"template_mined\": \"Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.ConnectException: Connection refused: no further information\", \"cluster_count\": 34}\n",
      "Input (402): Stopping MapTask metrics system...\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 35, \"cluster_size\": 1, \"template_mined\": \"Stopping MapTask metrics system...\", \"cluster_count\": 35}\n",
      "Input (403): MapTask metrics system stopped.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 3, \"cluster_size\": 6, \"template_mined\": \"MapTask metrics system <:*:>\", \"cluster_count\": 35}\n",
      "Input (404): MapTask metrics system shutdown complete.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 36, \"cluster_size\": 1, \"template_mined\": \"MapTask metrics system shutdown complete.\", \"cluster_count\": 36}\n",
      "Input (406): Created MRAppMaster for application appattempt_1445087491445_0007_000002\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 37, \"cluster_size\": 1, \"template_mined\": \"Created MRAppMaster for application appattempt <:NUM:> <:NUM:> <:NUM:>\", \"cluster_count\": 37}\n",
      "Input (408): Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 7 cluster_timestamp: 1445087491445 } attemptId: 2 } keyId: -1547346236)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 38, \"cluster_size\": 1, \"template_mined\": \"Kind: YARN AM RM TOKEN, Service: , Ident: (appAttemptId { application id { id: <:NUM:> cluster timestamp: <:NUM:> } attemptId: <:NUM:> } keyId: <:NUM:>)\", \"cluster_count\": 38}\n",
      "Input (409): Using mapred newApiCommitter.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 39, \"cluster_size\": 1, \"template_mined\": \"Using mapred newApiCommitter.\", \"cluster_count\": 39}\n",
      "Input (410): OutputCommitter set in config null\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 40, \"cluster_size\": 1, \"template_mined\": \"OutputCommitter set in config null\", \"cluster_count\": 40}\n",
      "Input (411): OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 41, \"cluster_size\": 1, \"template_mined\": \"OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\", \"cluster_count\": 41}\n",
      "Input (412): Registering class org.apache.hadoop.mapreduce.jobhistory.EventType for class org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 42, \"cluster_size\": 1, \"template_mined\": \"Registering class org.apache.hadoop.mapreduce.jobhistory.EventType for class org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler\", \"cluster_count\": 42}\n",
      "Input (413): Registering class org.apache.hadoop.mapreduce.v2.app.job.event.JobEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 42, \"cluster_size\": 2, \"template_mined\": \"Registering class <:*:> for class <:*:>\", \"cluster_count\": 42}\n",
      "Input (420): Default file system [hdfs://msra-sa-41:9000]\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 43, \"cluster_size\": 1, \"template_mined\": \"Default file system [hdfs://msra-sa-<:NUM:>:<:NUM:>]\", \"cluster_count\": 43}\n",
      "Input (423): Emitting job history data to the timeline server is not enabled\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 44, \"cluster_size\": 1, \"template_mined\": \"Emitting job history data to the timeline server is not enabled\", \"cluster_count\": 44}\n",
      "Input (424): Recovery is enabled. Will try to recover from previous life on best effort basis.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 45, \"cluster_size\": 1, \"template_mined\": \"Recovery is enabled. Will try to recover from previous life on best effort basis.\", \"cluster_count\": 45}\n",
      "Input (426): Previous history file is at hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007/job_1445087491445_0007_1.jhist\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 46, \"cluster_size\": 1, \"template_mined\": \"Previous history file is at hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist\", \"cluster_count\": 46}\n",
      "Input (427): Read from history task task_1445087491445_0007_m_000001\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 47, \"cluster_size\": 1, \"template_mined\": \"Read from history task task <:NUM:> <:NUM:> m <:NUM:>\", \"cluster_count\": 47}\n",
      "Input (437): Read completed tasks from history 10\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 48, \"cluster_size\": 1, \"template_mined\": \"Read completed tasks from history <:NUM:>\", \"cluster_count\": 48}\n",
      "Input (441): MRAppMaster metrics system started\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 49, \"cluster_size\": 1, \"template_mined\": \"MRAppMaster metrics system started\", \"cluster_count\": 49}\n",
      "Input (442): Adding job token for job_1445087491445_0007 to jobTokenSecretManager\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 50, \"cluster_size\": 1, \"template_mined\": \"Adding job token for job <:NUM:> <:NUM:> to jobTokenSecretManager\", \"cluster_count\": 50}\n",
      "Input (443): Not uberizing job_1445087491445_0007 because: not enabled; too many maps; too much input;\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 51, \"cluster_size\": 1, \"template_mined\": \"Not uberizing job <:NUM:> <:NUM:> because: not enabled; too many maps; too much input;\", \"cluster_count\": 51}\n",
      "Input (444): Input size for job job_1445087491445_0007 = 1313861632. Number of splits = 10\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 52, \"cluster_size\": 1, \"template_mined\": \"Input size for job job <:NUM:> <:NUM:> = <:NUM:>. Number of splits = <:NUM:>\", \"cluster_count\": 52}\n",
      "Input (445): Number of reduces for job job_1445087491445_0007 = 1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 53, \"cluster_size\": 1, \"template_mined\": \"Number of reduces for job job <:NUM:> <:NUM:> = <:NUM:>\", \"cluster_count\": 53}\n",
      "Input (446): job_1445087491445_0007Job Transitioned from NEW to INITED\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 54, \"cluster_size\": 1, \"template_mined\": \"job <:NUM:> 0007Job Transitioned from NEW to INITED\", \"cluster_count\": 54}\n",
      "Input (447): MRAppMaster launching normal, non-uberized, multi-container job job_1445087491445_0007.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 55, \"cluster_size\": 1, \"template_mined\": \"MRAppMaster launching normal, non-uberized, multi-container job job <:NUM:> <:NUM:>.\", \"cluster_count\": 55}\n",
      "Input (448): Using callQueue class java.util.concurrent.LinkedBlockingQueue\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 56, \"cluster_size\": 1, \"template_mined\": \"Using callQueue class java.util.concurrent.LinkedBlockingQueue\", \"cluster_count\": 56}\n",
      "Input (449): Starting Socket Reader #1 for port 24281\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 57, \"cluster_size\": 1, \"template_mined\": \"Starting Socket Reader #<:NUM:> for port <:NUM:>\", \"cluster_count\": 57}\n",
      "Input (450): Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 58, \"cluster_size\": 1, \"template_mined\": \"Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server\", \"cluster_count\": 58}\n",
      "Input (451): IPC Server listener on 24281: starting\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 59, \"cluster_size\": 1, \"template_mined\": \"IPC Server listener on <:NUM:>: starting\", \"cluster_count\": 59}\n",
      "Input (452): IPC Server Responder: starting\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 60, \"cluster_size\": 1, \"template_mined\": \"IPC Server Responder: starting\", \"cluster_count\": 60}\n",
      "Input (453): Instantiated MRClientService at MSRA-SA-41.fareast.corp.microsoft.com/10.190.173.170:24281\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 61, \"cluster_size\": 1, \"template_mined\": \"Instantiated MRClientService at MSRA-SA-<:NUM:>.fareast.corp.microsoft.com/<:IP:>:<:NUM:>\", \"cluster_count\": 61}\n",
      "Input (454): Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 62, \"cluster_size\": 1, \"template_mined\": \"Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\", \"cluster_count\": 62}\n",
      "Input (455): Http request log for http.requests.mapreduce is not defined\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 63, \"cluster_size\": 1, \"template_mined\": \"Http request log for http.requests.mapreduce is not defined\", \"cluster_count\": 63}\n",
      "Input (456): Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 64, \"cluster_size\": 1, \"template_mined\": \"Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\", \"cluster_count\": 64}\n",
      "Input (457): Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context mapreduce\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 65, \"cluster_size\": 1, \"template_mined\": \"Added filter AM PROXY FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context mapreduce\", \"cluster_count\": 65}\n",
      "Input (458): Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context static\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 65, \"cluster_size\": 2, \"template_mined\": \"Added filter AM PROXY FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context <:*:>\", \"cluster_count\": 65}\n",
      "Input (459): adding path spec: /mapreduce/*\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 66, \"cluster_size\": 1, \"template_mined\": \"adding path spec: /mapreduce/*\", \"cluster_count\": 66}\n",
      "Input (460): adding path spec: /ws/*\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 66, \"cluster_size\": 2, \"template_mined\": \"adding path spec: <:*:>\", \"cluster_count\": 66}\n",
      "Input (461): Jetty bound to port 24289\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 67, \"cluster_size\": 1, \"template_mined\": \"Jetty bound to port <:NUM:>\", \"cluster_count\": 67}\n",
      "Input (462): jetty-6.1.26\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 68, \"cluster_size\": 1, \"template_mined\": \"jetty-<:NUM:>.<:NUM:>.<:NUM:>\", \"cluster_count\": 68}\n",
      "Input (463): Extract jar:file:/D:/hadoop-2.6.0-localbox/share/hadoop/yarn/hadoop-yarn-common-2.6.0-SNAPSHOT.jar!/webapps/mapreduce to C:\\Users\\msrabi\\AppData\\Local\\Temp\\2\\Jetty_0_0_0_0_24289_mapreduce____.6ucjbs\\webapp\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 69, \"cluster_size\": 1, \"template_mined\": \"Extract jar:file:/D:/hadoop-<:NUM:>.<:NUM:>.<:NUM:>-localbox/share/hadoop/yarn/hadoop-yarn-common-<:NUM:>.<:NUM:>.<:NUM:>-SNAPSHOT.jar!/webapps/mapreduce to C:\\\\Users\\\\msrabi\\\\AppData\\\\Local\\\\Temp\\\\<:NUM:>\\\\Jetty <:NUM:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> mapreduce .6ucjbs\\\\webapp\", \"cluster_count\": 69}\n",
      "Input (464): Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:24289\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 70, \"cluster_size\": 1, \"template_mined\": \"Started HttpServer2$SelectChannelConnectorWithSafeStartup@<:IP:>:<:NUM:>\", \"cluster_count\": 70}\n",
      "Input (465): Web app /mapreduce started at 24289\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 71, \"cluster_size\": 1, \"template_mined\": \"Web app /mapreduce started at <:NUM:>\", \"cluster_count\": 71}\n",
      "Input (466): Registered webapp guice modules\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 72, \"cluster_size\": 1, \"template_mined\": \"Registered webapp guice modules\", \"cluster_count\": 72}\n",
      "Input (467): JOB_CREATE job_1445087491445_0007\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 73, \"cluster_size\": 1, \"template_mined\": \"JOB CREATE job <:NUM:> <:NUM:>\", \"cluster_count\": 73}\n",
      "Input (472): nodeBlacklistingEnabled:true\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 74, \"cluster_size\": 1, \"template_mined\": \"nodeBlacklistingEnabled:true\", \"cluster_count\": 74}\n",
      "Input (473): maxTaskFailuresPerNode is 3\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 75, \"cluster_size\": 1, \"template_mined\": \"maxTaskFailuresPerNode is <:NUM:>\", \"cluster_count\": 75}\n",
      "Input (474): blacklistDisablePercent is 33\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 76, \"cluster_size\": 1, \"template_mined\": \"blacklistDisablePercent is <:NUM:>\", \"cluster_count\": 76}\n",
      "Input (475): Connecting to ResourceManager at MSRA-SA-41/10.190.173.170:8030\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 77, \"cluster_size\": 1, \"template_mined\": \"Connecting to ResourceManager at MSRA-SA-<:NUM:>/<:IP:>:<:NUM:>\", \"cluster_count\": 77}\n",
      "Input (476): maxContainerCapability: <memory:8192, vCores:32>\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 78, \"cluster_size\": 1, \"template_mined\": \"maxContainerCapability: <memory:<:NUM:>, vCores:<:NUM:>>\", \"cluster_count\": 78}\n",
      "Input (477): queue: default\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 79, \"cluster_size\": 1, \"template_mined\": \"queue: default\", \"cluster_count\": 79}\n",
      "Input (478): Upper limit on the thread pool size is 500\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 80, \"cluster_size\": 1, \"template_mined\": \"Upper limit on the thread pool size is <:NUM:>\", \"cluster_count\": 80}\n",
      "Input (479): yarn.client.max-cached-nodemanagers-proxies : 0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 81, \"cluster_size\": 1, \"template_mined\": \"yarn.client.max-cached-nodemanagers-proxies : <:NUM:>\", \"cluster_count\": 81}\n",
      "Input (480): job_1445087491445_0007Job Transitioned from INITED to SETUP\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 54, \"cluster_size\": 2, \"template_mined\": \"job <:NUM:> 0007Job Transitioned from <:*:> to <:*:>\", \"cluster_count\": 81}\n",
      "Input (481): Processing the event EventType: JOB_SETUP\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 82, \"cluster_size\": 1, \"template_mined\": \"Processing the event EventType: JOB SETUP\", \"cluster_count\": 82}\n",
      "Input (483): Recovering task task_1445087491445_0007_m_000000 from prior app attempt, status was SUCCEEDED\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 83, \"cluster_size\": 1, \"template_mined\": \"Recovering task task <:NUM:> <:NUM:> m <:NUM:> from prior app attempt, status was SUCCEEDED\", \"cluster_count\": 83}\n",
      "Input (484): Resolved MININT-75DGDAM1.fareast.corp.microsoft.com to /default-rack\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 84, \"cluster_size\": 1, \"template_mined\": \"Resolved MININT-75DGDAM1.fareast.corp.microsoft.com to /default-rack\", \"cluster_count\": 84}\n",
      "Input (485): Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 84, \"cluster_size\": 2, \"template_mined\": \"Resolved <:*:> to /default-rack\", \"cluster_count\": 84}\n",
      "Input (487): TaskAttempt: [attempt_1445087491445_0007_m_000000_0] using containerId: [container_1445087491445_0007_01_000002 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:30535]\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 85, \"cluster_size\": 1, \"template_mined\": \"TaskAttempt: [attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>] using containerId: [container <:NUM:> <:NUM:> <:NUM:> <:NUM:> on NM: [MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>]\", \"cluster_count\": 85}\n",
      "Input (488): attempt_1445087491445_0007_m_000000_0 TaskAttempt Transitioned from NEW to SUCCEEDED\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 86, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> TaskAttempt Transitioned from NEW to SUCCEEDED\", \"cluster_count\": 86}\n",
      "Input (489): Task succeeded with attempt attempt_1445087491445_0007_m_000000_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 87, \"cluster_size\": 1, \"template_mined\": \"Task succeeded with attempt attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 87}\n",
      "Input (490): task_1445087491445_0007_m_000000 Task Transitioned from NEW to SUCCEEDED\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 88, \"cluster_size\": 1, \"template_mined\": \"task <:NUM:> <:NUM:> m <:NUM:> Task Transitioned from NEW to SUCCEEDED\", \"cluster_count\": 88}\n",
      "Input (500): Event Writer setup for JobId: job_1445087491445_0007, File: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007/job_1445087491445_0007_2.jhist\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 89, \"cluster_size\": 1, \"template_mined\": \"Event Writer setup for JobId: job <:NUM:> <:NUM:>, File: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist\", \"cluster_count\": 89}\n",
      "Input (564): task_1445087491445_0007_r_000000 Task Transitioned from NEW to SCHEDULED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 88, \"cluster_size\": 11, \"template_mined\": \"task <:NUM:> <:NUM:> <:*:> <:NUM:> Task Transitioned from NEW to <:*:>\", \"cluster_count\": 89}\n",
      "Input (565): Num completed Tasks: 1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 90, \"cluster_size\": 1, \"template_mined\": \"Num completed Tasks: <:NUM:>\", \"cluster_count\": 90}\n",
      "Input (575): attempt_1445087491445_0007_r_000000_1000 TaskAttempt Transitioned from NEW to UNASSIGNED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 86, \"cluster_size\": 11, \"template_mined\": \"attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from NEW to <:*:>\", \"cluster_count\": 90}\n",
      "Input (576): reduceResourceRequest:<memory:1024, vCores:1>\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 91, \"cluster_size\": 1, \"template_mined\": \"reduceResourceRequest:<memory:<:NUM:>, vCores:<:NUM:>>\", \"cluster_count\": 91}\n",
      "Input (577): Before Scheduling: PendingReds:1 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:10 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 92, \"cluster_size\": 1, \"template_mined\": \"Before Scheduling: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\", \"cluster_count\": 92}\n",
      "Input (578): Recalculating schedule, headroom=<memory:16384, vCores:-26>\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 93, \"cluster_size\": 1, \"template_mined\": \"Recalculating schedule, headroom=<memory:<:NUM:>, vCores:<:NUM:>>\", \"cluster_count\": 93}\n",
      "Input (579): Reduce slow start threshold reached. Scheduling reduces.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 94, \"cluster_size\": 1, \"template_mined\": \"Reduce slow start threshold reached. Scheduling reduces.\", \"cluster_count\": 94}\n",
      "Input (580): All maps assigned. Ramping up all remaining reduces:1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 95, \"cluster_size\": 1, \"template_mined\": \"All maps assigned. Ramping up all remaining reduces:<:NUM:>\", \"cluster_count\": 95}\n",
      "Input (581): After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:1 AssignedMaps:0 AssignedReds:0 CompletedMaps:10 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 96, \"cluster_size\": 1, \"template_mined\": \"After Scheduling: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\", \"cluster_count\": 96}\n",
      "Input (582): getResources() for application_1445087491445_0007: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:16384, vCores:-26> knownNMs=6\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 97, \"cluster_size\": 1, \"template_mined\": \"getResources() for application <:NUM:> <:NUM:>: ask=<:NUM:> release= <:NUM:> newContainers=<:NUM:> finishedContainers=<:NUM:> resourcelimit=<memory:<:NUM:>, vCores:<:NUM:>> knownNMs=<:NUM:>\", \"cluster_count\": 97}\n",
      "Input (583): Got allocated containers 1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 98, \"cluster_size\": 1, \"template_mined\": \"Got allocated containers <:NUM:>\", \"cluster_count\": 98}\n",
      "Input (584): Assigned to reduce\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 99, \"cluster_size\": 1, \"template_mined\": \"Assigned to reduce\", \"cluster_count\": 99}\n",
      "Input (585): Assigned container container_1445087491445_0007_02_000003 to attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 100, \"cluster_size\": 1, \"template_mined\": \"Assigned container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> to attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 100}\n",
      "Input (588): The job-jar file on the remote FS is hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007/job.jar\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 101, \"cluster_size\": 1, \"template_mined\": \"The job-jar file on the remote FS is hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job.jar\", \"cluster_count\": 101}\n",
      "Input (589): The job-conf file on the remote FS is /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007/job.xml\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 101, \"cluster_size\": 2, \"template_mined\": \"The <:*:> file on the remote FS is <:*:> <:NUM:> <:*:>\", \"cluster_count\": 101}\n",
      "Input (590): Adding #0 tokens and #1 secret keys for NM use for launching container\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 102, \"cluster_size\": 1, \"template_mined\": \"Adding #<:NUM:> tokens and #<:NUM:> secret keys for NM use for launching container\", \"cluster_count\": 102}\n",
      "Input (591): Size of containertokens_dob is 1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 103, \"cluster_size\": 1, \"template_mined\": \"Size of containertokens dob is <:NUM:>\", \"cluster_count\": 103}\n",
      "Input (592): Putting shuffle token in serviceData\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 104, \"cluster_size\": 1, \"template_mined\": \"Putting shuffle token in serviceData\", \"cluster_count\": 104}\n",
      "Input (593): attempt_1445087491445_0007_r_000000_1000 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 86, \"cluster_size\": 12, \"template_mined\": \"attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> to <:*:>\", \"cluster_count\": 104}\n",
      "Input (594): Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445087491445_0007_02_000003 taskAttempt attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 105, \"cluster_size\": 1, \"template_mined\": \"Processing the event EventType: CONTAINER REMOTE LAUNCH for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> taskAttempt attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 105}\n",
      "Input (595): Launching attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 106, \"cluster_size\": 1, \"template_mined\": \"Launching attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 106}\n",
      "Input (596): Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:49130\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 107, \"cluster_size\": 1, \"template_mined\": \"Opening proxy : MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>\", \"cluster_count\": 107}\n",
      "Input (597): Shuffle port returned by ContainerManager for attempt_1445087491445_0007_r_000000_1000 : 13562\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 108, \"cluster_size\": 1, \"template_mined\": \"Shuffle port returned by ContainerManager for attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> : <:NUM:>\", \"cluster_count\": 108}\n",
      "Input (598): TaskAttempt: [attempt_1445087491445_0007_r_000000_1000] using containerId: [container_1445087491445_0007_02_000003 on NM: [MSRA-SA-39.fareast.corp.microsoft.com:49130]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 85, \"cluster_size\": 11, \"template_mined\": \"TaskAttempt: [attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>] using containerId: [container <:NUM:> <:NUM:> <:NUM:> <:NUM:> on NM: [MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>]\", \"cluster_count\": 108}\n",
      "Input (600): ATTEMPT_START task_1445087491445_0007_r_000000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 109, \"cluster_size\": 1, \"template_mined\": \"ATTEMPT START task <:NUM:> <:NUM:> r <:NUM:>\", \"cluster_count\": 109}\n",
      "Input (601): task_1445087491445_0007_r_000000 Task Transitioned from SCHEDULED to RUNNING\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 88, \"cluster_size\": 12, \"template_mined\": \"task <:NUM:> <:NUM:> <:*:> <:NUM:> Task Transitioned from <:*:> to <:*:>\", \"cluster_count\": 109}\n",
      "Input (603): Auth successful for job_1445087491445_0007 (auth:SIMPLE)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 110, \"cluster_size\": 1, \"template_mined\": \"Auth successful for job <:NUM:> <:NUM:> (auth:SIMPLE)\", \"cluster_count\": 110}\n",
      "Input (604): JVM with ID : jvm_1445087491445_0007_r_000003 asked for a task\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 111, \"cluster_size\": 1, \"template_mined\": \"JVM with ID : jvm <:NUM:> <:NUM:> r <:NUM:> asked for a task\", \"cluster_count\": 111}\n",
      "Input (605): JVM with ID: jvm_1445087491445_0007_r_000003 given task: attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 112, \"cluster_size\": 1, \"template_mined\": \"JVM with ID: jvm <:NUM:> <:NUM:> r <:NUM:> given task: attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 112}\n",
      "Input (606): MapCompletionEvents request from attempt_1445087491445_0007_r_000000_1000. startIndex 0 maxEvents 10000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 113, \"cluster_size\": 1, \"template_mined\": \"MapCompletionEvents request from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>. startIndex <:NUM:> maxEvents <:NUM:>\", \"cluster_count\": 113}\n",
      "Input (612): Progress of TaskAttempt attempt_1445087491445_0007_r_000000_1000 is : 0.13333334\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 114, \"cluster_size\": 1, \"template_mined\": \"Progress of TaskAttempt attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> is : <:NUM:>.<:NUM:>\", \"cluster_count\": 114}\n",
      "Input (699): DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743028_2240\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 115, \"cluster_size\": 1, \"template_mined\": \"DFSOutputStream ResponseProcessor exception for block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:>\", \"cluster_count\": 115}\n",
      "Input (700): Bad response ERROR for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743028_2240 from datanode 10.86.169.121:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 116, \"cluster_size\": 1, \"template_mined\": \"Bad response ERROR for block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> from datanode <:IP:>:<:NUM:>\", \"cluster_count\": 116}\n",
      "Input (702): Error Recovery for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743028_2240 in pipeline 10.190.173.170:50010, 10.86.169.121:50010: bad datanode 10.86.169.121:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 117, \"cluster_size\": 1, \"template_mined\": \"Error Recovery for block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> in pipeline <:IP:>:<:NUM:>, <:IP:>:<:NUM:>: bad datanode <:IP:>:<:NUM:>\", \"cluster_count\": 117}\n",
      "Input (819): Commit-pending state update from attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 118, \"cluster_size\": 1, \"template_mined\": \"Commit-pending state update from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 118}\n",
      "Input (820): attempt_1445087491445_0007_r_000000_1000 TaskAttempt Transitioned from RUNNING to COMMIT_PENDING\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 119, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> TaskAttempt Transitioned from RUNNING to COMMIT PENDING\", \"cluster_count\": 119}\n",
      "Input (821): attempt_1445087491445_0007_r_000000_1000 given a go for committing the task output.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 120, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> given a go for committing the task output.\", \"cluster_count\": 120}\n",
      "Input (822): Commit go/no-go request from attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 121, \"cluster_size\": 1, \"template_mined\": \"Commit go/no-go request from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 121}\n",
      "Input (823): Result of canCommit for attempt_1445087491445_0007_r_000000_1000:true\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 122, \"cluster_size\": 1, \"template_mined\": \"Result of canCommit for attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>:true\", \"cluster_count\": 122}\n",
      "Input (825): Done acknowledgement from attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 123, \"cluster_size\": 1, \"template_mined\": \"Done acknowledgement from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 123}\n",
      "Input (826): attempt_1445087491445_0007_r_000000_1000 TaskAttempt Transitioned from COMMIT_PENDING to SUCCESS_CONTAINER_CLEANUP\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 124, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> TaskAttempt Transitioned from COMMIT PENDING to SUCCESS CONTAINER CLEANUP\", \"cluster_count\": 124}\n",
      "Input (827): Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445087491445_0007_02_000003 taskAttempt attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 105, \"cluster_size\": 2, \"template_mined\": \"Processing the event EventType: CONTAINER REMOTE <:*:> for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> taskAttempt attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 124}\n",
      "Input (828): KILLING attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 125, \"cluster_size\": 1, \"template_mined\": \"KILLING attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 125}\n",
      "Input (830): attempt_1445087491445_0007_r_000000_1000 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 120, \"cluster_size\": 2, \"template_mined\": \"attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:>\", \"cluster_count\": 125}\n",
      "Input (831): Task succeeded with attempt attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 87, \"cluster_size\": 11, \"template_mined\": \"Task succeeded with attempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 125}\n",
      "Input (835): Processing the event EventType: JOB_COMMIT\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 82, \"cluster_size\": 2, \"template_mined\": \"Processing the event EventType: JOB <:*:>\", \"cluster_count\": 125}\n",
      "Input (836): Calling handler for JobFinishedEvent\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 126, \"cluster_size\": 1, \"template_mined\": \"Calling handler for JobFinishedEvent\", \"cluster_count\": 126}\n",
      "Input (838): We are finishing cleanly so this is the last retry\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 127, \"cluster_size\": 1, \"template_mined\": \"We are finishing cleanly so this is the last retry\", \"cluster_count\": 127}\n",
      "Input (839): Notify RMCommunicator isAMLastRetry: true\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 128, \"cluster_size\": 1, \"template_mined\": \"Notify RMCommunicator isAMLastRetry: true\", \"cluster_count\": 128}\n",
      "Input (840): RMCommunicator notified that shouldUnregistered is: true\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 129, \"cluster_size\": 1, \"template_mined\": \"RMCommunicator notified that shouldUnregistered is: true\", \"cluster_count\": 129}\n",
      "Input (841): Notify JHEH isAMLastRetry: true\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 128, \"cluster_size\": 2, \"template_mined\": \"Notify <:*:> isAMLastRetry: true\", \"cluster_count\": 129}\n",
      "Input (842): JobHistoryEventHandler notified that forceJobCompletion is true\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 130, \"cluster_size\": 1, \"template_mined\": \"JobHistoryEventHandler notified that forceJobCompletion is true\", \"cluster_count\": 130}\n",
      "Input (843): Calling stop for all the services\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 131, \"cluster_size\": 1, \"template_mined\": \"Calling stop for all the services\", \"cluster_count\": 131}\n",
      "Input (844): Stopping JobHistoryEventHandler. Size of the outstanding queue size is 0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 132, \"cluster_size\": 1, \"template_mined\": \"Stopping JobHistoryEventHandler. Size of the outstanding queue size is <:NUM:>\", \"cluster_count\": 132}\n",
      "Input (846): Copying hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007/job_1445087491445_0007_2.jhist to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 133, \"cluster_size\": 1, \"template_mined\": \"Copying hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist to hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\", \"cluster_count\": 133}\n",
      "Input (848): Copied to done location: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 134, \"cluster_size\": 1, \"template_mined\": \"Copied to done location: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\", \"cluster_count\": 134}\n",
      "Input (850): Copying hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007/job_1445087491445_0007_2_conf.xml to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 135, \"cluster_size\": 1, \"template_mined\": \"Copying hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:> conf.xml to hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\", \"cluster_count\": 135}\n",
      "Input (854): Moved tmp to done: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 136, \"cluster_size\": 1, \"template_mined\": \"Moved tmp to done: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\", \"cluster_count\": 136}\n",
      "Input (863): Stopped JobHistoryEventHandler. super.stop()\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 137, \"cluster_size\": 1, \"template_mined\": \"Stopped JobHistoryEventHandler. super.stop()\", \"cluster_count\": 137}\n",
      "Input (864): Setting job diagnostics to\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 138, \"cluster_size\": 1, \"template_mined\": \"Setting job diagnostics to\", \"cluster_count\": 138}\n",
      "Input (865): History url is http://MSRA-SA-41.fareast.corp.microsoft.com:19888/jobhistory/job/job_1445087491445_0007\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 139, \"cluster_size\": 1, \"template_mined\": \"History url is http://MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>/jobhistory/job/job <:NUM:> <:NUM:>\", \"cluster_count\": 139}\n",
      "Input (866): Waiting for application to be successfully unregistered.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 140, \"cluster_size\": 1, \"template_mined\": \"Waiting for application to be successfully unregistered.\", \"cluster_count\": 140}\n",
      "Input (867): Final Stats: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:1 CompletedMaps:10 CompletedReds:1 ContAlloc:1 ContRel:0 HostLocal:0 RackLocal:0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 141, \"cluster_size\": 1, \"template_mined\": \"Final Stats: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\", \"cluster_count\": 141}\n",
      "Input (868): Deleting staging directory hdfs://msra-sa-41:9000 /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 142, \"cluster_size\": 1, \"template_mined\": \"Deleting staging directory hdfs://msra-sa-<:NUM:>:<:NUM:> /tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>\", \"cluster_count\": 142}\n",
      "Input (869): Stopping server on 24300\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 143, \"cluster_size\": 1, \"template_mined\": \"Stopping server on <:NUM:>\", \"cluster_count\": 143}\n",
      "Input (870): Stopping IPC Server listener on 24300\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 144, \"cluster_size\": 1, \"template_mined\": \"Stopping IPC Server listener on <:NUM:>\", \"cluster_count\": 144}\n",
      "Input (871): TaskHeartbeatHandler thread interrupted\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 145, \"cluster_size\": 1, \"template_mined\": \"TaskHeartbeatHandler thread interrupted\", \"cluster_count\": 145}\n",
      "Input (872): Stopping IPC Server Responder\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 146, \"cluster_size\": 1, \"template_mined\": \"Stopping IPC Server Responder\", \"cluster_count\": 146}\n",
      "Input (1571): job_1445182159119_0016Job Transitioned from NEW to INITED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 54, \"cluster_size\": 6, \"template_mined\": \"job <:NUM:> <:*:> Transitioned from <:*:> to <:*:>\", \"cluster_count\": 146}\n",
      "Input (1588): Extract jar:file:/D:/hadoop-2.6.0-localbox/share/hadoop/yarn/hadoop-yarn-common-2.6.0-SNAPSHOT.jar!/webapps/mapreduce to C:\\Users\\msrabi\\AppData\\Local\\Temp\\2\\Jetty_0_0_0_0_55793_mapreduce____.11rul4\\webapp\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 69, \"cluster_size\": 2, \"template_mined\": \"Extract jar:file:/D:/hadoop-<:NUM:>.<:NUM:>.<:NUM:>-localbox/share/hadoop/yarn/hadoop-yarn-common-<:NUM:>.<:NUM:>.<:NUM:>-SNAPSHOT.jar!/webapps/mapreduce to C:\\\\Users\\\\msrabi\\\\AppData\\\\Local\\\\Temp\\\\<:NUM:>\\\\Jetty <:NUM:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> mapreduce <:*:>\", \"cluster_count\": 146}\n",
      "Input (1600): Connecting to ResourceManager at msra-sa-41/10.190.173.170:8030\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 77, \"cluster_size\": 2, \"template_mined\": \"Connecting to ResourceManager at <:*:>\", \"cluster_count\": 146}\n",
      "Input (1650): mapResourceRequest:<memory:1024, vCores:1>\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 147, \"cluster_size\": 1, \"template_mined\": \"mapResourceRequest:<memory:<:NUM:>, vCores:<:NUM:>>\", \"cluster_count\": 147}\n",
      "Input (1656): Reduce slow start threshold not met. completedMapsForReduceSlowstart 1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 148, \"cluster_size\": 1, \"template_mined\": \"Reduce slow start threshold not met. completedMapsForReduceSlowstart <:NUM:>\", \"cluster_count\": 148}\n",
      "Input (1658): Assigned container container_1445182159119_0016_01_000002 to attempt_1445182159119_0016_m_000000_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 100, \"cluster_size\": 2, \"template_mined\": \"Assigned container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> to attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 148}\n",
      "Input (1678): Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445182159119_0016_01_000002 taskAttempt attempt_1445182159119_0016_m_000000_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 105, \"cluster_size\": 3, \"template_mined\": \"Processing the event EventType: CONTAINER REMOTE <:*:> for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> taskAttempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 148}\n",
      "Input (1682): Launching attempt_1445182159119_0016_m_000003_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 106, \"cluster_size\": 2, \"template_mined\": \"Launching attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 148}\n",
      "Input (1690): Shuffle port returned by ContainerManager for attempt_1445182159119_0016_m_000002_0 : 13562\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 108, \"cluster_size\": 2, \"template_mined\": \"Shuffle port returned by ContainerManager for attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> : <:NUM:>\", \"cluster_count\": 148}\n",
      "Input (1702): ATTEMPT_START task_1445182159119_0016_m_000003\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 109, \"cluster_size\": 2, \"template_mined\": \"ATTEMPT START task <:NUM:> <:NUM:> <:*:> <:NUM:>\", \"cluster_count\": 148}\n",
      "Input (1725): Opening proxy : MININT-FNANLI5.fareast.corp.microsoft.com:59190\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 107, \"cluster_size\": 7, \"template_mined\": \"Opening proxy : <:*:>\", \"cluster_count\": 148}\n",
      "Input (1727): TaskAttempt: [attempt_1445182159119_0016_m_000004_0] using containerId: [container_1445182159119_0016_01_000006 on NM: [MININT-FNANLI5.fareast.corp.microsoft.com:59190]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 85, \"cluster_size\": 16, \"template_mined\": \"TaskAttempt: [attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>] using containerId: [container <:NUM:> <:NUM:> <:NUM:> <:NUM:> on NM: <:*:>\", \"cluster_count\": 148}\n",
      "Input (1749): JVM with ID : jvm_1445182159119_0016_m_000002 asked for a task\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 111, \"cluster_size\": 2, \"template_mined\": \"JVM with ID : jvm <:NUM:> <:NUM:> <:*:> <:NUM:> asked for a task\", \"cluster_count\": 148}\n",
      "Input (1750): JVM with ID: jvm_1445182159119_0016_m_000002 given task: attempt_1445182159119_0016_m_000000_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 112, \"cluster_size\": 2, \"template_mined\": \"JVM with ID: jvm <:NUM:> <:NUM:> <:*:> <:NUM:> given task: attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 148}\n",
      "Input (1828): Cannot assign container Container: [ContainerId: container_1445182159119_0016_01_000012, NodeId: MININT-75DGDAM1.fareast.corp.microsoft.com:58081, NodeHttpAddress: MININT-75DGDAM1.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.86.165.66:58081 }, ] for a map as either  container memory less than required <memory:1024, vCores:1> or no pending map tasks - maps.isEmpty=true\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 149, \"cluster_size\": 1, \"template_mined\": \"Cannot assign container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: MININT-75DGDAM1.fareast.corp.microsoft.com:<:NUM:>, NodeHttpAddress: MININT-75DGDAM1.fareast.corp.microsoft.com:<:NUM:>, Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] for a map as either container memory less than required <memory:<:NUM:>, vCores:<:NUM:>> or no pending map tasks - maps.isEmpty=true\", \"cluster_count\": 149}\n",
      "Input (1829): Cannot assign container Container: [ContainerId: container_1445182159119_0016_01_000013, NodeId: MININT-FNANLI5.fareast.corp.microsoft.com:59190, NodeHttpAddress: MININT-FNANLI5.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.86.169.121:59190 }, ] for a map as either  container memory less than required <memory:1024, vCores:1> or no pending map tasks - maps.isEmpty=true\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 149, \"cluster_size\": 2, \"template_mined\": \"Cannot assign container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] for a map as either container memory less than required <memory:<:NUM:>, vCores:<:NUM:>> or no pending map tasks - maps.isEmpty=true\", \"cluster_count\": 149}\n",
      "Input (1837): Received completed container container_1445182159119_0016_01_000013\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 150, \"cluster_size\": 1, \"template_mined\": \"Received completed container container <:NUM:> <:NUM:> <:NUM:> <:NUM:>\", \"cluster_count\": 150}\n",
      "Input (1838): Container complete event for unknown container id container_1445182159119_0016_01_000013\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 151, \"cluster_size\": 1, \"template_mined\": \"Container complete event for unknown container id container <:NUM:> <:NUM:> <:NUM:> <:NUM:>\", \"cluster_count\": 151}\n",
      "Input (1851): Progress of TaskAttempt attempt_1445182159119_0016_m_000000_0 is : 0.10291508\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 114, \"cluster_size\": 164, \"template_mined\": \"Progress of TaskAttempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> is : <:NUM:>.<:NUM:>\", \"cluster_count\": 151}\n",
      "Input (2195): Socket Reader #1 for port 55796: readAndProcess from client 10.86.165.66 threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 152, \"cluster_size\": 1, \"template_mined\": \"Socket Reader #<:NUM:> for port <:NUM:>: readAndProcess from client <:IP:> threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]\", \"cluster_count\": 152}\n",
      "Input (2196): An existing connection was forcibly closed by the remote host\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 153, \"cluster_size\": 1, \"template_mined\": \"An existing connection was forcibly closed by the remote host\", \"cluster_count\": 153}\n",
      "Input (2246): Done acknowledgement from attempt_1445182159119_0016_m_000003_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 123, \"cluster_size\": 2, \"template_mined\": \"Done acknowledgement from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 153}\n",
      "Input (2247): attempt_1445182159119_0016_m_000003_0 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 154, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> TaskAttempt Transitioned from RUNNING to SUCCESS CONTAINER CLEANUP\", \"cluster_count\": 154}\n",
      "Input (2249): KILLING attempt_1445182159119_0016_m_000003_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 125, \"cluster_size\": 2, \"template_mined\": \"KILLING attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 154}\n",
      "Input (2251): attempt_1445182159119_0016_m_000003_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 154, \"cluster_size\": 2, \"template_mined\": \"attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> <:*:> <:*:> <:*:> <:*:>\", \"cluster_count\": 154}\n",
      "Input (2255): DefaultSpeculator.addSpeculativeAttempt -- we are speculating task_1445182159119_0016_m_000004\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 155, \"cluster_size\": 1, \"template_mined\": \"DefaultSpeculator.addSpeculativeAttempt -- we are speculating task <:NUM:> <:NUM:> m <:NUM:>\", \"cluster_count\": 155}\n",
      "Input (2256): We launched 1 speculations.  Sleeping 15000 milliseconds.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 156, \"cluster_size\": 1, \"template_mined\": \"We launched <:NUM:> speculations. Sleeping <:NUM:> milliseconds.\", \"cluster_count\": 156}\n",
      "Input (2257): Scheduling a redundant attempt for task task_1445182159119_0016_m_000004\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 157, \"cluster_size\": 1, \"template_mined\": \"Scheduling a redundant attempt for task task <:NUM:> <:NUM:> m <:NUM:>\", \"cluster_count\": 157}\n",
      "Input (2266): completedMapPercent 0.1 totalResourceLimit:<memory:10240, vCores:-17> finalMapResourceLimit:<memory:9216, vCores:-16> finalReduceResourceLimit:<memory:1024, vCores:-1> netScheduledMapResource:<memory:11264, vCores:11> netScheduledReduceResource:<memory:0, vCores:0>\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 158, \"cluster_size\": 1, \"template_mined\": \"completedMapPercent <:NUM:>.<:NUM:> totalResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> finalMapResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> finalReduceResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> netScheduledMapResource:<memory:<:NUM:>, vCores:<:NUM:>> netScheduledReduceResource:<memory:<:NUM:>, vCores:<:NUM:>>\", \"cluster_count\": 158}\n",
      "Input (2267): Ramping up 1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 159, \"cluster_size\": 1, \"template_mined\": \"Ramping up <:NUM:>\", \"cluster_count\": 159}\n",
      "Input (2284): Diagnostics report from attempt_1445182159119_0016_m_000003_0: Container killed by the ApplicationMaster.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 160, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Container killed by the ApplicationMaster.\", \"cluster_count\": 160}\n",
      "Input (2771): Issuing kill to other attempt attempt_1445182159119_0016_m_000004_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 161, \"cluster_size\": 1, \"template_mined\": \"Issuing kill to other attempt attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 161}\n",
      "Input (2801): attempt_1445182159119_0016_m_000004_0 TaskAttempt Transitioned from KILL_CONTAINER_CLEANUP to KILL_TASK_CLEANUP\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 162, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> TaskAttempt Transitioned from KILL CONTAINER CLEANUP to KILL TASK CLEANUP\", \"cluster_count\": 162}\n",
      "Input (2802): Processing the event EventType: TASK_ABORT\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 82, \"cluster_size\": 4, \"template_mined\": \"Processing the event EventType: <:*:> <:*:>\", \"cluster_count\": 162}\n",
      "Input (2803): Could not delete hdfs://msra-sa-41:9000/pageout/out1/_temporary/1/_temporary/attempt_1445182159119_0016_m_000004_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 163, \"cluster_size\": 1, \"template_mined\": \"Could not delete hdfs://msra-sa-<:NUM:>:<:NUM:>/pageout/out1/ temporary/<:NUM:>/ temporary/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 163}\n",
      "Input (3287): Retrying connect to server: MININT-75DGDAM1.fareast.corp.microsoft.com/10.86.165.66:58081. Already tried 0 time(s); maxRetries=45\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 164, \"cluster_size\": 1, \"template_mined\": \"Retrying connect to server: MININT-75DGDAM1.fareast.corp.microsoft.com/<:IP:>:<:NUM:>. Already tried <:NUM:> time(s); maxRetries=<:NUM:>\", \"cluster_count\": 164}\n",
      "Input (3667): Retrying connect to server: MININT-75DGDAM1.fareast.corp.microsoft.com/10.86.165.66:58081. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 165, \"cluster_size\": 1, \"template_mined\": \"Retrying connect to server: MININT-75DGDAM1.fareast.corp.microsoft.com/<:IP:>:<:NUM:>. Already tried <:NUM:> time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=<:NUM:>, sleepTime=<:NUM:> MILLISECONDS)\", \"cluster_count\": 165}\n",
      "Input (3913): Diagnostics report from attempt_1445182159119_0016_m_000009_0: AttemptID:attempt_1445182159119_0016_m_000009_0 Timed out after 600 secs\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 166, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: AttemptID:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> Timed out after <:NUM:> secs\", \"cluster_count\": 166}\n",
      "Input (3993): Killing taskAttempt:attempt_1445182159119_0016_m_000005_0 because it is running on unusable node:MININT-75DGDAM1.fareast.corp.microsoft.com:58081\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 167, \"cluster_size\": 1, \"template_mined\": \"Killing taskAttempt:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> because it is running on unusable node:MININT-75DGDAM1.fareast.corp.microsoft.com:<:NUM:>\", \"cluster_count\": 167}\n",
      "Input (4000): Diagnostics report from attempt_1445182159119_0016_m_000009_0: Container released on a *lost* node\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 168, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Container released on a *lost* node\", \"cluster_count\": 168}\n",
      "Input (5523): Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"04DN8IQ/10.86.164.9\"; destination host is: \"minint-fnanli5.fareast.corp.microsoft.com\":52839;\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 169, \"cluster_size\": 1, \"template_mined\": \"Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \\\"04DN8IQ/<:IP:>\\\"; destination host is: \\\"minint-fnanli5.fareast.corp.microsoft.com\\\":<:NUM:>;\", \"cluster_count\": 169}\n",
      "Input (5531): java.io.IOException: An existing connection was forcibly closed by the remote host\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 170, \"cluster_size\": 1, \"template_mined\": \"java.io.IOException: An existing connection was forcibly closed by the remote host\", \"cluster_count\": 170}\n",
      "Input (5552): Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 0 time(s); maxRetries=45\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 164, \"cluster_size\": 12, \"template_mined\": \"Retrying connect to server: <:*:> Already tried <:NUM:> time(s); maxRetries=<:NUM:>\", \"cluster_count\": 170}\n",
      "Input (6134): ReduceTask metrics system started\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 171, \"cluster_size\": 1, \"template_mined\": \"ReduceTask metrics system started\", \"cluster_count\": 171}\n",
      "Input (6142): Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@64bfc93b\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 172, \"cluster_size\": 1, \"template_mined\": \"Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@64bfc93b\", \"cluster_count\": 172}\n",
      "Input (6143): MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 173, \"cluster_size\": 1, \"template_mined\": \"MergerManager: memoryLimit=<:NUM:>, maxSingleShuffleLimit=<:NUM:>, mergeThreshold=<:NUM:>, ioSortFactor=<:NUM:>, memToMemMergeOutputsThreshold=<:NUM:>\", \"cluster_count\": 173}\n",
      "Input (6145): attempt_1445094324383_0005_r_000000_0: Got 6 new map-outputs\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 174, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>: Got <:NUM:> new map-outputs\", \"cluster_count\": 174}\n",
      "Input (6146): Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 6 to fetcher#4\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 175, \"cluster_size\": 1, \"template_mined\": \"Assigning MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:> with <:NUM:> to fetcher#<:NUM:>\", \"cluster_count\": 175}\n",
      "Input (6147): assigned 6 of 6 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#4\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 176, \"cluster_size\": 1, \"template_mined\": \"assigned <:NUM:> of <:NUM:> to MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:> to fetcher#<:NUM:>\", \"cluster_count\": 176}\n",
      "Input (6148): for url=13562/mapOutput?job=job_1445094324383_0005&reduce=0&map=attempt_1445094324383_0005_m_000000_0,attempt_1445094324383_0005_m_000005_0,attempt_1445094324383_0005_m_000003_0,attempt_1445094324383_0005_m_000001_0,attempt_1445094324383_0005_m_000004_0,attempt_1445094324383_0005_m_000002_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 177, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 177}\n",
      "Input (6149): attempt_1445094324383_0005_m_000000_0: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (32663142)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 178, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Shuffling to disk since <:NUM:> is greater than maxSingleShuffleLimit (<:NUM:>)\", \"cluster_count\": 178}\n",
      "Input (6150): fetcher#4 about to shuffle output of map attempt_1445094324383_0005_m_000000_0 decomp: 216988123 len: 216988127 to DISK\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 179, \"cluster_size\": 1, \"template_mined\": \"fetcher#<:NUM:> about to shuffle output of map attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> decomp: <:NUM:> len: <:NUM:> to DISK\", \"cluster_count\": 179}\n",
      "Input (6151): Ignoring obsolete output of KILLED map-task: 'attempt_1445094324383_0005_m_000006_1'\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 180, \"cluster_size\": 1, \"template_mined\": \"Ignoring obsolete output of KILLED map-task: 'attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>'\", \"cluster_count\": 180}\n",
      "Input (6424): Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@49c051a1\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 172, \"cluster_size\": 2, \"template_mined\": \"Using ShuffleConsumerPlugin: <:*:>\", \"cluster_count\": 180}\n",
      "Input (6430): for url=13562/mapOutput?job=job_1445094324383_0004&reduce=0&map=attempt_1445094324383_0004_m_000001_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 181, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 181}\n",
      "Input (6534): Instantiated MRClientService at MININT-FNANLI5.fareast.corp.microsoft.com/10.86.169.121:61543\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 61, \"cluster_size\": 3, \"template_mined\": \"Instantiated MRClientService at <:*:>\", \"cluster_count\": 181}\n",
      "Input (6546): Extract jar:file:/D:/hadoop-2.6.0-localbox/share/hadoop/yarn/hadoop-yarn-common-2.6.0-SNAPSHOT.jar!/webapps/mapreduce to C:\\Users\\msrabi\\AppData\\Local\\Temp\\Jetty_0_0_0_0_61550_mapreduce____.mrek2k\\webapp\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 69, \"cluster_size\": 3, \"template_mined\": \"Extract jar:file:/D:/hadoop-<:NUM:>.<:NUM:>.<:NUM:>-localbox/share/hadoop/yarn/hadoop-yarn-common-<:NUM:>.<:NUM:>.<:NUM:>-SNAPSHOT.jar!/webapps/mapreduce to <:*:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> mapreduce <:*:>\", \"cluster_count\": 181}\n",
      "Input (7469): attempt_1445182159119_0001_r_000000_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 154, \"cluster_size\": 48, \"template_mined\": \"attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> <:*:> <:*:> <:*:> <:*:>\", \"cluster_count\": 181}\n",
      "Input (7478): Diagnostics report from attempt_1445182159119_0001_r_000000_0: Container killed by the ApplicationMaster.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 160, \"cluster_size\": 23, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: Container killed by the ApplicationMaster.\", \"cluster_count\": 181}\n",
      "Input (7508): History url is http://MININT-FNANLI5.fareast.corp.microsoft.com:19888/jobhistory/job/job_1445182159119_0001\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 139, \"cluster_size\": 2, \"template_mined\": \"History url is <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 181}\n",
      "Input (8504): Could not delete hdfs://msra-sa-41:9000/pageout/out5/_temporary/1/_temporary/attempt_1445062781478_0020_m_000001_1\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 163, \"cluster_size\": 4, \"template_mined\": \"Could not delete <:*:> temporary/<:NUM:>/ temporary/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 181}\n",
      "Input (9881): Diagnostics report from attempt_1445076437777_0002_m_000006_0:\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 182, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>:\", \"cluster_count\": 182}\n",
      "Processing line: 10000, rate 7591.4 lines/sec, 182 clusters so far.\n",
      "Input (10972): Slow ReadProcessor read fields took 54719ms (threshold=30000ms); ack: seqno: -2 status: SUCCESS status: ERROR downstreamAckTimeNanos: 0, targets: [10.86.164.15:50010, 10.86.169.121:50010]\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 183, \"cluster_size\": 1, \"template_mined\": \"Slow ReadProcessor read fields took 54719ms (threshold=30000ms); ack: seqno: <:NUM:> status: SUCCESS status: ERROR downstreamAckTimeNanos: <:NUM:>, targets: [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>]\", \"cluster_count\": 183}\n",
      "Input (10977): In stop, writing event TASK_FINISHED\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 184, \"cluster_size\": 1, \"template_mined\": \"In stop, writing event TASK FINISHED\", \"cluster_count\": 184}\n",
      "Input (10978): In stop, writing event JOB_FINISHED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 184, \"cluster_size\": 2, \"template_mined\": \"In stop, writing event <:*:> FINISHED\", \"cluster_count\": 184}\n",
      "Input (14012): Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MININT-75DGDAM1/10.86.165.66\"; destination host is: \"minint-fnanli5.fareast.corp.microsoft.com\":52839;\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 169, \"cluster_size\": 2, \"template_mined\": \"Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: \\\"minint-fnanli5.fareast.corp.microsoft.com\\\":<:NUM:>;\", \"cluster_count\": 184}\n",
      "Input (17015): Read 172334808 bytes from map-output for attempt_1445094324383_0001_m_000009_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 185, \"cluster_size\": 1, \"template_mined\": \"Read <:NUM:> bytes from map-output for attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 185}\n",
      "Input (17016): MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 5943ms\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 186, \"cluster_size\": 1, \"template_mined\": \"MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:> freed by fetcher#<:NUM:> in 5943ms\", \"cluster_count\": 186}\n",
      "Input (17019): for url=13562/mapOutput?job=job_1445094324383_0001&reduce=0&map=attempt_1445094324383_0001_m_000001_0,attempt_1445094324383_0001_m_000008_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 187, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 187}\n",
      "Input (17023): MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 8239ms\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 186, \"cluster_size\": 2, \"template_mined\": \"MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:> freed by fetcher#<:NUM:> in <:*:>\", \"cluster_count\": 187}\n",
      "Input (17026): for url=13562/mapOutput?job=job_1445094324383_0001&reduce=0&map=attempt_1445094324383_0001_m_000003_0,attempt_1445094324383_0001_m_000006_0,attempt_1445094324383_0001_m_000005_0,attempt_1445094324383_0001_m_000004_0,attempt_1445094324383_0001_m_000007_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 188, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 188}\n",
      "Input (17055): EventFetcher is interrupted.. Returning\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 189, \"cluster_size\": 1, \"template_mined\": \"EventFetcher is interrupted.. Returning\", \"cluster_count\": 189}\n",
      "Input (17057): finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 190, \"cluster_size\": 1, \"template_mined\": \"finalMerge called with <:NUM:> in-memory map-outputs and <:NUM:> on-disk map-outputs\", \"cluster_count\": 190}\n",
      "Input (17058): Merging 10 files, 2125289789 bytes from disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 191, \"cluster_size\": 1, \"template_mined\": \"Merging <:NUM:> files, <:NUM:> bytes from disk\", \"cluster_count\": 191}\n",
      "Input (17059): Merging 0 segments, 0 bytes from memory into reduce\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 192, \"cluster_size\": 1, \"template_mined\": \"Merging <:NUM:> segments, <:NUM:> bytes from memory into reduce\", \"cluster_count\": 192}\n",
      "Input (17062): mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 193, \"cluster_size\": 1, \"template_mined\": \"mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\", \"cluster_count\": 193}\n",
      "Input (17063): Task:attempt_1445094324383_0001_r_000000_0 is done. And is in the process of committing\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 26, \"cluster_size\": 36, \"template_mined\": \"Task:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> is done. And is in the process of committing\", \"cluster_count\": 193}\n",
      "Input (17064): Task attempt_1445094324383_0001_r_000000_0 is allowed to commit now\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 194, \"cluster_size\": 1, \"template_mined\": \"Task attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> is allowed to commit now\", \"cluster_count\": 194}\n",
      "Input (17065): Saved output of task 'attempt_1445094324383_0001_r_000000_0' to hdfs://msra-sa-41:9000/out/out4/_temporary/1/task_1445094324383_0001_r_000000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 195, \"cluster_size\": 1, \"template_mined\": \"Saved output of task 'attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>' to hdfs://msra-sa-<:NUM:>:<:NUM:>/out/out4/ temporary/<:NUM:>/task <:NUM:> <:NUM:> r <:NUM:>\", \"cluster_count\": 195}\n",
      "Input (17066): Task 'attempt_1445094324383_0001_r_000000_0' done.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 27, \"cluster_size\": 36, \"template_mined\": \"Task 'attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>' done.\", \"cluster_count\": 195}\n",
      "Input (18036): for url=13562/mapOutput?job=job_1445087491445_0007&reduce=0&map=attempt_1445087491445_0007_m_000006_0,attempt_1445087491445_0007_m_000007_0,attempt_1445087491445_0007_m_000008_0,attempt_1445087491445_0007_m_000009_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 196, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 196}\n",
      "Input (18075): Exception in createBlockOutputStream\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 197, \"cluster_size\": 1, \"template_mined\": \"Exception in createBlockOutputStream\", \"cluster_count\": 197}\n",
      "Input (18076): Bad connect ack with firstBadLink as 10.86.169.121:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 198, \"cluster_size\": 1, \"template_mined\": \"Bad connect ack with firstBadLink as <:IP:>:<:NUM:>\", \"cluster_count\": 198}\n",
      "Input (18080): Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073743056_2272\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 199, \"cluster_size\": 1, \"template_mined\": \"Abandoning BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:>\", \"cluster_count\": 199}\n",
      "Input (18081): Excluding datanode 10.86.169.121:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 200, \"cluster_size\": 1, \"template_mined\": \"Excluding datanode <:IP:>:<:NUM:>\", \"cluster_count\": 200}\n",
      "Input (18084): Saved output of task 'attempt_1445087491445_0007_r_000000_1000' to hdfs://msra-sa-41:9000/out/out1/_temporary/2/task_1445087491445_0007_r_000000\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 195, \"cluster_size\": 2, \"template_mined\": \"Saved output of task 'attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>' to <:*:> temporary/<:NUM:>/task <:NUM:> <:NUM:> r <:NUM:>\", \"cluster_count\": 200}\n",
      "Processing line: 20000, rate 11991.7 lines/sec, 200 clusters so far.\n",
      "Input (21068): Killing taskAttempt:attempt_1445094324383_0002_r_000000_0 because it is running on unusable node:04DN8IQ.fareast.corp.microsoft.com:55452\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 167, \"cluster_size\": 4, \"template_mined\": \"Killing taskAttempt:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> because it is running on unusable <:*:>\", \"cluster_count\": 200}\n",
      "Input (21072): Diagnostics report from attempt_1445094324383_0002_r_000000_0: Container released on a *lost* node\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 168, \"cluster_size\": 4, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: Container released on a *lost* node\", \"cluster_count\": 200}\n",
      "Input (21076): attempt_1445094324383_0002_r_000000_0 TaskAttempt Transitioned from KILL_CONTAINER_CLEANUP to KILL_TASK_CLEANUP\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 162, \"cluster_size\": 24, \"template_mined\": \"attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from KILL CONTAINER CLEANUP to KILL TASK CLEANUP\", \"cluster_count\": 200}\n",
      "Input (21731): Could not delete hdfs://msra-sa-41:9000/out/out1/_temporary/1/_temporary/attempt_1445094324383_0002_r_000000_1\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 163, \"cluster_size\": 24, \"template_mined\": \"Could not delete <:*:> temporary/<:NUM:>/ temporary/attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 200}\n",
      "Input (21986): Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#4\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 175, \"cluster_size\": 14, \"template_mined\": \"Assigning <:*:> with <:NUM:> to fetcher#<:NUM:>\", \"cluster_count\": 200}\n",
      "Input (21988): assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#4\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 176, \"cluster_size\": 14, \"template_mined\": \"assigned <:NUM:> of <:NUM:> to <:*:> to fetcher#<:NUM:>\", \"cluster_count\": 200}\n",
      "Input (21999): 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 857ms\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 201, \"cluster_size\": 1, \"template_mined\": \"04DN8IQ.fareast.corp.microsoft.com:<:NUM:> freed by fetcher#<:NUM:> in 857ms\", \"cluster_count\": 201}\n",
      "Input (22037): 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 603ms\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 201, \"cluster_size\": 2, \"template_mined\": \"04DN8IQ.fareast.corp.microsoft.com:<:NUM:> freed by fetcher#<:NUM:> in <:*:>\", \"cluster_count\": 201}\n",
      "Input (24095): Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MININT-FNANLI5/127.0.0.1\"; destination host is: \"10.190.173.170\":29630;\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 169, \"cluster_size\": 4, \"template_mined\": \"Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\", \"cluster_count\": 201}\n",
      "Input (24122): Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 165, \"cluster_size\": 151, \"template_mined\": \"Retrying connect to server: <:*:> Already tried <:NUM:> time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=<:NUM:>, sleepTime=<:NUM:> MILLISECONDS)\", \"cluster_count\": 201}\n",
      "Input (24132): Communication exception: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to 10.190.173.170:29630 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 202, \"cluster_size\": 1, \"template_mined\": \"Communication exception: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to <:IP:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 202}\n",
      "Input (24145): java.net.NoRouteToHostException: No route to host: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 203, \"cluster_size\": 1, \"template_mined\": \"java.net.NoRouteToHostException: No route to host: no further information\", \"cluster_count\": 203}\n",
      "Input (24194): Process Thread Dump: Communication exception\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 204, \"cluster_size\": 1, \"template_mined\": \"Process Thread Dump: Communication exception\", \"cluster_count\": 204}\n",
      "Input (24197): WAITING\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 205, \"cluster_size\": 1, \"template_mined\": \"WAITING\", \"cluster_count\": 205}\n",
      "Input (24198): 0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 206, \"cluster_size\": 1, \"template_mined\": \"<:NUM:>\", \"cluster_count\": 206}\n",
      "Input (24207): TIMED_WAITING\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 207, \"cluster_size\": 1, \"template_mined\": \"TIMED WAITING\", \"cluster_count\": 207}\n",
      "Input (24217): RUNNABLE\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 208, \"cluster_size\": 1, \"template_mined\": \"RUNNABLE\", \"cluster_count\": 208}\n",
      "Input (24326): Last retry, killing attempt_1445144423722_0022_m_000004_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 209, \"cluster_size\": 1, \"template_mined\": \"Last retry, killing attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 209}\n",
      "Input (24420): for url=13562/mapOutput?job=job_1445144423722_0022&reduce=0&map=attempt_1445144423722_0022_m_000002_0,attempt_1445144423722_0022_m_000000_0,attempt_1445144423722_0022_m_000001_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 210, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 210}\n",
      "Input (25583): Runnning cleanup for the task\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 211, \"cluster_size\": 1, \"template_mined\": \"Runnning cleanup for the task\", \"cluster_count\": 211}\n",
      "Input (28556): Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 212, \"cluster_size\": 1, \"template_mined\": \"Address change detected. Old: msra-sa-<:NUM:>/<:IP:>:<:NUM:> New: msra-sa-<:NUM:>:<:NUM:>\", \"cluster_count\": 212}\n",
      "Input (28557): Failed to renew lease for [DFSClient_NONMAPREDUCE_483047941_1] for 46 seconds.  Will retry shortly ...\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 213, \"cluster_size\": 1, \"template_mined\": \"Failed to renew lease for [DFSClient NONMAPREDUCE <:NUM:> <:NUM:>] for <:NUM:> seconds. Will retry shortly ...\", \"cluster_count\": 213}\n",
      "Input (28558): No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 214, \"cluster_size\": 1, \"template_mined\": \"No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 214}\n",
      "Input (29238): ERROR IN CONTACTING RM.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 215, \"cluster_size\": 1, \"template_mined\": \"ERROR IN CONTACTING RM.\", \"cluster_count\": 215}\n",
      "Input (29239): Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MININT-FNANLI5/127.0.0.1\"; destination host is: \"msra-sa-41\":8030;\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 216, \"cluster_size\": 1, \"template_mined\": \"Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \\\"MININT-FNANLI5/<:IP:>\\\"; destination host is: \\\"msra-sa-<:NUM:>\\\":<:NUM:>;\", \"cluster_count\": 216}\n",
      "Input (29352): Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \"MININT-FNANLI5/127.0.0.1\"; destination host is: \"msra-sa-41\":8030;\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 217, \"cluster_size\": 1, \"template_mined\": \"Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \\\"MININT-FNANLI5/<:IP:>\\\"; destination host is: \\\"msra-sa-<:NUM:>\\\":<:NUM:>;\", \"cluster_count\": 217}\n",
      "Input (29370): java.io.IOException: Couldn't set up IO streams\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 218, \"cluster_size\": 1, \"template_mined\": \"java.io.IOException: Couldn't set up IO streams\", \"cluster_count\": 218}\n",
      "Input (29376): java.nio.channels.UnresolvedAddressException\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 219, \"cluster_size\": 1, \"template_mined\": \"java.nio.channels.UnresolvedAddressException\", \"cluster_count\": 219}\n",
      "Processing line: 30000, rate 15856.5 lines/sec, 219 clusters so far.\n",
      "Input (30801): Ignoring obsolete output of FAILED map-task: 'attempt_1445182159119_0002_m_000007_0'\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 180, \"cluster_size\": 3, \"template_mined\": \"Ignoring obsolete output of <:*:> map-task: 'attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>'\", \"cluster_count\": 219}\n",
      "Input (30847): Exception in getting events\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 220, \"cluster_size\": 1, \"template_mined\": \"Exception in getting events\", \"cluster_count\": 220}\n",
      "Input (30848): Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MSRA-SA-41/10.190.173.170\"; destination host is: \"minint-fnanli5.fareast.corp.microsoft.com\":52839;\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 216, \"cluster_size\": 2, \"template_mined\": \"Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\", \"cluster_count\": 220}\n",
      "Input (31728): MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 36889ms\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 201, \"cluster_size\": 10, \"template_mined\": \"<:*:> freed by fetcher#<:NUM:> in <:*:>\", \"cluster_count\": 220}\n",
      "Input (37555): TaskAttempt killed because it ran on unusable node 04DN8IQ.fareast.corp.microsoft.com:55452. AttemptId:attempt_1445087491445_0004_m_000002_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 221, \"cluster_size\": 1, \"template_mined\": \"TaskAttempt killed because it ran on unusable node 04DN8IQ.fareast.corp.microsoft.com:<:NUM:>. AttemptId:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 221}\n",
      "Input (38182): TaskAttempt killed because it ran on unusable node MININT-FNANLI5.fareast.corp.microsoft.com:55629. AttemptId:attempt_1445087491445_0004_m_000005_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 221, \"cluster_size\": 7, \"template_mined\": \"TaskAttempt killed because it ran on unusable node <:*:> AttemptId:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 221}\n",
      "Input (38193): Ramping down all scheduled reduces:0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 222, \"cluster_size\": 1, \"template_mined\": \"Ramping down all scheduled reduces:<:NUM:>\", \"cluster_count\": 222}\n",
      "Input (38194): Going to preempt 1 due to lack of space for maps\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 223, \"cluster_size\": 1, \"template_mined\": \"Going to preempt <:NUM:> due to lack of space for maps\", \"cluster_count\": 223}\n",
      "Input (38195): Preempting attempt_1445087491445_0004_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 224, \"cluster_size\": 1, \"template_mined\": \"Preempting attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 224}\n",
      "Input (38222): Reduce preemption successful attempt_1445087491445_0004_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 225, \"cluster_size\": 1, \"template_mined\": \"Reduce preemption successful attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 225}\n",
      "Processing line: 40000, rate 16942.7 lines/sec, 225 clusters so far.\n",
      "Input (40973): Merging 4 intermediate segments out of a total of 13\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 226, \"cluster_size\": 1, \"template_mined\": \"Merging <:NUM:> intermediate segments out of a total of <:NUM:>\", \"cluster_count\": 226}\n",
      "Input (41035): Communication exception: java.net.ConnectException: Call From MSRA-SA-39/172.22.149.145 to minint-fnanli5.fareast.corp.microsoft.com:49594 failed on connection exception: java.net.ConnectException: Connection timed out: no further information; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 227, \"cluster_size\": 1, \"template_mined\": \"Communication exception: java.net.ConnectException: Call From MSRA-SA-<:NUM:>/<:IP:> to minint-fnanli5.fareast.corp.microsoft.com:<:NUM:> failed on connection exception: java.net.ConnectException: Connection timed out: no further information; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused\", \"cluster_count\": 227}\n",
      "Input (41048): java.net.ConnectException: Connection timed out: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 228, \"cluster_size\": 1, \"template_mined\": \"java.net.ConnectException: Connection timed out: no further information\", \"cluster_count\": 228}\n",
      "Input (41210): Stopping ReduceTask metrics system...\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 35, \"cluster_size\": 56, \"template_mined\": \"Stopping <:*:> metrics system...\", \"cluster_count\": 228}\n",
      "Input (41211): ReduceTask metrics system stopped.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 171, \"cluster_size\": 18, \"template_mined\": \"ReduceTask metrics system <:*:>\", \"cluster_count\": 228}\n",
      "Input (41212): ReduceTask metrics system shutdown complete.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 229, \"cluster_size\": 1, \"template_mined\": \"ReduceTask metrics system shutdown complete.\", \"cluster_count\": 229}\n",
      "Input (49153): Slow ReadProcessor read fields took 48944ms (threshold=30000ms); ack: seqno: -2 status: SUCCESS status: ERROR downstreamAckTimeNanos: 0, targets: [172.22.149.145:50010, 10.86.169.121:50010]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 183, \"cluster_size\": 2, \"template_mined\": \"Slow ReadProcessor read fields took <:*:> (threshold=30000ms); ack: seqno: <:NUM:> status: SUCCESS status: ERROR downstreamAckTimeNanos: <:NUM:>, targets: [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>]\", \"cluster_count\": 229}\n",
      "Processing line: 50000, rate 19349.8 lines/sec, 229 clusters so far.\n",
      "Input (51306): Releasing unassigned and invalid container Container: [ContainerId: container_1445076437777_0005_01_000011, NodeId: MININT-75DGDAM1.fareast.corp.microsoft.com:53425, NodeHttpAddress: MININT-75DGDAM1.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.86.165.66:53425 }, ]. RM may have assignment issues\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 230, \"cluster_size\": 1, \"template_mined\": \"Releasing unassigned and invalid container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: MININT-75DGDAM1.fareast.corp.microsoft.com:<:NUM:>, NodeHttpAddress: MININT-75DGDAM1.fareast.corp.microsoft.com:<:NUM:>, Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ]. RM may have assignment issues\", \"cluster_count\": 230}\n",
      "Input (53107): Connection retry failed with 4 attempts in 180 seconds\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 231, \"cluster_size\": 1, \"template_mined\": \"Connection retry failed with <:NUM:> attempts in <:NUM:> seconds\", \"cluster_count\": 231}\n",
      "Input (53108): Failed to connect to MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 map outputs\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 232, \"cluster_size\": 1, \"template_mined\": \"Failed to connect to MININT-FNANLI5.fareast.corp.microsoft.com:<:NUM:> with <:NUM:> map outputs\", \"cluster_count\": 232}\n",
      "Input (53109): Connection timed out: connect\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 233, \"cluster_size\": 1, \"template_mined\": \"Connection timed out: connect\", \"cluster_count\": 233}\n",
      "Input (53130): Reporting fetch failure for attempt_1445087491445_0004_m_000005_0 to jobtracker.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 234, \"cluster_size\": 1, \"template_mined\": \"Reporting fetch failure for attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> to jobtracker.\", \"cluster_count\": 234}\n",
      "Input (55432): Task: attempt_1445182159119_0002_m_000007_0 - exited : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt_1445182159119_0002_m_000007_0/file.out\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 235, \"cluster_size\": 1, \"template_mined\": \"Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\", \"cluster_count\": 235}\n",
      "Input (55448): Diagnostics report from attempt_1445182159119_0002_m_000007_0: Error: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt_1445182159119_0002_m_000007_0/file.out\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 236, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\", \"cluster_count\": 236}\n",
      "Input (55484): attempt_1445182159119_0002_m_000007_0 TaskAttempt Transitioned from FAIL_CONTAINER_CLEANUP to FAIL_TASK_CLEANUP\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 162, \"cluster_size\": 72, \"template_mined\": \"attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> CONTAINER CLEANUP to <:*:> TASK CLEANUP\", \"cluster_count\": 236}\n",
      "Input (55488): 1 failures on node 04DN8IQ.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 237, \"cluster_size\": 1, \"template_mined\": \"<:NUM:> failures on node 04DN8IQ.fareast.corp.microsoft.com\", \"cluster_count\": 237}\n",
      "Input (57955): Task: attempt_1445182159119_0015_m_000005_0 - exited : java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 238, \"cluster_size\": 1, \"template_mined\": \"Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 238}\n",
      "Input (57968): Diagnostics report from attempt_1445182159119_0015_m_000005_0: Error: java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 239, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 239}\n",
      "Input (58126): Task: attempt_1445182159119_0015_m_000005_0 - exited : java.io.IOException: Spill failed\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 240, \"cluster_size\": 1, \"template_mined\": \"Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.io.IOException: Spill failed\", \"cluster_count\": 240}\n",
      "Input (58148): org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 241, \"cluster_size\": 1, \"template_mined\": \"org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 241}\n",
      "Input (58158): java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 242, \"cluster_size\": 1, \"template_mined\": \"java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 242}\n",
      "Input (58164): Diagnostics report from attempt_1445182159119_0015_m_000005_0: Error: java.io.IOException: Spill failed\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 243, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: java.io.IOException: Spill failed\", \"cluster_count\": 243}\n",
      "Input (58371): Added attempt_1445182159119_0015_m_000006_1 to list of failed maps\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 244, \"cluster_size\": 1, \"template_mined\": \"Added attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> to list of failed maps\", \"cluster_count\": 244}\n",
      "Input (58391): Assigning container Container: [ContainerId: container_1445182159119_0015_01_000012, NodeId: MSRA-SA-41.fareast.corp.microsoft.com:10769, NodeHttpAddress: MSRA-SA-41.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 5, Token: Token { kind: ContainerToken, service: 10.190.173.170:10769 }, ] to fast fail map\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 245, \"cluster_size\": 1, \"template_mined\": \"Assigning container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>, NodeHttpAddress: MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>, Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] to fast fail map\", \"cluster_count\": 245}\n",
      "Input (58392): Assigned from earlierFailedMaps\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 246, \"cluster_size\": 1, \"template_mined\": \"Assigned from earlierFailedMaps\", \"cluster_count\": 246}\n",
      "Processing line: 60000, rate 15459.5 lines/sec, 246 clusters so far.\n",
      "Input (61066): Unable to parse prior job history, aborting recovery\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 247, \"cluster_size\": 1, \"template_mined\": \"Unable to parse prior job history, aborting recovery\", \"cluster_count\": 247}\n",
      "Input (61067): Incompatible event log version: null\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 248, \"cluster_size\": 1, \"template_mined\": \"Incompatible event log version: null\", \"cluster_count\": 248}\n",
      "Input (61082): Could not parse the old history file. Will not have old AMinfos\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 249, \"cluster_size\": 1, \"template_mined\": \"Could not parse the old history file. Will not have old AMinfos\", \"cluster_count\": 249}\n",
      "Processing line: 70000, rate 16194.5 lines/sec, 249 clusters so far.\n",
      "Input (71736): Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MININT-75DGDAM1/10.86.165.66\"; destination host is: \"04dn8iq.fareast.corp.microsoft.com\":49470;\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 250, \"cluster_size\": 1, \"template_mined\": \"Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \\\"MININT-75DGDAM1/<:IP:>\\\"; destination host is: \\\"04dn8iq.fareast.corp.microsoft.com\\\":<:NUM:>;\", \"cluster_count\": 250}\n",
      "Processing line: 80000, rate 22441.5 lines/sec, 250 clusters so far.\n",
      "Input (81569): No route to host: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 251, \"cluster_size\": 1, \"template_mined\": \"No route to host: no further information\", \"cluster_count\": 251}\n",
      "Input (81597): Failed to connect to /172.22.149.145:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 252, \"cluster_size\": 1, \"template_mined\": \"Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information\", \"cluster_count\": 252}\n",
      "Input (81684): Could not obtain BP-1347369012-10.190.173.170-1444972147527:blk_1073742989_2201 from any node: java.io.IOException: No live nodes contain block BP-1347369012-10.190.173.170-1444972147527:blk_1073742989_2201 after checking nodes = [172.22.149.145:50010, 10.190.173.170:50010], ignoredNodes = null No live nodes contain current block Block locations: 172.22.149.145:50010 10.190.173.170:50010 Dead nodes:  10.190.173.170:50010 172.22.149.145:50010. Will get new block locations from namenode and retry...\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 253, \"cluster_size\": 1, \"template_mined\": \"Could not obtain BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> from any node: java.io.IOException: No live nodes contain block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> after checking nodes = [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>], ignoredNodes = null No live nodes contain current block Block locations: <:IP:>:<:NUM:> <:IP:>:<:NUM:> Dead nodes: <:IP:>:<:NUM:> <:IP:>:<:NUM:>. Will get new block locations from namenode and retry...\", \"cluster_count\": 253}\n",
      "Input (81685): DFS chooseDataNode: got # 1 IOException, will wait for 1857.517062515084 msec.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 254, \"cluster_size\": 1, \"template_mined\": \"DFS chooseDataNode: got # <:NUM:> IOException, will wait for <:NUM:>.<:NUM:> msec.\", \"cluster_count\": 254}\n",
      "Input (81803): Could not obtain BP-1347369012-10.190.173.170-1444972147527:blk_1073742989_2201 from any node: java.io.IOException: No live nodes contain block BP-1347369012-10.190.173.170-1444972147527:blk_1073742989_2201 after checking nodes = [172.22.149.145:50010, 10.190.173.170:50010], ignoredNodes = null No live nodes contain current block Block locations: 172.22.149.145:50010 10.190.173.170:50010 Dead nodes:  10.190.173.170:50010 172.22.149.145:50010 10.86.169.121:50010. Will get new block locations from namenode and retry...\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 255, \"cluster_size\": 1, \"template_mined\": \"Could not obtain BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> from any node: java.io.IOException: No live nodes contain block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> after checking nodes = [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>], ignoredNodes = null No live nodes contain current block Block locations: <:IP:>:<:NUM:> <:IP:>:<:NUM:> Dead nodes: <:IP:>:<:NUM:> <:IP:>:<:NUM:> <:IP:>:<:NUM:>. Will get new block locations from namenode and retry...\", \"cluster_count\": 255}\n",
      "Input (81806): DFS Read\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 256, \"cluster_size\": 1, \"template_mined\": \"DFS Read\", \"cluster_count\": 256}\n",
      "Input (83813): Releasing unassigned and invalid container Container: [ContainerId: container_1445062781478_0013_01_000012, NodeId: MININT-FNANLI5.fareast.corp.microsoft.com:64642, NodeHttpAddress: MININT-FNANLI5.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.86.169.121:64642 }, ]. RM may have assignment issues\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 230, \"cluster_size\": 2, \"template_mined\": \"Releasing unassigned and invalid container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ]. RM may have assignment issues\", \"cluster_count\": 256}\n",
      "Processing line: 90000, rate 17285.5 lines/sec, 256 clusters so far.\n",
      "Input (93956): DataStreamer Exception\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 257, \"cluster_size\": 1, \"template_mined\": \"DataStreamer Exception\", \"cluster_count\": 257}\n",
      "Processing line: 100000, rate 28163.4 lines/sec, 257 clusters so far.\n",
      "Processing line: 110000, rate 45357.7 lines/sec, 257 clusters so far.\n",
      "Input (112621): Could not contact RM after 360000 milliseconds.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 258, \"cluster_size\": 1, \"template_mined\": \"Could not contact RM after <:NUM:> milliseconds.\", \"cluster_count\": 258}\n",
      "Input (112622): Error communicating with RM: Could not contact RM after 360000 milliseconds.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 259, \"cluster_size\": 1, \"template_mined\": \"Error communicating with RM: Could not contact RM after <:NUM:> milliseconds.\", \"cluster_count\": 259}\n",
      "Input (112629): Error writing History Event: org.apache.hadoop.mapreduce.jobhistory.JobUnsuccessfulCompletionEvent@13bb724a\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 260, \"cluster_size\": 1, \"template_mined\": \"Error writing History Event: org.apache.hadoop.mapreduce.jobhistory.JobUnsuccessfulCompletionEvent@13bb724a\", \"cluster_count\": 260}\n",
      "Input (112663): Thread Thread[eventHandlingThread,5,main] threw an Exception.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 261, \"cluster_size\": 1, \"template_mined\": \"Thread Thread[eventHandlingThread,<:NUM:>,main] threw an Exception.\", \"cluster_count\": 261}\n",
      "Input (112664): java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 262, \"cluster_size\": 1, \"template_mined\": \"java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 262}\n",
      "Input (112708): Found jobId job_1445175094696_0003 to have not been closed. Will close\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 263, \"cluster_size\": 1, \"template_mined\": \"Found jobId job <:NUM:> <:NUM:> to have not been closed. Will close\", \"cluster_count\": 263}\n",
      "Input (112709): Error writing History Event: org.apache.hadoop.mapreduce.jobhistory.JobUnsuccessfulCompletionEvent@2890300b\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 260, \"cluster_size\": 2, \"template_mined\": \"Error writing History Event: <:*:>\", \"cluster_count\": 263}\n",
      "Input (112743): Service JobHistoryEventHandler failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 264, \"cluster_size\": 1, \"template_mined\": \"Service JobHistoryEventHandler failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 264}\n",
      "Input (112790): When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 265, \"cluster_size\": 1, \"template_mined\": \"When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 265}\n",
      "Input (112839): cleanup failed for container container_1445175094696_0003_01_000012 : java.lang.IllegalArgumentException: java.net.UnknownHostException: MSRA-SA-39.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 266, \"cluster_size\": 1, \"template_mined\": \"cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: MSRA-SA-<:NUM:>.fareast.corp.microsoft.com\", \"cluster_count\": 266}\n",
      "Input (112863): java.net.UnknownHostException: MSRA-SA-39.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 267, \"cluster_size\": 1, \"template_mined\": \"java.net.UnknownHostException: MSRA-SA-<:NUM:>.fareast.corp.microsoft.com\", \"cluster_count\": 267}\n",
      "Input (112866): Diagnostics report from attempt_1445175094696_0003_r_000000_0: cleanup failed for container container_1445175094696_0003_01_000012 : java.lang.IllegalArgumentException: java.net.UnknownHostException: MSRA-SA-39.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 268, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>: cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: MSRA-SA-<:NUM:>.fareast.corp.microsoft.com\", \"cluster_count\": 268}\n",
      "Input (112936): Exception while unregistering\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 269, \"cluster_size\": 1, \"template_mined\": \"Exception while unregistering\", \"cluster_count\": 269}\n",
      "Input (112984): Skipping cleaning up the staging dir. assuming AM will be retried.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 270, \"cluster_size\": 1, \"template_mined\": \"Skipping cleaning up the staging dir. assuming AM will be retried.\", \"cluster_count\": 270}\n",
      "Input (112989): Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 264, \"cluster_size\": 2, \"template_mined\": \"Service <:*:> failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 270}\n",
      "Input (113036): Graceful stop failed\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 271, \"cluster_size\": 1, \"template_mined\": \"Graceful stop failed\", \"cluster_count\": 271}\n",
      "Processing line: 120000, rate 30903.9 lines/sec, 271 clusters so far.\n",
      "Processing line: 130000, rate 41334.3 lines/sec, 271 clusters so far.\n",
      "Processing line: 140000, rate 37887.0 lines/sec, 271 clusters so far.\n",
      "Processing line: 150000, rate 44423.0 lines/sec, 271 clusters so far.\n",
      "Processing line: 160000, rate 39395.2 lines/sec, 271 clusters so far.\n",
      "Processing line: 170000, rate 42733.6 lines/sec, 271 clusters so far.\n",
      "Processing line: 180000, rate 41061.8 lines/sec, 271 clusters so far.\n",
      "Processing line: 190000, rate 41728.1 lines/sec, 271 clusters so far.\n",
      "Input (190822): Failed on local exception: java.net.SocketException: Permission denied: no further information; Host Details : local host is: \"MININT-FNANLI5/127.0.0.1\"; destination host is: \"msra-sa-41\":9000;\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 217, \"cluster_size\": 174, \"template_mined\": \"Failed on local exception: <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> Host Details : local host is: \\\"MININT-FNANLI5/<:IP:>\\\"; destination host is: \\\"msra-sa-<:NUM:>\\\":<:NUM:>;\", \"cluster_count\": 271}\n",
      "Input (190841): java.net.SocketException: Permission denied: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 272, \"cluster_size\": 1, \"template_mined\": \"java.net.SocketException: Permission denied: no further information\", \"cluster_count\": 272}\n",
      "Input (195348): Diagnostics report from attempt_1445087491445_0009_r_000000_0: AttemptID:attempt_1445087491445_0009_r_000000_0 Timed out after 600 secs\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 166, \"cluster_size\": 5, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: AttemptID:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> Timed out after <:NUM:> secs\", \"cluster_count\": 272}\n",
      "Processing line: 200000, rate 20340.0 lines/sec, 272 clusters so far.\n",
      "Input (200543): MapCompletionEvents reques\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 273, \"cluster_size\": 1, \"template_mined\": \"MapCompletionEvents reques\", \"cluster_count\": 273}\n",
      "Processing line: 210000, rate 17670.4 lines/sec, 273 clusters so far.\n",
      "Processing line: 220000, rate 19102.5 lines/sec, 273 clusters so far.\n",
      "Input (221630): Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MININT-FNANLI5/10.86.169.121\"; destination host is: \"04dn8iq.fareast.corp.microsoft.com\":49470;\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 250, \"cluster_size\": 2, \"template_mined\": \"Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: \\\"04dn8iq.fareast.corp.microsoft.com\\\":<:NUM:>;\", \"cluster_count\": 273}\n",
      "Input (224638): WordCount and PageRank. Each application has been run for several times, simulating both normal and abnormal cases with injected specific failures.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 274, \"cluster_size\": 1, \"template_mined\": \"WordCount and PageRank. Each application has been run for several times, simulating both normal and abnormal cases with injected specific failures.\", \"cluster_count\": 274}\n",
      "Input (225454): Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MSRA-SA-41/10.190.173.170\"; destination host is: \"minint-fnanli5.fareast.corp.microsoft.com\":62270;\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 250, \"cluster_size\": 3, \"template_mined\": \"Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\", \"cluster_count\": 274}\n",
      "Processing line: 230000, rate 20161.4 lines/sec, 274 clusters so far.\n",
      "Input (231113): Task attempt_1445182159119_0004_m_000004_0 failed : org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 275, \"cluster_size\": 1, \"template_mined\": \"Task attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> failed : org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 275}\n",
      "Input (233135): Progress of TaskAttempt attempt_1445087491445_0004_m_000003_1 is : 9.742042E-4\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 114, \"cluster_size\": 34147, \"template_mined\": \"Progress of TaskAttempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> is : <:*:>\", \"cluster_count\": 275}\n",
      "Input (233779): Error communicating with RM: Resource Manager doesn't recognize AttemptId: application_1445087491445_0004\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 276, \"cluster_size\": 1, \"template_mined\": \"Error communicating with RM: Resource Manager doesn't recognize AttemptId: application <:NUM:> <:NUM:>\", \"cluster_count\": 276}\n",
      "Input (233780): Resource Manager doesn't recognize AttemptId: application_1445087491445_0004\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 277, \"cluster_size\": 1, \"template_mined\": \"Resource Manager doesn't recognize AttemptId: application <:NUM:> <:NUM:>\", \"cluster_count\": 277}\n",
      "Input (233785): org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException: Application attempt appattempt_1445087491445_0004_000001 doesn't exist in ApplicationMasterService cache.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 278, \"cluster_size\": 1, \"template_mined\": \"org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException: Application attempt appattempt <:NUM:> <:NUM:> <:NUM:> doesn't exist in ApplicationMasterService cache.\", \"cluster_count\": 278}\n",
      "Input (233814): org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException): Application attempt appattempt_1445087491445_0004_000001 doesn't exist in ApplicationMasterService cache.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 279, \"cluster_size\": 1, \"template_mined\": \"org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException): Application attempt appattempt <:NUM:> <:NUM:> <:NUM:> doesn't exist in ApplicationMasterService cache.\", \"cluster_count\": 279}\n",
      "Input (233834): Notify RMCommunicator isAMLastRetry: false\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 128, \"cluster_size\": 75, \"template_mined\": \"Notify <:*:> isAMLastRetry: <:*:>\", \"cluster_count\": 279}\n",
      "Input (233835): RMCommunicator notified that shouldUnregistered is: false\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 129, \"cluster_size\": 38, \"template_mined\": \"RMCommunicator notified that shouldUnregistered is: <:*:>\", \"cluster_count\": 279}\n",
      "Input (233837): JobHistoryEventHandler notified that forceJobCompletion is false\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 130, \"cluster_size\": 38, \"template_mined\": \"JobHistoryEventHandler notified that forceJobCompletion is <:*:>\", \"cluster_count\": 279}\n",
      "Processing line: 240000, rate 19207.0 lines/sec, 279 clusters so far.\n",
      "Processing line: 250000, rate 39597.7 lines/sec, 279 clusters so far.\n",
      "Input (255041): Error closing writer for JobID: job_1445144423722_0023\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 280, \"cluster_size\": 1, \"template_mined\": \"Error closing writer for JobID: job <:NUM:> <:NUM:>\", \"cluster_count\": 280}\n",
      "Input (255105): Service JobHistoryEventHandler failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 281, \"cluster_size\": 1, \"template_mined\": \"Service JobHistoryEventHandler failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\", \"cluster_count\": 281}\n",
      "Input (255106): java.nio.channels.ClosedChannelException\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 282, \"cluster_size\": 1, \"template_mined\": \"java.nio.channels.ClosedChannelException\", \"cluster_count\": 282}\n",
      "Input (255131): When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 283, \"cluster_size\": 1, \"template_mined\": \"When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\", \"cluster_count\": 283}\n",
      "Input (255159): cleanup failed for container container_1445144423722_0023_01_000002 : java.lang.IllegalArgumentException: java.net.UnknownHostException: 04DN8IQ.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 266, \"cluster_size\": 2, \"template_mined\": \"cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: <:*:>\", \"cluster_count\": 283}\n",
      "Input (255183): java.net.UnknownHostException: 04DN8IQ.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 267, \"cluster_size\": 3, \"template_mined\": \"java.net.UnknownHostException: <:*:>\", \"cluster_count\": 283}\n",
      "Input (255187): Diagnostics report from attempt_1445144423722_0023_m_000000_0: cleanup failed for container container_1445144423722_0023_01_000002 : java.lang.IllegalArgumentException: java.net.UnknownHostException: 04DN8IQ.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 268, \"cluster_size\": 2, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: <:*:>\", \"cluster_count\": 283}\n",
      "Input (255491): Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 281, \"cluster_size\": 2, \"template_mined\": \"Service <:*:> failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\", \"cluster_count\": 283}\n",
      "Processing line: 260000, rate 33824.1 lines/sec, 283 clusters so far.\n",
      "Processing line: 270000, rate 37700.4 lines/sec, 283 clusters so far.\n",
      "Processing line: 280000, rate 47453.9 lines/sec, 283 clusters so far.\n",
      "Processing line: 290000, rate 45309.3 lines/sec, 283 clusters so far.\n",
      "Input (295221): Exception running child : java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 284, \"cluster_size\": 1, \"template_mined\": \"Exception running child : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 284}\n",
      "Input (295280): Exception cleaning up: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 285, \"cluster_size\": 1, \"template_mined\": \"Exception cleaning up: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 285}\n",
      "Input (299046): Task: attempt_1445182159119_0014_r_000000_0 - exited : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 286, \"cluster_size\": 1, \"template_mined\": \"Task: attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> - exited : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\", \"cluster_count\": 286}\n",
      "Input (299076): Diagnostics report from attempt_1445182159119_0014_r_000000_0: Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 287, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>: Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\", \"cluster_count\": 287}\n",
      "Processing line: 300000, rate 27424.8 lines/sec, 287 clusters so far.\n",
      "Input (301332): Task: attempt_1445182159119_0003_m_000000_0 - failed due to FSError: java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 288, \"cluster_size\": 1, \"template_mined\": \"Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - failed due to FSError: java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 288}\n",
      "Input (301333): Diagnostics report from attempt_1445182159119_0003_m_000000_0: FSError: java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 239, \"cluster_size\": 9, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: <:*:> java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 288}\n",
      "Input (301474): Assigning container Container: [ContainerId: container_1445182159119_0003_01_000016, NodeId: 04DN8IQ.fareast.corp.microsoft.com:64260, NodeHttpAddress: 04DN8IQ.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 5, Token: Token { kind: ContainerToken, service: 10.86.164.138:64260 }, ] to fast fail map\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 245, \"cluster_size\": 4, \"template_mined\": \"Assigning container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] to fast fail map\", \"cluster_count\": 288}\n",
      "Processing line: 310000, rate 18728.7 lines/sec, 288 clusters so far.\n",
      "Input (313342): for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000002_0,attempt_1445087491445_0002_m_000003_1,attempt_1445087491445_0002_m_000004_0,attempt_1445087491445_0002_m_000005_0,attempt_1445087491445_0002_m_000008_0,attempt_1445087491445_0002_m_000009_0,attempt_1445087491445_0002_m_000011_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 289, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 289}\n",
      "Input (317282): Task: attempt_1445144423722_0020_m_000002_0 - exited : java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 290, \"cluster_size\": 1, \"template_mined\": \"Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 290}\n",
      "Input (317339): Diagnostics report from attempt_1445144423722_0020_m_000002_0: Error: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 291, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 291}\n",
      "Input (317532): Task cleanup failed for attempt attempt_1445144423722_0020_m_000002_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 292, \"cluster_size\": 1, \"template_mined\": \"Task cleanup failed for attempt attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 292}\n",
      "Input (317651): 1 failures on node MININT-FNANLI5.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 237, \"cluster_size\": 9, \"template_mined\": \"<:NUM:> failures on node <:*:>\", \"cluster_count\": 292}\n",
      "Processing line: 320000, rate 26965.7 lines/sec, 292 clusters so far.\n",
      "Processing line: 330000, rate 41024.5 lines/sec, 292 clusters so far.\n",
      "Input (332788): In stop, writing event MAP_ATTEMPT_FAILED\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 293, \"cluster_size\": 1, \"template_mined\": \"In stop, writing event MAP ATTEMPT FAILED\", \"cluster_count\": 293}\n",
      "Input (332789): Service JobHistoryEventHandler failed in state STOPPED; cause: org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 294, \"cluster_size\": 1, \"template_mined\": \"Service JobHistoryEventHandler failed in state STOPPED; cause: org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\", \"cluster_count\": 294}\n",
      "Input (332790): Attempt to process a enum when a union was expected.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 295, \"cluster_size\": 1, \"template_mined\": \"Attempt to process a enum when a union was expected.\", \"cluster_count\": 295}\n",
      "Input (332812): When stopping the service JobHistoryEventHandler : org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 296, \"cluster_size\": 1, \"template_mined\": \"When stopping the service JobHistoryEventHandler : org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\", \"cluster_count\": 296}\n",
      "Input (333349): Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state STOPPED; cause: org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 294, \"cluster_size\": 2, \"template_mined\": \"Service <:*:> failed in state STOPPED; cause: org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\", \"cluster_count\": 296}\n",
      "Processing line: 340000, rate 33635.8 lines/sec, 296 clusters so far.\n",
      "Processing line: 350000, rate 38467.0 lines/sec, 296 clusters so far.\n",
      "Processing line: 360000, rate 38024.8 lines/sec, 296 clusters so far.\n",
      "Processing line: 370000, rate 41480.9 lines/sec, 296 clusters so far.\n",
      "Processing line: 380000, rate 23016.7 lines/sec, 296 clusters so far.\n",
      "Input (382915): Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#3\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 297, \"cluster_size\": 1, \"template_mined\": \"Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\", \"cluster_count\": 297}\n",
      "Input (386489): Shuffle failed : local error on this node: 04DN8IQ/10.86.164.138\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 298, \"cluster_size\": 1, \"template_mined\": \"Shuffle failed : local error on this node: 04DN8IQ/<:IP:>\", \"cluster_count\": 298}\n",
      "Input (388352): Ignoring exception during close for org.apache.hadoop.mapred.MapTask$NewOutputCollector@6a6585ee\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 299, \"cluster_size\": 1, \"template_mined\": \"Ignoring exception during close for org.apache.hadoop.mapred.MapTask$NewOutputCollector@6a6585ee\", \"cluster_count\": 299}\n",
      "Input (388364): Exception running child : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt_1445182159119_0002_m_000007_0/file.out\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 300, \"cluster_size\": 1, \"template_mined\": \"Exception running child : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\", \"cluster_count\": 300}\n",
      "Processing line: 390000, rate 17012.6 lines/sec, 300 clusters so far.\n",
      "Input (394043): IPC Server handler 29 on 58622, call statusUpdate(attempt_1445094324383_0003_m_000000_0, org.apache.hadoop.mapred.MapTaskStatus@cdcdbf7), rpc version=2, client version=19, methodsFingerPrint=937413979 from 10.86.169.121:52490 Call#68 Retry#0: output error\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 301, \"cluster_size\": 1, \"template_mined\": \"IPC Server handler <:NUM:> on <:NUM:>, call statusUpdate(attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>, org.apache.hadoop.mapred.MapTaskStatus@cdcdbf7), rpc version=<:NUM:>, client version=<:NUM:>, methodsFingerPrint=<:NUM:> from <:IP:>:<:NUM:> Call#<:NUM:> Retry#<:NUM:>: output error\", \"cluster_count\": 301}\n",
      "Input (394044): IPC Server handler 29 on 58622 caught an exception\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 302, \"cluster_size\": 1, \"template_mined\": \"IPC Server handler <:NUM:> on <:NUM:> caught an exception\", \"cluster_count\": 302}\n",
      "--- Done processing file in 16.55 sec. Total of 395363 lines, rate 23887.2 lines/sec, 302 clusters\n",
      "ID=28    : size=201731    : \n",
      "ID=114   : size=45882     : Progress of TaskAttempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> is : <:*:>\n",
      "ID=113   : size=18140     : MapCompletionEvents request from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>. startIndex <:NUM:> maxEvents <:NUM:>\n",
      "ID=212   : size=5795      : Address change detected. Old: msra-sa-<:NUM:>/<:IP:>:<:NUM:> New: msra-sa-<:NUM:>:<:NUM:>\n",
      "ID=12    : size=5542      : (EQUATOR) <:NUM:> kvi <:NUM:>(<:NUM:>)\n",
      "ID=18    : size=5339      : Spilling map output\n",
      "ID=19    : size=5339      : bufstart = <:NUM:>; bufend = <:NUM:>; bufvoid = <:NUM:>\n",
      "ID=20    : size=5339      : kvstart = <:NUM:>(<:NUM:>); kvend = <:NUM:>(<:NUM:>); length = <:NUM:>/<:NUM:>\n",
      "ID=203   : size=5330      : java.net.NoRouteToHostException: No route to host: no further information\n",
      "ID=214   : size=5303      : No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=213   : size=5300      : Failed to renew lease for [DFSClient NONMAPREDUCE <:NUM:> <:NUM:>] for <:NUM:> seconds. Will retry shortly ...\n",
      "ID=21    : size=5204      : Finished spill <:NUM:>\n",
      "ID=22    : size=4579      : (RESET) equator <:NUM:> kv <:NUM:>(<:NUM:>) kvi <:NUM:>(<:NUM:>)\n",
      "ID=93    : size=4415      : Recalculating schedule, headroom=<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "ID=148   : size=3841      : Reduce slow start threshold not met. completedMapsForReduceSlowstart <:NUM:>\n",
      "ID=84    : size=3297      : Resolved <:*:> to /default-rack\n",
      "ID=86    : size=2921      : attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> to <:*:>\n",
      "ID=164   : size=2517      : Retrying connect to server: <:*:> Already tried <:NUM:> time(s); maxRetries=<:NUM:>\n",
      "ID=88    : size=2074      : task <:NUM:> <:NUM:> <:*:> <:NUM:> Task Transitioned from <:*:> to <:*:>\n",
      "ID=107   : size=1757      : Opening proxy : <:*:>\n",
      "ID=105   : size=1744      : Processing the event EventType: CONTAINER REMOTE <:*:> for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> taskAttempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=154   : size=1611      : attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> <:*:> <:*:> <:*:> <:*:>\n",
      "ID=96    : size=1249      : After Scheduling: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\n",
      "ID=3     : size=1140      : MapTask metrics system <:*:>\n",
      "ID=165   : size=1048      : Retrying connect to server: <:*:> Already tried <:NUM:> time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=<:NUM:>, sleepTime=<:NUM:> MILLISECONDS)\n",
      "ID=85    : size=1033      : TaskAttempt: [attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>] using containerId: [container <:NUM:> <:NUM:> <:NUM:> <:NUM:> on NM: <:*:>\n",
      "ID=1     : size=978       : loaded properties from hadoop-metrics2.properties\n",
      "ID=2     : size=978       : Scheduled snapshot period at <:NUM:> second(s).\n",
      "ID=4     : size=978       : Executing with tokens:\n",
      "ID=110   : size=962       : Auth successful for job <:NUM:> <:NUM:> (auth:SIMPLE)\n",
      "ID=5     : size=909       : Kind: mapreduce.job, Service: job <:NUM:> <:NUM:>, Ident: <:*:>\n",
      "ID=6     : size=909       : Sleeping for 0ms before retrying again. Got null now.\n",
      "ID=7     : size=907       : mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application <:NUM:> <:NUM:>\n",
      "ID=8     : size=907       : session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "ID=100   : size=906       : Assigned container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> to attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=106   : size=906       : Launching attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=108   : size=906       : Shuffle port returned by ContainerManager for attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> : <:NUM:>\n",
      "ID=9     : size=905       : ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "ID=10    : size=905       : Using ResourceCalculatorProcessTree : <:*:>\n",
      "ID=109   : size=905       : ATTEMPT START task <:NUM:> <:NUM:> <:*:> <:NUM:>\n",
      "ID=111   : size=902       : JVM with ID : jvm <:NUM:> <:NUM:> <:*:> <:NUM:> asked for a task\n",
      "ID=112   : size=902       : JVM with ID: jvm <:NUM:> <:NUM:> <:*:> <:NUM:> given task: attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=97    : size=899       : getResources() for application <:NUM:> <:NUM:>: ask=<:NUM:> release= <:NUM:> newContainers=<:NUM:> finishedContainers=<:NUM:> resourcelimit=<memory:<:NUM:>, vCores:<:NUM:>> knownNMs=<:NUM:>\n",
      "ID=125   : size=851       : KILLING attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=11    : size=834       : Processing split: <:*:>\n",
      "ID=13    : size=834       : mapreduce.task.io.sort.mb: <:NUM:>\n",
      "ID=14    : size=834       : soft limit at <:NUM:>\n",
      "ID=15    : size=834       : bufstart = <:NUM:>; bufvoid = <:NUM:>\n",
      "ID=16    : size=834       : kvstart = <:NUM:>; length = <:NUM:>\n",
      "ID=17    : size=834       : Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "ID=150   : size=826       : Received completed container container <:NUM:> <:NUM:> <:NUM:> <:NUM:>\n",
      "ID=92    : size=797       : Before Scheduling: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\n",
      "ID=160   : size=733       : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: Container killed by the ApplicationMaster.\n",
      "ID=87    : size=701       : Task succeeded with attempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=90    : size=701       : Num completed Tasks: <:NUM:>\n",
      "ID=24    : size=678       : Merging <:NUM:> sorted segments\n",
      "ID=25    : size=678       : Down to the last merge-pass, with <:NUM:> segments left of total size: <:NUM:> bytes\n",
      "ID=178   : size=667       : attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Shuffling to disk since <:NUM:> is greater than maxSingleShuffleLimit (<:NUM:>)\n",
      "ID=179   : size=664       : fetcher#<:NUM:> about to shuffle output of map attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> decomp: <:NUM:> len: <:NUM:> to DISK\n",
      "ID=185   : size=649       : Read <:NUM:> bytes from map-output for attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "ID=23    : size=644       : Starting flush of map output\n",
      "ID=26    : size=625       : Task:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> is done. And is in the process of committing\n",
      "ID=42    : size=621       : Registering class <:*:> for class <:*:>\n",
      "ID=27    : size=616       : Task 'attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>' done.\n",
      "ID=98    : size=606       : Got allocated containers <:NUM:>\n",
      "ID=123   : size=605       : Done acknowledgement from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=158   : size=531       : completedMapPercent <:NUM:>.<:NUM:> totalResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> finalMapResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> finalReduceResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> netScheduledMapResource:<memory:<:NUM:>, vCores:<:NUM:>> netScheduledReduceResource:<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "ID=217   : size=487       : Failed on local exception: <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> Host Details : local host is: \"MININT-FNANLI5/<:IP:>\"; destination host is: \"msra-sa-<:NUM:>\":<:NUM:>;\n",
      "ID=215   : size=480       : ERROR IN CONTACTING RM.\n",
      "ID=218   : size=479       : java.io.IOException: Couldn't set up IO streams\n",
      "ID=219   : size=479       : java.nio.channels.UnresolvedAddressException\n",
      "ID=175   : size=472       : Assigning <:*:> with <:NUM:> to fetcher#<:NUM:>\n",
      "ID=176   : size=472       : assigned <:NUM:> of <:NUM:> to <:*:> to fetcher#<:NUM:>\n",
      "ID=174   : size=414       : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>: Got <:NUM:> new map-outputs\n",
      "ID=186   : size=383       : MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:> freed by fetcher#<:NUM:> in <:*:>\n",
      "ID=181   : size=381       : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=82    : size=335       : Processing the event EventType: <:*:> <:*:>\n",
      "ID=35    : size=317       : Stopping <:*:> metrics system...\n",
      "ID=54    : size=306       : job <:NUM:> <:*:> Transitioned from <:*:> to <:*:>\n",
      "ID=36    : size=302       : MapTask metrics system shutdown complete.\n",
      "ID=155   : size=255       : DefaultSpeculator.addSpeculativeAttempt -- we are speculating task <:NUM:> <:NUM:> m <:NUM:>\n",
      "ID=156   : size=255       : We launched <:NUM:> speculations. Sleeping <:NUM:> milliseconds.\n",
      "ID=43    : size=226       : Default file system [hdfs://msra-sa-<:NUM:>:<:NUM:>]\n",
      "ID=153   : size=218       : An existing connection was forcibly closed by the remote host\n",
      "ID=152   : size=217       : Socket Reader #<:NUM:> for port <:NUM:>: readAndProcess from client <:IP:> threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]\n",
      "ID=162   : size=216       : attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> CONTAINER CLEANUP to <:*:> TASK CLEANUP\n",
      "ID=163   : size=216       : Could not delete <:*:> temporary/<:NUM:>/ temporary/attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=157   : size=194       : Scheduling a redundant attempt for task task <:NUM:> <:NUM:> m <:NUM:>\n",
      "ID=161   : size=182       : Issuing kill to other attempt attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "ID=136   : size=141       : Moved tmp to done: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "ID=56    : size=138       : Using callQueue class java.util.concurrent.LinkedBlockingQueue\n",
      "ID=57    : size=138       : Starting Socket Reader #<:NUM:> for port <:NUM:>\n",
      "ID=59    : size=138       : IPC Server listener on <:NUM:>: starting\n",
      "ID=60    : size=138       : IPC Server Responder: starting\n",
      "ID=65    : size=138       : Added filter AM PROXY FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context <:*:>\n",
      "ID=66    : size=138       : adding path spec: <:*:>\n",
      "ID=101   : size=138       : The <:*:> file on the remote FS is <:*:> <:NUM:> <:*:>\n",
      "ID=206   : size=126       : <:NUM:>\n",
      "ID=120   : size=119       : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:>\n",
      "ID=128   : size=104       : Notify <:*:> isAMLastRetry: <:*:>\n",
      "ID=222   : size=100       : Ramping down all scheduled reduces:<:NUM:>\n",
      "ID=223   : size=100       : Going to preempt <:NUM:> due to lack of space for maps\n",
      "ID=47    : size=96        : Read from history task task <:NUM:> <:NUM:> m <:NUM:>\n",
      "ID=83    : size=96        : Recovering task task <:NUM:> <:NUM:> m <:NUM:> from prior app attempt, status was SUCCEEDED\n",
      "ID=134   : size=94        : Copied to done location: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "ID=171   : size=86        : ReduceTask metrics system <:*:>\n",
      "ID=180   : size=84        : Ignoring obsolete output of <:*:> map-task: 'attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>'\n",
      "ID=99    : size=73        : Assigned to reduce\n",
      "ID=201   : size=72        : <:*:> freed by fetcher#<:NUM:> in <:*:>\n",
      "ID=172   : size=71        : Using ShuffleConsumerPlugin: <:*:>\n",
      "ID=173   : size=71        : MergerManager: memoryLimit=<:NUM:>, maxSingleShuffleLimit=<:NUM:>, mergeThreshold=<:NUM:>, ioSortFactor=<:NUM:>, memToMemMergeOutputsThreshold=<:NUM:>\n",
      "ID=37    : size=69        : Created MRAppMaster for application appattempt <:NUM:> <:NUM:> <:NUM:>\n",
      "ID=38    : size=69        : Kind: YARN AM RM TOKEN, Service: , Ident: (appAttemptId { application id { id: <:NUM:> cluster timestamp: <:NUM:> } attemptId: <:NUM:> } keyId: <:NUM:>)\n",
      "ID=39    : size=69        : Using mapred newApiCommitter.\n",
      "ID=40    : size=69        : OutputCommitter set in config null\n",
      "ID=41    : size=69        : OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "ID=44    : size=69        : Emitting job history data to the timeline server is not enabled\n",
      "ID=49    : size=69        : MRAppMaster metrics system started\n",
      "ID=50    : size=69        : Adding job token for job <:NUM:> <:NUM:> to jobTokenSecretManager\n",
      "ID=51    : size=69        : Not uberizing job <:NUM:> <:NUM:> because: not enabled; too many maps; too much input;\n",
      "ID=52    : size=69        : Input size for job job <:NUM:> <:NUM:> = <:NUM:>. Number of splits = <:NUM:>\n",
      "ID=53    : size=69        : Number of reduces for job job <:NUM:> <:NUM:> = <:NUM:>\n",
      "ID=55    : size=69        : MRAppMaster launching normal, non-uberized, multi-container job job <:NUM:> <:NUM:>.\n",
      "ID=58    : size=69        : Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server\n",
      "ID=61    : size=69        : Instantiated MRClientService at <:*:>\n",
      "ID=62    : size=69        : Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n",
      "ID=63    : size=69        : Http request log for http.requests.mapreduce is not defined\n",
      "ID=64    : size=69        : Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n",
      "ID=67    : size=69        : Jetty bound to port <:NUM:>\n",
      "ID=68    : size=69        : jetty-<:NUM:>.<:NUM:>.<:NUM:>\n",
      "ID=69    : size=69        : Extract jar:file:/D:/hadoop-<:NUM:>.<:NUM:>.<:NUM:>-localbox/share/hadoop/yarn/hadoop-yarn-common-<:NUM:>.<:NUM:>.<:NUM:>-SNAPSHOT.jar!/webapps/mapreduce to <:*:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> mapreduce <:*:>\n",
      "ID=70    : size=69        : Started HttpServer2$SelectChannelConnectorWithSafeStartup@<:IP:>:<:NUM:>\n",
      "ID=71    : size=69        : Web app /mapreduce started at <:NUM:>\n",
      "ID=72    : size=69        : Registered webapp guice modules\n",
      "ID=73    : size=69        : JOB CREATE job <:NUM:> <:NUM:>\n",
      "ID=74    : size=69        : nodeBlacklistingEnabled:true\n",
      "ID=75    : size=69        : maxTaskFailuresPerNode is <:NUM:>\n",
      "ID=76    : size=69        : blacklistDisablePercent is <:NUM:>\n",
      "ID=77    : size=69        : Connecting to ResourceManager at <:*:>\n",
      "ID=78    : size=69        : maxContainerCapability: <memory:<:NUM:>, vCores:<:NUM:>>\n",
      "ID=79    : size=69        : queue: default\n",
      "ID=80    : size=69        : Upper limit on the thread pool size is <:NUM:>\n",
      "ID=81    : size=69        : yarn.client.max-cached-nodemanagers-proxies : <:NUM:>\n",
      "ID=89    : size=69        : Event Writer setup for JobId: job <:NUM:> <:NUM:>, File: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist\n",
      "ID=91    : size=69        : reduceResourceRequest:<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "ID=102   : size=69        : Adding #<:NUM:> tokens and #<:NUM:> secret keys for NM use for launching container\n",
      "ID=103   : size=69        : Size of containertokens dob is <:NUM:>\n",
      "ID=104   : size=69        : Putting shuffle token in serviceData\n",
      "ID=94    : size=65        : Reduce slow start threshold reached. Scheduling reduces.\n",
      "ID=147   : size=65        : mapResourceRequest:<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "ID=190   : size=56        : finalMerge called with <:NUM:> in-memory map-outputs and <:NUM:> on-disk map-outputs\n",
      "ID=191   : size=56        : Merging <:NUM:> files, <:NUM:> bytes from disk\n",
      "ID=192   : size=56        : Merging <:NUM:> segments, <:NUM:> bytes from memory into reduce\n",
      "ID=193   : size=56        : mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "ID=167   : size=55        : Killing taskAttempt:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> because it is running on unusable <:*:>\n",
      "ID=168   : size=55        : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: Container released on a *lost* node\n",
      "ID=170   : size=55        : java.io.IOException: An existing connection was forcibly closed by the remote host\n",
      "ID=189   : size=54        : EventFetcher is interrupted.. Returning\n",
      "ID=129   : size=52        : RMCommunicator notified that shouldUnregistered is: <:*:>\n",
      "ID=130   : size=52        : JobHistoryEventHandler notified that forceJobCompletion is <:*:>\n",
      "ID=131   : size=52        : Calling stop for all the services\n",
      "ID=132   : size=52        : Stopping JobHistoryEventHandler. Size of the outstanding queue size is <:NUM:>\n",
      "ID=127   : size=50        : We are finishing cleanly so this is the last retry\n",
      "ID=137   : size=48        : Stopped JobHistoryEventHandler. super.stop()\n",
      "ID=138   : size=48        : Setting job diagnostics to\n",
      "ID=139   : size=48        : History url is <:*:> <:NUM:> <:NUM:>\n",
      "ID=141   : size=48        : Final Stats: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\n",
      "ID=143   : size=48        : Stopping server on <:NUM:>\n",
      "ID=144   : size=48        : Stopping IPC Server listener on <:NUM:>\n",
      "ID=145   : size=48        : TaskHeartbeatHandler thread interrupted\n",
      "ID=146   : size=48        : Stopping IPC Server Responder\n",
      "ID=194   : size=48        : Task attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> is allowed to commit now\n",
      "ID=195   : size=48        : Saved output of task 'attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>' to <:*:> temporary/<:NUM:>/task <:NUM:> <:NUM:> r <:NUM:>\n",
      "ID=118   : size=47        : Commit-pending state update from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "ID=119   : size=47        : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> TaskAttempt Transitioned from RUNNING to COMMIT PENDING\n",
      "ID=121   : size=47        : Commit go/no-go request from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "ID=122   : size=47        : Result of canCommit for attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>:true\n",
      "ID=124   : size=47        : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> TaskAttempt Transitioned from COMMIT PENDING to SUCCESS CONTAINER CLEANUP\n",
      "ID=126   : size=47        : Calling handler for JobFinishedEvent\n",
      "ID=133   : size=47        : Copying hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist to hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "ID=135   : size=47        : Copying hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:> conf.xml to hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "ID=140   : size=45        : Waiting for application to be successfully unregistered.\n",
      "ID=142   : size=45        : Deleting staging directory hdfs://msra-sa-<:NUM:>:<:NUM:> /tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>\n",
      "ID=169   : size=44        : Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\n",
      "ID=95    : size=43        : All maps assigned. Ramping up all remaining reduces:<:NUM:>\n",
      "ID=159   : size=31        : Ramping up <:NUM:>\n",
      "ID=197   : size=30        : Exception in createBlockOutputStream\n",
      "ID=198   : size=30        : Bad connect ack with firstBadLink as <:IP:>:<:NUM:>\n",
      "ID=199   : size=30        : Abandoning BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:>\n",
      "ID=200   : size=30        : Excluding datanode <:IP:>:<:NUM:>\n",
      "ID=187   : size=29        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=208   : size=25        : RUNNABLE\n",
      "ID=221   : size=25        : TaskAttempt killed because it ran on unusable node <:*:> AttemptId:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "ID=267   : size=24        : java.net.UnknownHostException: <:*:>\n",
      "ID=210   : size=21        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=207   : size=20        : TIMED WAITING\n",
      "ID=251   : size=20        : No route to host: no further information\n",
      "ID=46    : size=19        : Previous history file is at hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist\n",
      "ID=151   : size=19        : Container complete event for unknown container id container <:NUM:> <:NUM:> <:NUM:> <:NUM:>\n",
      "ID=182   : size=19        : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>:\n",
      "ID=205   : size=18        : WAITING\n",
      "ID=242   : size=18        : java.io.IOException: There is not enough space on the disk\n",
      "ID=241   : size=17        : org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\n",
      "ID=29    : size=16        : I/O error constructing remote block reader.\n",
      "ID=239   : size=16        : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: <:*:> java.io.IOException: There is not enough space on the disk\n",
      "ID=45    : size=15        : Recovery is enabled. Will try to recover from previous life on best effort basis.\n",
      "ID=196   : size=15        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=229   : size=15        : ReduceTask metrics system shutdown complete.\n",
      "ID=115   : size=14        : DFSOutputStream ResponseProcessor exception for block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:>\n",
      "ID=116   : size=14        : Bad response ERROR for block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> from datanode <:IP:>:<:NUM:>\n",
      "ID=117   : size=14        : Error Recovery for block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> in pipeline <:IP:>:<:NUM:>, <:IP:>:<:NUM:>: bad datanode <:IP:>:<:NUM:>\n",
      "ID=262   : size=14        : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=149   : size=12        : Cannot assign container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] for a map as either container memory less than required <memory:<:NUM:>, vCores:<:NUM:>> or no pending map tasks - maps.isEmpty=true\n",
      "ID=188   : size=12        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=266   : size=12        : cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: <:*:>\n",
      "ID=268   : size=12        : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: <:*:>\n",
      "ID=48    : size=11        : Read completed tasks from history <:NUM:>\n",
      "ID=30    : size=10        : Connection timed out: no further information\n",
      "ID=202   : size=10        : Communication exception: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to <:IP:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=237   : size=10        : <:NUM:> failures on node <:*:>\n",
      "ID=244   : size=10        : Added attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> to list of failed maps\n",
      "ID=252   : size=10        : Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information\n",
      "ID=183   : size=9         : Slow ReadProcessor read fields took <:*:> (threshold=30000ms); ack: seqno: <:NUM:> status: SUCCESS status: ERROR downstreamAckTimeNanos: <:NUM:>, targets: [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>]\n",
      "ID=177   : size=8         : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=245   : size=8         : Assigning container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] to fast fail map\n",
      "ID=246   : size=8         : Assigned from earlierFailedMaps\n",
      "ID=248   : size=8         : Incompatible event log version: null\n",
      "ID=272   : size=8         : java.net.SocketException: Permission denied: no further information\n",
      "ID=282   : size=8         : java.nio.channels.ClosedChannelException\n",
      "ID=166   : size=7         : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: AttemptID:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> Timed out after <:NUM:> secs\n",
      "ID=216   : size=7         : Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\n",
      "ID=226   : size=7         : Merging <:NUM:> intermediate segments out of a total of <:NUM:>\n",
      "ID=230   : size=7         : Releasing unassigned and invalid container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ]. RM may have assignment issues\n",
      "ID=32    : size=6         : Successfully connected to /<:IP:>:<:NUM:> for BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:>\n",
      "ID=211   : size=6         : Runnning cleanup for the task\n",
      "ID=238   : size=6         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.io.IOException: There is not enough space on the disk\n",
      "ID=243   : size=6         : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: java.io.IOException: Spill failed\n",
      "ID=254   : size=6         : DFS chooseDataNode: got # <:NUM:> IOException, will wait for <:NUM:>.<:NUM:> msec.\n",
      "ID=258   : size=6         : Could not contact RM after <:NUM:> milliseconds.\n",
      "ID=31    : size=5         : Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information\n",
      "ID=184   : size=5         : In stop, writing event <:*:> FINISHED\n",
      "ID=204   : size=5         : Process Thread Dump: Communication exception\n",
      "ID=209   : size=5         : Last retry, killing attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "ID=253   : size=5         : Could not obtain BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> from any node: java.io.IOException: No live nodes contain block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> after checking nodes = [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>], ignoredNodes = null No live nodes contain current block Block locations: <:IP:>:<:NUM:> <:IP:>:<:NUM:> Dead nodes: <:IP:>:<:NUM:> <:IP:>:<:NUM:>. Will get new block locations from namenode and retry...\n",
      "ID=247   : size=4         : Unable to parse prior job history, aborting recovery\n",
      "ID=249   : size=4         : Could not parse the old history file. Will not have old AMinfos\n",
      "ID=250   : size=4         : Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\n",
      "ID=260   : size=4         : Error writing History Event: <:*:>\n",
      "ID=287   : size=4         : Diagnostics report from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>: Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\n",
      "ID=291   : size=4         : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=295   : size=4         : Attempt to process a enum when a union was expected.\n",
      "ID=220   : size=3         : Exception in getting events\n",
      "ID=240   : size=3         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.io.IOException: Spill failed\n",
      "ID=256   : size=3         : DFS Read\n",
      "ID=257   : size=3         : DataStreamer Exception\n",
      "ID=259   : size=3         : Error communicating with RM: Could not contact RM after <:NUM:> milliseconds.\n",
      "ID=261   : size=3         : Thread Thread[eventHandlingThread,<:NUM:>,main] threw an Exception.\n",
      "ID=269   : size=3         : Exception while unregistering\n",
      "ID=270   : size=3         : Skipping cleaning up the staging dir. assuming AM will be retried.\n",
      "ID=271   : size=3         : Graceful stop failed\n",
      "ID=289   : size=3         : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=33    : size=2         : Connection refused: no further information\n",
      "ID=236   : size=2         : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\n",
      "ID=263   : size=2         : Found jobId job <:NUM:> <:NUM:> to have not been closed. Will close\n",
      "ID=264   : size=2         : Service <:*:> failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=276   : size=2         : Error communicating with RM: Resource Manager doesn't recognize AttemptId: application <:NUM:> <:NUM:>\n",
      "ID=277   : size=2         : Resource Manager doesn't recognize AttemptId: application <:NUM:> <:NUM:>\n",
      "ID=278   : size=2         : org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException: Application attempt appattempt <:NUM:> <:NUM:> <:NUM:> doesn't exist in ApplicationMasterService cache.\n",
      "ID=279   : size=2         : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException): Application attempt appattempt <:NUM:> <:NUM:> <:NUM:> doesn't exist in ApplicationMasterService cache.\n",
      "ID=281   : size=2         : Service <:*:> failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n",
      "ID=284   : size=2         : Exception running child : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=285   : size=2         : Exception cleaning up: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=286   : size=2         : Task: attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> - exited : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\n",
      "ID=288   : size=2         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - failed due to FSError: java.io.IOException: There is not enough space on the disk\n",
      "ID=290   : size=2         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=292   : size=2         : Task cleanup failed for attempt attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "ID=294   : size=2         : Service <:*:> failed in state STOPPED; cause: org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\n",
      "ID=297   : size=2         : Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\n",
      "ID=298   : size=2         : Shuffle failed : local error on this node: 04DN8IQ/<:IP:>\n",
      "ID=34    : size=1         : Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.ConnectException: Connection refused: no further information\n",
      "ID=224   : size=1         : Preempting attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "ID=225   : size=1         : Reduce preemption successful attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "ID=227   : size=1         : Communication exception: java.net.ConnectException: Call From MSRA-SA-<:NUM:>/<:IP:> to minint-fnanli5.fareast.corp.microsoft.com:<:NUM:> failed on connection exception: java.net.ConnectException: Connection timed out: no further information; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused\n",
      "ID=228   : size=1         : java.net.ConnectException: Connection timed out: no further information\n",
      "ID=231   : size=1         : Connection retry failed with <:NUM:> attempts in <:NUM:> seconds\n",
      "ID=232   : size=1         : Failed to connect to MININT-FNANLI5.fareast.corp.microsoft.com:<:NUM:> with <:NUM:> map outputs\n",
      "ID=233   : size=1         : Connection timed out: connect\n",
      "ID=234   : size=1         : Reporting fetch failure for attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> to jobtracker.\n",
      "ID=235   : size=1         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\n",
      "ID=255   : size=1         : Could not obtain BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> from any node: java.io.IOException: No live nodes contain block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> after checking nodes = [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>], ignoredNodes = null No live nodes contain current block Block locations: <:IP:>:<:NUM:> <:IP:>:<:NUM:> Dead nodes: <:IP:>:<:NUM:> <:IP:>:<:NUM:> <:IP:>:<:NUM:>. Will get new block locations from namenode and retry...\n",
      "ID=265   : size=1         : When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=273   : size=1         : MapCompletionEvents reques\n",
      "ID=274   : size=1         : WordCount and PageRank. Each application has been run for several times, simulating both normal and abnormal cases with injected specific failures.\n",
      "ID=275   : size=1         : Task attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> failed : org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\n",
      "ID=280   : size=1         : Error closing writer for JobID: job <:NUM:> <:NUM:>\n",
      "ID=283   : size=1         : When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n",
      "ID=293   : size=1         : In stop, writing event MAP ATTEMPT FAILED\n",
      "ID=296   : size=1         : When stopping the service JobHistoryEventHandler : org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\n",
      "ID=299   : size=1         : Ignoring exception during close for org.apache.hadoop.mapred.MapTask$NewOutputCollector@6a6585ee\n",
      "ID=300   : size=1         : Exception running child : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\n",
      "ID=301   : size=1         : IPC Server handler <:NUM:> on <:NUM:>, call statusUpdate(attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>, org.apache.hadoop.mapred.MapTaskStatus@cdcdbf7), rpc version=<:NUM:>, client version=<:NUM:>, methodsFingerPrint=<:NUM:> from <:IP:>:<:NUM:> Call#<:NUM:> Retry#<:NUM:>: output error\n",
      "ID=302   : size=1         : IPC Server handler <:NUM:> on <:NUM:> caught an exception\n",
      "Prefix Tree:\n",
      "<root>\n",
      "\t<L=4>\n",
      "\t\t\"loaded\" (cluster_count=1)\n",
      "\t\t\tID=1     : size=978       : loaded properties from hadoop-metrics2.properties\n",
      "\t\t\"MapTask\" (cluster_count=1)\n",
      "\t\t\tID=3     : size=1140      : MapTask metrics system <:*:>\n",
      "\t\t\"Using\" (cluster_count=2)\n",
      "\t\t\tID=10    : size=905       : Using ResourceCalculatorProcessTree : <:*:>\n",
      "\t\t\tID=56    : size=138       : Using callQueue class java.util.concurrent.LinkedBlockingQueue\n",
      "\t\t\"(EQUATOR)\" (cluster_count=1)\n",
      "\t\t\tID=12    : size=5542      : (EQUATOR) <:NUM:> kvi <:NUM:>(<:NUM:>)\n",
      "\t\t\"soft\" (cluster_count=1)\n",
      "\t\t\tID=14    : size=834       : soft limit at <:NUM:>\n",
      "\t\t\"Merging\" (cluster_count=1)\n",
      "\t\t\tID=24    : size=678       : Merging <:NUM:> sorted segments\n",
      "\t\t\"Stopping\" (cluster_count=3)\n",
      "\t\t\tID=35    : size=317       : Stopping <:*:> metrics system...\n",
      "\t\t\tID=143   : size=48        : Stopping server on <:NUM:>\n",
      "\t\t\tID=146   : size=48        : Stopping IPC Server Responder\n",
      "\t\t\"Default\" (cluster_count=1)\n",
      "\t\t\tID=43    : size=226       : Default file system [hdfs://msra-sa-<:NUM:>:<:NUM:>]\n",
      "\t\t\"MRAppMaster\" (cluster_count=1)\n",
      "\t\t\tID=49    : size=69        : MRAppMaster metrics system started\n",
      "\t\t\"IPC\" (cluster_count=1)\n",
      "\t\t\tID=60    : size=138       : IPC Server Responder: starting\n",
      "\t\t\"Instantiated\" (cluster_count=1)\n",
      "\t\t\tID=61    : size=69        : Instantiated MRClientService at <:*:>\n",
      "\t\t\"adding\" (cluster_count=1)\n",
      "\t\t\tID=66    : size=138       : adding path spec: <:*:>\n",
      "\t\t\"Registered\" (cluster_count=1)\n",
      "\t\t\tID=72    : size=69        : Registered webapp guice modules\n",
      "\t\t\"Resolved\" (cluster_count=1)\n",
      "\t\t\tID=84    : size=3297      : Resolved <:*:> to /default-rack\n",
      "\t\t\"Num\" (cluster_count=1)\n",
      "\t\t\tID=90    : size=701       : Num completed Tasks: <:NUM:>\n",
      "\t\t\"Recalculating\" (cluster_count=1)\n",
      "\t\t\tID=93    : size=4415      : Recalculating schedule, headroom=<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "\t\t\"Got\" (cluster_count=1)\n",
      "\t\t\tID=98    : size=606       : Got allocated containers <:NUM:>\n",
      "\t\t\"Opening\" (cluster_count=1)\n",
      "\t\t\tID=107   : size=1757      : Opening proxy : <:*:>\n",
      "\t\t\"Calling\" (cluster_count=1)\n",
      "\t\t\tID=126   : size=47        : Calling handler for JobFinishedEvent\n",
      "\t\t\"Notify\" (cluster_count=1)\n",
      "\t\t\tID=128   : size=104       : Notify <:*:> isAMLastRetry: <:*:>\n",
      "\t\t\"Setting\" (cluster_count=1)\n",
      "\t\t\tID=138   : size=48        : Setting job diagnostics to\n",
      "\t\t\"ReduceTask\" (cluster_count=1)\n",
      "\t\t\tID=171   : size=86        : ReduceTask metrics system <:*:>\n",
      "\t\t\"EventFetcher\" (cluster_count=1)\n",
      "\t\t\tID=189   : size=54        : EventFetcher is interrupted.. Returning\n",
      "\t\t\"Abandoning\" (cluster_count=1)\n",
      "\t\t\tID=199   : size=30        : Abandoning BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:>\n",
      "\t\t\"ERROR\" (cluster_count=1)\n",
      "\t\t\tID=215   : size=480       : ERROR IN CONTACTING RM.\n",
      "\t\t\"Exception\" (cluster_count=1)\n",
      "\t\t\tID=220   : size=3         : Exception in getting events\n",
      "\t\t\"Connection\" (cluster_count=1)\n",
      "\t\t\tID=233   : size=1         : Connection timed out: connect\n",
      "\t<L=6>\n",
      "\t\t\"Scheduled\" (cluster_count=1)\n",
      "\t\t\tID=2     : size=978       : Scheduled snapshot period at <:NUM:> second(s).\n",
      "\t\t\"mapreduce.cluster.local.dir\" (cluster_count=1)\n",
      "\t\t\tID=7     : size=907       : mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application <:NUM:> <:NUM:>\n",
      "\t\t\"session.id\" (cluster_count=1)\n",
      "\t\t\tID=8     : size=907       : session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "\t\t\"bufstart\" (cluster_count=1)\n",
      "\t\t\tID=15    : size=834       : bufstart = <:NUM:>; bufvoid = <:NUM:>\n",
      "\t\t\"kvstart\" (cluster_count=1)\n",
      "\t\t\tID=16    : size=834       : kvstart = <:NUM:>; length = <:NUM:>\n",
      "\t\t\"Map\" (cluster_count=1)\n",
      "\t\t\tID=17    : size=834       : Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "\t\t\"I/O\" (cluster_count=1)\n",
      "\t\t\tID=29    : size=16        : I/O error constructing remote block reader.\n",
      "\t\t\"Connection\" (cluster_count=1)\n",
      "\t\t\tID=30    : size=10        : Connection timed out: no further information\n",
      "\t\t\"Registering\" (cluster_count=1)\n",
      "\t\t\tID=42    : size=621       : Registering class <:*:> for class <:*:>\n",
      "\t\t\"Read\" (cluster_count=1)\n",
      "\t\t\tID=48    : size=11        : Read completed tasks from history <:NUM:>\n",
      "\t\t\"Adding\" (cluster_count=1)\n",
      "\t\t\tID=58    : size=69        : Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server\n",
      "\t\t\"IPC\" (cluster_count=1)\n",
      "\t\t\tID=59    : size=138       : IPC Server listener on <:NUM:>: starting\n",
      "\t\t\"Web\" (cluster_count=1)\n",
      "\t\t\tID=71    : size=69        : Web app /mapreduce started at <:NUM:>\n",
      "\t\t\"Processing\" (cluster_count=1)\n",
      "\t\t\tID=82    : size=335       : Processing the event EventType: <:*:> <:*:>\n",
      "\t\t\"Size\" (cluster_count=1)\n",
      "\t\t\tID=103   : size=69        : Size of containertokens dob is <:NUM:>\n",
      "\t\t\"RMCommunicator\" (cluster_count=1)\n",
      "\t\t\tID=129   : size=52        : RMCommunicator notified that shouldUnregistered is: <:*:>\n",
      "\t\t\"JobHistoryEventHandler\" (cluster_count=1)\n",
      "\t\t\tID=130   : size=52        : JobHistoryEventHandler notified that forceJobCompletion is <:*:>\n",
      "\t\t\"Calling\" (cluster_count=1)\n",
      "\t\t\tID=131   : size=52        : Calling stop for all the services\n",
      "\t\t\"History\" (cluster_count=1)\n",
      "\t\t\tID=139   : size=48        : History url is <:*:> <:NUM:> <:NUM:>\n",
      "\t\t\"Stopping\" (cluster_count=1)\n",
      "\t\t\tID=144   : size=48        : Stopping IPC Server listener on <:NUM:>\n",
      "\t\t\"MergerManager:\" (cluster_count=1)\n",
      "\t\t\tID=173   : size=71        : MergerManager: memoryLimit=<:NUM:>, maxSingleShuffleLimit=<:NUM:>, mergeThreshold=<:NUM:>, ioSortFactor=<:NUM:>, memToMemMergeOutputsThreshold=<:NUM:>\n",
      "\t\t\"Assigning\" (cluster_count=1)\n",
      "\t\t\tID=175   : size=472       : Assigning <:*:> with <:NUM:> to fetcher#<:NUM:>\n",
      "\t\t\"In\" (cluster_count=1)\n",
      "\t\t\tID=184   : size=5         : In stop, writing event <:*:> FINISHED\n",
      "\t\t\"MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>\" (cluster_count=1)\n",
      "\t\t\tID=186   : size=383       : MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:> freed by fetcher#<:NUM:> in <:*:>\n",
      "\t\t\"mapred.skip.on\" (cluster_count=1)\n",
      "\t\t\tID=193   : size=56        : mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "\t\t\"<:*:>\" (cluster_count=1)\n",
      "\t\t\tID=201   : size=72        : <:*:> freed by fetcher#<:NUM:> in <:*:>\n",
      "\t\t\"java.io.IOException:\" (cluster_count=1)\n",
      "\t\t\tID=218   : size=479       : java.io.IOException: Couldn't set up IO streams\n",
      "\t\t\"java.net.SocketException:\" (cluster_count=1)\n",
      "\t\t\tID=272   : size=8         : java.net.SocketException: Permission denied: no further information\n",
      "\t\t\"Ignoring\" (cluster_count=1)\n",
      "\t\t\tID=299   : size=1         : Ignoring exception during close for org.apache.hadoop.mapred.MapTask$NewOutputCollector@6a6585ee\n",
      "\t<L=3>\n",
      "\t\t\"Executing\" (cluster_count=1)\n",
      "\t\t\tID=4     : size=978       : Executing with tokens:\n",
      "\t\t\"Processing\" (cluster_count=1)\n",
      "\t\t\tID=11    : size=834       : Processing split: <:*:>\n",
      "\t\t\"Spilling\" (cluster_count=1)\n",
      "\t\t\tID=18    : size=5339      : Spilling map output\n",
      "\t\t\"Finished\" (cluster_count=1)\n",
      "\t\t\tID=21    : size=5204      : Finished spill <:NUM:>\n",
      "\t\t\"Using\" (cluster_count=2)\n",
      "\t\t\tID=39    : size=69        : Using mapred newApiCommitter.\n",
      "\t\t\tID=172   : size=71        : Using ShuffleConsumerPlugin: <:*:>\n",
      "\t\t\"OutputCommitter\" (cluster_count=1)\n",
      "\t\t\tID=41    : size=69        : OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "\t\t\"maxTaskFailuresPerNode\" (cluster_count=1)\n",
      "\t\t\tID=75    : size=69        : maxTaskFailuresPerNode is <:NUM:>\n",
      "\t\t\"blacklistDisablePercent\" (cluster_count=1)\n",
      "\t\t\tID=76    : size=69        : blacklistDisablePercent is <:NUM:>\n",
      "\t\t\"maxContainerCapability:\" (cluster_count=1)\n",
      "\t\t\tID=78    : size=69        : maxContainerCapability: <memory:<:NUM:>, vCores:<:NUM:>>\n",
      "\t\t\"yarn.client.max-cached-nodemanagers-proxies\" (cluster_count=1)\n",
      "\t\t\tID=81    : size=69        : yarn.client.max-cached-nodemanagers-proxies : <:NUM:>\n",
      "\t\t\"Assigned\" (cluster_count=2)\n",
      "\t\t\tID=99    : size=73        : Assigned to reduce\n",
      "\t\t\tID=246   : size=8         : Assigned from earlierFailedMaps\n",
      "\t\t\"Stopped\" (cluster_count=1)\n",
      "\t\t\tID=137   : size=48        : Stopped JobHistoryEventHandler. super.stop()\n",
      "\t\t\"TaskHeartbeatHandler\" (cluster_count=1)\n",
      "\t\t\tID=145   : size=48        : TaskHeartbeatHandler thread interrupted\n",
      "\t\t\"Ramping\" (cluster_count=1)\n",
      "\t\t\tID=159   : size=31        : Ramping up <:NUM:>\n",
      "\t\t\"Exception\" (cluster_count=2)\n",
      "\t\t\tID=197   : size=30        : Exception in createBlockOutputStream\n",
      "\t\t\tID=269   : size=3         : Exception while unregistering\n",
      "\t\t\"Excluding\" (cluster_count=1)\n",
      "\t\t\tID=200   : size=30        : Excluding datanode <:IP:>:<:NUM:>\n",
      "\t\t\"Graceful\" (cluster_count=1)\n",
      "\t\t\tID=271   : size=3         : Graceful stop failed\n",
      "\t<L=8>\n",
      "\t\t\"Kind:\" (cluster_count=1)\n",
      "\t\t\tID=5     : size=909       : Kind: mapreduce.job, Service: job <:NUM:> <:NUM:>, Ident: <:*:>\n",
      "\t\t\"Task\" (cluster_count=1)\n",
      "\t\t\tID=27    : size=616       : Task 'attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>' done.\n",
      "\t\t\"Successfully\" (cluster_count=1)\n",
      "\t\t\tID=32    : size=6         : Successfully connected to /<:IP:>:<:NUM:> for BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:>\n",
      "\t\t\"Created\" (cluster_count=1)\n",
      "\t\t\tID=37    : size=69        : Created MRAppMaster for application appattempt <:NUM:> <:NUM:> <:NUM:>\n",
      "\t\t\"job\" (cluster_count=1)\n",
      "\t\t\tID=54    : size=306       : job <:NUM:> <:*:> Transitioned from <:*:> to <:*:>\n",
      "\t\t\"Http\" (cluster_count=1)\n",
      "\t\t\tID=63    : size=69        : Http request log for http.requests.mapreduce is not defined\n",
      "\t\t\"All\" (cluster_count=1)\n",
      "\t\t\tID=95    : size=43        : All maps assigned. Ramping up all remaining reduces:<:NUM:>\n",
      "\t\t\"DFSOutputStream\" (cluster_count=1)\n",
      "\t\t\tID=115   : size=14        : DFSOutputStream ResponseProcessor exception for block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:>\n",
      "\t\t\"Reduce\" (cluster_count=1)\n",
      "\t\t\tID=148   : size=3841      : Reduce slow start threshold not met. completedMapsForReduceSlowstart <:NUM:>\n",
      "\t\t\"Received\" (cluster_count=1)\n",
      "\t\t\tID=150   : size=826       : Received completed container container <:NUM:> <:NUM:> <:NUM:> <:NUM:>\n",
      "\t\t\"assigned\" (cluster_count=1)\n",
      "\t\t\tID=176   : size=472       : assigned <:NUM:> of <:NUM:> to <:*:> to fetcher#<:NUM:>\n",
      "\t\t\"java.net.NoRouteToHostException:\" (cluster_count=1)\n",
      "\t\t\tID=203   : size=5330      : java.net.NoRouteToHostException: No route to host: no further information\n",
      "\t\t\"Unable\" (cluster_count=1)\n",
      "\t\t\tID=247   : size=4         : Unable to parse prior job history, aborting recovery\n",
      "\t\t\"Resource\" (cluster_count=1)\n",
      "\t\t\tID=277   : size=2         : Resource Manager doesn't recognize AttemptId: application <:NUM:> <:NUM:>\n",
      "\t\t\"Error\" (cluster_count=1)\n",
      "\t\t\tID=280   : size=1         : Error closing writer for JobID: job <:NUM:> <:NUM:>\n",
      "\t\t\"When\" (cluster_count=1)\n",
      "\t\t\tID=283   : size=1         : When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n",
      "\t<L=9>\n",
      "\t\t\"Sleeping\" (cluster_count=1)\n",
      "\t\t\tID=6     : size=909       : Sleeping for 0ms before retrying again. Got null now.\n",
      "\t\t\"bufstart\" (cluster_count=1)\n",
      "\t\t\tID=19    : size=5339      : bufstart = <:NUM:>; bufend = <:NUM:>; bufvoid = <:NUM:>\n",
      "\t\t\"kvstart\" (cluster_count=1)\n",
      "\t\t\tID=20    : size=5339      : kvstart = <:NUM:>(<:NUM:>); kvend = <:NUM:>(<:NUM:>); length = <:NUM:>/<:NUM:>\n",
      "\t\t\"Read\" (cluster_count=1)\n",
      "\t\t\tID=47    : size=96        : Read from history task task <:NUM:> <:NUM:> m <:NUM:>\n",
      "\t\t\"Adding\" (cluster_count=1)\n",
      "\t\t\tID=50    : size=69        : Adding job token for job <:NUM:> <:NUM:> to jobTokenSecretManager\n",
      "\t\t\"MRAppMaster\" (cluster_count=1)\n",
      "\t\t\tID=55    : size=69        : MRAppMaster launching normal, non-uberized, multi-container job job <:NUM:> <:NUM:>.\n",
      "\t\t\"Added\" (cluster_count=1)\n",
      "\t\t\tID=65    : size=138       : Added filter AM PROXY FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context <:*:>\n",
      "\t\t\"Upper\" (cluster_count=1)\n",
      "\t\t\tID=80    : size=69        : Upper limit on the thread pool size is <:NUM:>\n",
      "\t\t\"Done\" (cluster_count=1)\n",
      "\t\t\tID=123   : size=605       : Done acknowledgement from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "\t\t\"Copying\" (cluster_count=1)\n",
      "\t\t\tID=133   : size=47        : Copying hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist to hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=182   : size=19        : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>:\n",
      "\t\t\"Merging\" (cluster_count=1)\n",
      "\t\t\tID=192   : size=56        : Merging <:NUM:> segments, <:NUM:> bytes from memory into reduce\n",
      "\t\t\"Last\" (cluster_count=1)\n",
      "\t\t\tID=209   : size=5         : Last retry, killing attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "\t\t\"Reduce\" (cluster_count=1)\n",
      "\t\t\tID=225   : size=1         : Reduce preemption successful attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "\t\t\"Connection\" (cluster_count=1)\n",
      "\t\t\tID=231   : size=1         : Connection retry failed with <:NUM:> attempts in <:NUM:> seconds\n",
      "\t\t\"Failed\" (cluster_count=1)\n",
      "\t\t\tID=232   : size=1         : Failed to connect to MININT-FNANLI5.fareast.corp.microsoft.com:<:NUM:> with <:NUM:> map outputs\n",
      "\t\t\"java.io.IOException:\" (cluster_count=1)\n",
      "\t\t\tID=242   : size=18        : java.io.IOException: There is not enough space on the disk\n",
      "\t\t\"Service\" (cluster_count=1)\n",
      "\t\t\tID=281   : size=2         : Service <:*:> failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n",
      "\t\t\"Shuffle\" (cluster_count=1)\n",
      "\t\t\tID=298   : size=2         : Shuffle failed : local error on this node: 04DN8IQ/<:IP:>\n",
      "\t\t\"IPC\" (cluster_count=1)\n",
      "\t\t\tID=302   : size=1         : IPC Server handler <:NUM:> on <:NUM:> caught an exception\n",
      "\t<L=7>\n",
      "\t\t\"ProcfsBasedProcessTree\" (cluster_count=1)\n",
      "\t\t\tID=9     : size=905       : ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "\t\t\"(RESET)\" (cluster_count=1)\n",
      "\t\t\tID=22    : size=4579      : (RESET) equator <:NUM:> kv <:NUM:>(<:NUM:>) kvi <:NUM:>(<:NUM:>)\n",
      "\t\t\"Starting\" (cluster_count=1)\n",
      "\t\t\tID=57    : size=138       : Starting Socket Reader #<:NUM:> for port <:NUM:>\n",
      "\t\t\"Reduce\" (cluster_count=1)\n",
      "\t\t\tID=94    : size=65        : Reduce slow start threshold reached. Scheduling reduces.\n",
      "\t\t\"Launching\" (cluster_count=1)\n",
      "\t\t\tID=106   : size=906       : Launching attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "\t\t\"ATTEMPT\" (cluster_count=1)\n",
      "\t\t\tID=109   : size=905       : ATTEMPT START task <:NUM:> <:NUM:> <:*:> <:NUM:>\n",
      "\t\t\"Auth\" (cluster_count=1)\n",
      "\t\t\tID=110   : size=962       : Auth successful for job <:NUM:> <:NUM:> (auth:SIMPLE)\n",
      "\t\t\"KILLING\" (cluster_count=1)\n",
      "\t\t\tID=125   : size=851       : KILLING attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "\t\t\"Waiting\" (cluster_count=1)\n",
      "\t\t\tID=140   : size=45        : Waiting for application to be successfully unregistered.\n",
      "\t\t\"Deleting\" (cluster_count=1)\n",
      "\t\t\tID=142   : size=45        : Deleting staging directory hdfs://msra-sa-<:NUM:>:<:NUM:> /tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>\n",
      "\t\t\"We\" (cluster_count=1)\n",
      "\t\t\tID=156   : size=255       : We launched <:NUM:> speculations. Sleeping <:NUM:> milliseconds.\n",
      "\t\t\"Merging\" (cluster_count=1)\n",
      "\t\t\tID=191   : size=56        : Merging <:NUM:> files, <:NUM:> bytes from disk\n",
      "\t\t\"Bad\" (cluster_count=1)\n",
      "\t\t\tID=198   : size=30        : Bad connect ack with firstBadLink as <:IP:>:<:NUM:>\n",
      "\t\t\"Address\" (cluster_count=1)\n",
      "\t\t\tID=212   : size=5795      : Address change detected. Old: msra-sa-<:NUM:>/<:IP:>:<:NUM:> New: msra-sa-<:NUM:>:<:NUM:>\n",
      "\t\t\"Preempting\" (cluster_count=1)\n",
      "\t\t\tID=224   : size=1         : Preempting attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "\t\t\"java.net.ConnectException:\" (cluster_count=1)\n",
      "\t\t\tID=228   : size=1         : java.net.ConnectException: Connection timed out: no further information\n",
      "\t\t\"No\" (cluster_count=1)\n",
      "\t\t\tID=251   : size=20        : No route to host: no further information\n",
      "\t\t\"Could\" (cluster_count=1)\n",
      "\t\t\tID=258   : size=6         : Could not contact RM after <:NUM:> milliseconds.\n",
      "\t\t\"In\" (cluster_count=1)\n",
      "\t\t\tID=293   : size=1         : In stop, writing event MAP ATTEMPT FAILED\n",
      "\t<L=2>\n",
      "\t\t\"mapreduce.task.io.sort.mb:\" (cluster_count=1)\n",
      "\t\t\tID=13    : size=834       : mapreduce.task.io.sort.mb: <:NUM:>\n",
      "\t\t\"Started\" (cluster_count=1)\n",
      "\t\t\tID=70    : size=69        : Started HttpServer2$SelectChannelConnectorWithSafeStartup@<:IP:>:<:NUM:>\n",
      "\t\t\"queue:\" (cluster_count=1)\n",
      "\t\t\tID=79    : size=69        : queue: default\n",
      "\t\t\"reduceResourceRequest:<memory:<:NUM:>,\" (cluster_count=1)\n",
      "\t\t\tID=91    : size=69        : reduceResourceRequest:<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "\t\t\"mapResourceRequest:<memory:<:NUM:>,\" (cluster_count=1)\n",
      "\t\t\tID=147   : size=65        : mapResourceRequest:<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "\t\t\"TIMED\" (cluster_count=1)\n",
      "\t\t\tID=207   : size=20        : TIMED WAITING\n",
      "\t\t\"DFS\" (cluster_count=1)\n",
      "\t\t\tID=256   : size=3         : DFS Read\n",
      "\t\t\"DataStreamer\" (cluster_count=1)\n",
      "\t\t\tID=257   : size=3         : DataStreamer Exception\n",
      "\t\t\"java.net.UnknownHostException:\" (cluster_count=1)\n",
      "\t\t\tID=267   : size=24        : java.net.UnknownHostException: <:*:>\n",
      "\t\t\"MapCompletionEvents\" (cluster_count=1)\n",
      "\t\t\tID=273   : size=1         : MapCompletionEvents reques\n",
      "\t<L=5>\n",
      "\t\t\"Starting\" (cluster_count=1)\n",
      "\t\t\tID=23    : size=644       : Starting flush of map output\n",
      "\t\t\"Connection\" (cluster_count=1)\n",
      "\t\t\tID=33    : size=2         : Connection refused: no further information\n",
      "\t\t\"MapTask\" (cluster_count=1)\n",
      "\t\t\tID=36    : size=302       : MapTask metrics system shutdown complete.\n",
      "\t\t\"OutputCommitter\" (cluster_count=1)\n",
      "\t\t\tID=40    : size=69        : OutputCommitter set in config null\n",
      "\t\t\"Logging\" (cluster_count=1)\n",
      "\t\t\tID=62    : size=69        : Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n",
      "\t\t\"Added\" (cluster_count=1)\n",
      "\t\t\tID=64    : size=69        : Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n",
      "\t\t\"Jetty\" (cluster_count=1)\n",
      "\t\t\tID=67    : size=69        : Jetty bound to port <:NUM:>\n",
      "\t\t\"JOB\" (cluster_count=1)\n",
      "\t\t\tID=73    : size=69        : JOB CREATE job <:NUM:> <:NUM:>\n",
      "\t\t\"Connecting\" (cluster_count=1)\n",
      "\t\t\tID=77    : size=69        : Connecting to ResourceManager at <:*:>\n",
      "\t\t\"Putting\" (cluster_count=1)\n",
      "\t\t\tID=104   : size=69        : Putting shuffle token in serviceData\n",
      "\t\t\"Copied\" (cluster_count=1)\n",
      "\t\t\tID=134   : size=94        : Copied to done location: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "\t\t\"Moved\" (cluster_count=1)\n",
      "\t\t\tID=136   : size=141       : Moved tmp to done: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "\t\t\"Process\" (cluster_count=1)\n",
      "\t\t\tID=204   : size=5         : Process Thread Dump: Communication exception\n",
      "\t\t\"Runnning\" (cluster_count=1)\n",
      "\t\t\tID=211   : size=6         : Runnning cleanup for the task\n",
      "\t\t\"Ramping\" (cluster_count=1)\n",
      "\t\t\tID=222   : size=100       : Ramping down all scheduled reduces:<:NUM:>\n",
      "\t\t\"ReduceTask\" (cluster_count=1)\n",
      "\t\t\tID=229   : size=15        : ReduceTask metrics system shutdown complete.\n",
      "\t\t\"<:NUM:>\" (cluster_count=1)\n",
      "\t\t\tID=237   : size=10        : <:NUM:> failures on node <:*:>\n",
      "\t\t\"Incompatible\" (cluster_count=1)\n",
      "\t\t\tID=248   : size=8         : Incompatible event log version: null\n",
      "\t\t\"Error\" (cluster_count=1)\n",
      "\t\t\tID=260   : size=4         : Error writing History Event: <:*:>\n",
      "\t\t\"Thread\" (cluster_count=1)\n",
      "\t\t\tID=261   : size=3         : Thread Thread[eventHandlingThread,<:NUM:>,main] threw an Exception.\n",
      "\t<L=14>\n",
      "\t\t\"Down\" (cluster_count=1)\n",
      "\t\t\tID=25    : size=678       : Down to the last merge-pass, with <:NUM:> segments left of total size: <:NUM:> bytes\n",
      "\t\t\"Recovery\" (cluster_count=1)\n",
      "\t\t\tID=45    : size=15        : Recovery is enabled. Will try to recover from previous life on best effort basis.\n",
      "\t\t\"Not\" (cluster_count=1)\n",
      "\t\t\tID=51    : size=69        : Not uberizing job <:NUM:> <:NUM:> because: not enabled; too many maps; too much input;\n",
      "\t\t\"Input\" (cluster_count=1)\n",
      "\t\t\tID=52    : size=69        : Input size for job job <:NUM:> <:NUM:> = <:NUM:>. Number of splits = <:NUM:>\n",
      "\t\t\"Recovering\" (cluster_count=1)\n",
      "\t\t\tID=83    : size=96        : Recovering task task <:NUM:> <:NUM:> m <:NUM:> from prior app attempt, status was SUCCEEDED\n",
      "\t\t\"Assigned\" (cluster_count=1)\n",
      "\t\t\tID=100   : size=906       : Assigned container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> to attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "\t\t\"Shuffle\" (cluster_count=1)\n",
      "\t\t\tID=108   : size=906       : Shuffle port returned by ContainerManager for attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> : <:NUM:>\n",
      "\t\t\"Error\" (cluster_count=1)\n",
      "\t\t\tID=117   : size=14        : Error Recovery for block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> in pipeline <:IP:>:<:NUM:>, <:IP:>:<:NUM:>: bad datanode <:IP:>:<:NUM:>\n",
      "\t\t\"attempt\" (cluster_count=2)\n",
      "\t\t\tID=120   : size=119       : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:>\n",
      "\t\t\tID=154   : size=1611      : attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> <:*:> <:*:> <:*:> <:*:>\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=160   : size=733       : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: Container killed by the ApplicationMaster.\n",
      "\t\t\"Killing\" (cluster_count=1)\n",
      "\t\t\tID=167   : size=55        : Killing taskAttempt:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> because it is running on unusable <:*:>\n",
      "\t\t\"for\" (cluster_count=1)\n",
      "\t\t\tID=181   : size=381       : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "\t<L=15>\n",
      "\t\t\"Task:attempt\" (cluster_count=1)\n",
      "\t\t\tID=26    : size=625       : Task:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> is done. And is in the process of committing\n",
      "\t\t\"Event\" (cluster_count=1)\n",
      "\t\t\tID=89    : size=69        : Event Writer setup for JobId: job <:NUM:> <:NUM:>, File: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist\n",
      "\t\t\"attempt\" (cluster_count=1)\n",
      "\t\t\tID=124   : size=47        : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> TaskAttempt Transitioned from COMMIT PENDING to SUCCESS CONTAINER CLEANUP\n",
      "\t\t\"Retrying\" (cluster_count=1)\n",
      "\t\t\tID=165   : size=1048      : Retrying connect to server: <:*:> Already tried <:NUM:> time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=<:NUM:>, sleepTime=<:NUM:> MILLISECONDS)\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=168   : size=55        : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: Container released on a *lost* node\n",
      "\t\t\"TaskAttempt\" (cluster_count=1)\n",
      "\t\t\tID=221   : size=25        : TaskAttempt killed because it ran on unusable node <:*:> AttemptId:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "\t<L=0> (cluster_count=1)\n",
      "\t\tID=28    : size=201731    : \n",
      "\t<L=19>\n",
      "\t\t\"Failed\" (cluster_count=1)\n",
      "\t\t\tID=31    : size=5         : Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information\n",
      "\t\t\"fetcher#<:NUM:>\" (cluster_count=1)\n",
      "\t\t\tID=179   : size=664       : fetcher#<:NUM:> about to shuffle output of map attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> decomp: <:NUM:> len: <:NUM:> to DISK\n",
      "\t\t\"Slow\" (cluster_count=1)\n",
      "\t\t\tID=183   : size=9         : Slow ReadProcessor read fields took <:*:> (threshold=30000ms); ack: seqno: <:NUM:> status: SUCCESS status: ERROR downstreamAckTimeNanos: <:NUM:>, targets: [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>]\n",
      "\t\t\"for\" (cluster_count=1)\n",
      "\t\t\tID=187   : size=29        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "\t\t\"Task:\" (cluster_count=1)\n",
      "\t\t\tID=238   : size=6         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.io.IOException: There is not enough space on the disk\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=239   : size=16        : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: <:*:> java.io.IOException: There is not enough space on the disk\n",
      "\t\t\"Task\" (cluster_count=1)\n",
      "\t\t\tID=275   : size=1         : Task attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> failed : org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\n",
      "\t\t\"Exception\" (cluster_count=1)\n",
      "\t\t\tID=300   : size=1         : Exception running child : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\n",
      "\t<L=18>\n",
      "\t\t\"Failed\" (cluster_count=1)\n",
      "\t\t\tID=34    : size=1         : Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.ConnectException: Connection refused: no further information\n",
      "\t\t\"Service\" (cluster_count=1)\n",
      "\t\t\tID=294   : size=2         : Service <:*:> failed in state STOPPED; cause: org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\n",
      "\t<L=24>\n",
      "\t\t\"Kind:\" (cluster_count=1)\n",
      "\t\t\tID=38    : size=69        : Kind: YARN AM RM TOKEN, Service: , Ident: (appAttemptId { application id { id: <:NUM:> cluster timestamp: <:NUM:> } attemptId: <:NUM:> } keyId: <:NUM:>)\n",
      "\t\t\"for\" (cluster_count=1)\n",
      "\t\t\tID=210   : size=21        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "\t\t\"Communication\" (cluster_count=1)\n",
      "\t\t\tID=227   : size=1         : Communication exception: java.net.ConnectException: Call From MSRA-SA-<:NUM:>/<:IP:> to minint-fnanli5.fareast.corp.microsoft.com:<:NUM:> failed on connection exception: java.net.ConnectException: Connection timed out: no further information; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused\n",
      "\t<L=11>\n",
      "\t\t\"Emitting\" (cluster_count=1)\n",
      "\t\t\tID=44    : size=69        : Emitting job history data to the timeline server is not enabled\n",
      "\t\t\"Previous\" (cluster_count=1)\n",
      "\t\t\tID=46    : size=19        : Previous history file is at hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist\n",
      "\t\t\"Extract\" (cluster_count=1)\n",
      "\t\t\tID=69    : size=69        : Extract jar:file:/D:/hadoop-<:NUM:>.<:NUM:>.<:NUM:>-localbox/share/hadoop/yarn/hadoop-yarn-common-<:NUM:>.<:NUM:>.<:NUM:>-SNAPSHOT.jar!/webapps/mapreduce to <:*:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> mapreduce <:*:>\n",
      "\t\t\"task\" (cluster_count=1)\n",
      "\t\t\tID=88    : size=2074      : task <:NUM:> <:NUM:> <:*:> <:NUM:> Task Transitioned from <:*:> to <:*:>\n",
      "\t\t\"The\" (cluster_count=1)\n",
      "\t\t\tID=101   : size=138       : The <:*:> file on the remote FS is <:*:> <:NUM:> <:*:>\n",
      "\t\t\"Bad\" (cluster_count=1)\n",
      "\t\t\tID=116   : size=14        : Bad response ERROR for block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> from datanode <:IP:>:<:NUM:>\n",
      "\t\t\"Scheduling\" (cluster_count=1)\n",
      "\t\t\tID=157   : size=194       : Scheduling a redundant attempt for task task <:NUM:> <:NUM:> m <:NUM:>\n",
      "\t\t\"Issuing\" (cluster_count=1)\n",
      "\t\t\tID=161   : size=182       : Issuing kill to other attempt attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "\t\t\"Could\" (cluster_count=1)\n",
      "\t\t\tID=163   : size=216       : Could not delete <:*:> temporary/<:NUM:>/ temporary/attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "\t\t\"java.io.IOException:\" (cluster_count=1)\n",
      "\t\t\tID=170   : size=55        : java.io.IOException: An existing connection was forcibly closed by the remote host\n",
      "\t\t\"Going\" (cluster_count=1)\n",
      "\t\t\tID=223   : size=100       : Going to preempt <:NUM:> due to lack of space for maps\n",
      "\t\t\"DFS\" (cluster_count=1)\n",
      "\t\t\tID=254   : size=6         : DFS chooseDataNode: got # <:NUM:> IOException, will wait for <:NUM:>.<:NUM:> msec.\n",
      "\t\t\"Error\" (cluster_count=1)\n",
      "\t\t\tID=259   : size=3         : Error communicating with RM: Could not contact RM after <:NUM:> milliseconds.\n",
      "\t\t\"Skipping\" (cluster_count=1)\n",
      "\t\t\tID=270   : size=3         : Skipping cleaning up the staging dir. assuming AM will be retried.\n",
      "\t\t\"Task\" (cluster_count=1)\n",
      "\t\t\tID=292   : size=2         : Task cleanup failed for attempt attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "\t<L=10>\n",
      "\t\t\"Number\" (cluster_count=1)\n",
      "\t\t\tID=53    : size=69        : Number of reduces for job job <:NUM:> <:NUM:> = <:NUM:>\n",
      "\t\t\"Task\" (cluster_count=1)\n",
      "\t\t\tID=87    : size=701       : Task succeeded with attempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "\t\t\"Commit-pending\" (cluster_count=1)\n",
      "\t\t\tID=118   : size=47        : Commit-pending state update from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "\t\t\"Commit\" (cluster_count=1)\n",
      "\t\t\tID=121   : size=47        : Commit go/no-go request from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "\t\t\"Result\" (cluster_count=1)\n",
      "\t\t\tID=122   : size=47        : Result of canCommit for attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>:true\n",
      "\t\t\"We\" (cluster_count=1)\n",
      "\t\t\tID=127   : size=50        : We are finishing cleanly so this is the last retry\n",
      "\t\t\"Stopping\" (cluster_count=1)\n",
      "\t\t\tID=132   : size=52        : Stopping JobHistoryEventHandler. Size of the outstanding queue size is <:NUM:>\n",
      "\t\t\"Copying\" (cluster_count=1)\n",
      "\t\t\tID=135   : size=47        : Copying hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:> conf.xml to hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "\t\t\"An\" (cluster_count=1)\n",
      "\t\t\tID=153   : size=218       : An existing connection was forcibly closed by the remote host\n",
      "\t\t\"DefaultSpeculator.addSpeculativeAttempt\" (cluster_count=1)\n",
      "\t\t\tID=155   : size=255       : DefaultSpeculator.addSpeculativeAttempt -- we are speculating task <:NUM:> <:NUM:> m <:NUM:>\n",
      "\t\t\"Retrying\" (cluster_count=1)\n",
      "\t\t\tID=164   : size=2517      : Retrying connect to server: <:*:> Already tried <:NUM:> time(s); maxRetries=<:NUM:>\n",
      "\t\t\"attempt\" (cluster_count=1)\n",
      "\t\t\tID=174   : size=414       : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>: Got <:NUM:> new map-outputs\n",
      "\t\t\"finalMerge\" (cluster_count=1)\n",
      "\t\t\tID=190   : size=56        : finalMerge called with <:NUM:> in-memory map-outputs and <:NUM:> on-disk map-outputs\n",
      "\t\t\"Merging\" (cluster_count=1)\n",
      "\t\t\tID=226   : size=7         : Merging <:NUM:> intermediate segments out of a total of <:NUM:>\n",
      "\t\t\"org.apache.hadoop.fs.FSError:\" (cluster_count=1)\n",
      "\t\t\tID=241   : size=17        : org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\n",
      "\t\t\"Attempt\" (cluster_count=1)\n",
      "\t\t\tID=295   : size=4         : Attempt to process a enum when a union was expected.\n",
      "\t\t\"Exception\" (cluster_count=1)\n",
      "\t\t\tID=297   : size=2         : Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\n",
      "\t<L=1> (cluster_count=7)\n",
      "\t\tID=68    : size=69        : jetty-<:NUM:>.<:NUM:>.<:NUM:>\n",
      "\t\tID=74    : size=69        : nodeBlacklistingEnabled:true\n",
      "\t\tID=205   : size=18        : WAITING\n",
      "\t\tID=206   : size=126       : <:NUM:>\n",
      "\t\tID=208   : size=25        : RUNNABLE\n",
      "\t<L=17>\n",
      "\t\t\"TaskAttempt:\" (cluster_count=1)\n",
      "\t\t\tID=85    : size=1033      : TaskAttempt: [attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>] using containerId: [container <:NUM:> <:NUM:> <:NUM:> <:NUM:> on NM: <:*:>\n",
      "\t\t\"Saved\" (cluster_count=1)\n",
      "\t\t\tID=195   : size=48        : Saved output of task 'attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>' to <:*:> temporary/<:NUM:>/task <:NUM:> <:NUM:> r <:NUM:>\n",
      "\t\t\"When\" (cluster_count=1)\n",
      "\t\t\tID=296   : size=1         : When stopping the service JobHistoryEventHandler : org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\n",
      "\t<L=12>\n",
      "\t\t\"attempt\" (cluster_count=1)\n",
      "\t\t\tID=86    : size=2921      : attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> to <:*:>\n",
      "\t\t\"Progress\" (cluster_count=1)\n",
      "\t\t\tID=114   : size=45882     : Progress of TaskAttempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> is : <:*:>\n",
      "\t\t\"Container\" (cluster_count=1)\n",
      "\t\t\tID=151   : size=19        : Container complete event for unknown container id container <:NUM:> <:NUM:> <:NUM:> <:NUM:>\n",
      "\t\t\"completedMapPercent\" (cluster_count=1)\n",
      "\t\t\tID=158   : size=531       : completedMapPercent <:NUM:>.<:NUM:> totalResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> finalMapResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> finalReduceResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> netScheduledMapResource:<memory:<:NUM:>, vCores:<:NUM:>> netScheduledReduceResource:<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "\t\t\"Ignoring\" (cluster_count=1)\n",
      "\t\t\tID=180   : size=84        : Ignoring obsolete output of <:*:> map-task: 'attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>'\n",
      "\t\t\"Read\" (cluster_count=1)\n",
      "\t\t\tID=185   : size=649       : Read <:NUM:> bytes from map-output for attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "\t\t\"Task\" (cluster_count=1)\n",
      "\t\t\tID=194   : size=48        : Task attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> is allowed to commit now\n",
      "\t\t\"Reporting\" (cluster_count=1)\n",
      "\t\t\tID=234   : size=1         : Reporting fetch failure for attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> to jobtracker.\n",
      "\t\t\"Added\" (cluster_count=1)\n",
      "\t\t\tID=244   : size=10        : Added attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> to list of failed maps\n",
      "\t\t\"Could\" (cluster_count=1)\n",
      "\t\t\tID=249   : size=4         : Could not parse the old history file. Will not have old AMinfos\n",
      "\t\t\"Found\" (cluster_count=1)\n",
      "\t\t\tID=263   : size=2         : Found jobId job <:NUM:> <:NUM:> to have not been closed. Will close\n",
      "\t\t\"Error\" (cluster_count=1)\n",
      "\t\t\tID=276   : size=2         : Error communicating with RM: Resource Manager doesn't recognize AttemptId: application <:NUM:> <:NUM:>\n",
      "\t\t\"org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException:\" (cluster_count=1)\n",
      "\t\t\tID=278   : size=2         : org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException: Application attempt appattempt <:NUM:> <:NUM:> <:NUM:> doesn't exist in ApplicationMasterService cache.\n",
      "\t\t\"org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException):\" (cluster_count=1)\n",
      "\t\t\tID=279   : size=2         : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException): Application attempt appattempt <:NUM:> <:NUM:> <:NUM:> doesn't exist in ApplicationMasterService cache.\n",
      "\t<L=13>\n",
      "\t\t\"Before\" (cluster_count=1)\n",
      "\t\t\tID=92    : size=797       : Before Scheduling: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\n",
      "\t\t\"After\" (cluster_count=1)\n",
      "\t\t\tID=96    : size=1249      : After Scheduling: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\n",
      "\t\t\"getResources()\" (cluster_count=1)\n",
      "\t\t\tID=97    : size=899       : getResources() for application <:NUM:> <:NUM:>: ask=<:NUM:> release= <:NUM:> newContainers=<:NUM:> finishedContainers=<:NUM:> resourcelimit=<memory:<:NUM:>, vCores:<:NUM:>> knownNMs=<:NUM:>\n",
      "\t\t\"Adding\" (cluster_count=1)\n",
      "\t\t\tID=102   : size=69        : Adding #<:NUM:> tokens and #<:NUM:> secret keys for NM use for launching container\n",
      "\t\t\"JVM\" (cluster_count=1)\n",
      "\t\t\tID=111   : size=902       : JVM with ID : jvm <:NUM:> <:NUM:> <:*:> <:NUM:> asked for a task\n",
      "\t\t\"MapCompletionEvents\" (cluster_count=1)\n",
      "\t\t\tID=113   : size=18140     : MapCompletionEvents request from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>. startIndex <:NUM:> maxEvents <:NUM:>\n",
      "\t\t\"attempt\" (cluster_count=1)\n",
      "\t\t\tID=119   : size=47        : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> TaskAttempt Transitioned from RUNNING to COMMIT PENDING\n",
      "\t\t\"Final\" (cluster_count=1)\n",
      "\t\t\tID=141   : size=48        : Final Stats: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\n",
      "\t\t\"Task:\" (cluster_count=1)\n",
      "\t\t\tID=240   : size=3         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.io.IOException: Spill failed\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=243   : size=6         : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: java.io.IOException: Spill failed\n",
      "\t\t\"cleanup\" (cluster_count=1)\n",
      "\t\t\tID=266   : size=12        : cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: <:*:>\n",
      "\t<L=21>\n",
      "\t\t\"Processing\" (cluster_count=1)\n",
      "\t\t\tID=105   : size=1744      : Processing the event EventType: CONTAINER REMOTE <:*:> for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> taskAttempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "\t\t\"Failed\" (cluster_count=1)\n",
      "\t\t\tID=217   : size=487       : Failed on local exception: <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> Host Details : local host is: \"MININT-FNANLI5/<:IP:>\"; destination host is: \"msra-sa-<:NUM:>\":<:NUM:>;\n",
      "\t\t\"WordCount\" (cluster_count=1)\n",
      "\t\t\tID=274   : size=1         : WordCount and PageRank. Each application has been run for several times, simulating both normal and abnormal cases with injected specific failures.\n",
      "\t\t\"Task:\" (cluster_count=1)\n",
      "\t\t\tID=288   : size=2         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - failed due to FSError: java.io.IOException: There is not enough space on the disk\n",
      "\t<L=16>\n",
      "\t\t\"JVM\" (cluster_count=1)\n",
      "\t\t\tID=112   : size=902       : JVM with ID: jvm <:NUM:> <:NUM:> <:*:> <:NUM:> given task: attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "\t\t\"attempt\" (cluster_count=2)\n",
      "\t\t\tID=162   : size=216       : attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> CONTAINER CLEANUP to <:*:> TASK CLEANUP\n",
      "\t\t\tID=178   : size=667       : attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Shuffling to disk since <:NUM:> is greater than maxSingleShuffleLimit (<:NUM:>)\n",
      "\t\t\"Failed\" (cluster_count=1)\n",
      "\t\t\tID=213   : size=5300      : Failed to renew lease for [DFSClient NONMAPREDUCE <:NUM:> <:NUM:>] for <:NUM:> seconds. Will retry shortly ...\n",
      "\t\t\"Task:\" (cluster_count=1)\n",
      "\t\t\tID=286   : size=2         : Task: attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> - exited : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=287   : size=4         : Diagnostics report from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>: Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\n",
      "\t<L=47>\n",
      "\t\t\"Cannot\" (cluster_count=1)\n",
      "\t\t\tID=149   : size=12        : Cannot assign container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] for a map as either container memory less than required <memory:<:NUM:>, vCores:<:NUM:>> or no pending map tasks - maps.isEmpty=true\n",
      "\t<L=23>\n",
      "\t\t\"Socket\" (cluster_count=1)\n",
      "\t\t\tID=152   : size=217       : Socket Reader #<:NUM:> for port <:NUM:>: readAndProcess from client <:IP:> threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]\n",
      "\t<L=20>\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=166   : size=7         : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: AttemptID:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> Timed out after <:NUM:> secs\n",
      "\t\t\"Failed\" (cluster_count=1)\n",
      "\t\t\tID=252   : size=10        : Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information\n",
      "\t<L=29>\n",
      "\t\t\"Communication\" (cluster_count=2)\n",
      "\t\t\tID=169   : size=44        : Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\n",
      "\t\t\tID=202   : size=10        : Communication exception: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to <:IP:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "\t\t\"for\" (cluster_count=1)\n",
      "\t\t\tID=196   : size=15        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "\t<L=39>\n",
      "\t\t\"for\" (cluster_count=1)\n",
      "\t\t\tID=177   : size=8         : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "\t<L=34>\n",
      "\t\t\"for\" (cluster_count=1)\n",
      "\t\t\tID=188   : size=12        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "\t\t\"When\" (cluster_count=1)\n",
      "\t\t\tID=265   : size=1         : When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "\t<L=26>\n",
      "\t\t\"No\" (cluster_count=1)\n",
      "\t\t\tID=214   : size=5303      : No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "\t\t\"Failed\" (cluster_count=1)\n",
      "\t\t\tID=216   : size=7         : Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\n",
      "\t<L=35>\n",
      "\t\t\"Releasing\" (cluster_count=1)\n",
      "\t\t\tID=230   : size=7         : Releasing unassigned and invalid container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ]. RM may have assignment issues\n",
      "\t\t\"Service\" (cluster_count=1)\n",
      "\t\t\tID=264   : size=2         : Service <:*:> failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "\t<L=25>\n",
      "\t\t\"Task:\" (cluster_count=1)\n",
      "\t\t\tID=235   : size=1         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=236   : size=2         : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\n",
      "\t\t\"IPC\" (cluster_count=1)\n",
      "\t\t\tID=301   : size=1         : IPC Server handler <:NUM:> on <:NUM:>, call statusUpdate(attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>, org.apache.hadoop.mapred.MapTaskStatus@cdcdbf7), rpc version=<:NUM:>, client version=<:NUM:>, methodsFingerPrint=<:NUM:> from <:IP:>:<:NUM:> Call#<:NUM:> Retry#<:NUM:>: output error\n",
      "\t<L=31>\n",
      "\t\t\"Assigning\" (cluster_count=1)\n",
      "\t\t\tID=245   : size=8         : Assigning container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] to fast fail map\n",
      "\t\t\"Failure\" (cluster_count=1)\n",
      "\t\t\tID=250   : size=4         : Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\n",
      "\t\t\"Exception\" (cluster_count=1)\n",
      "\t\t\tID=284   : size=2         : Exception running child : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "\t<L=50>\n",
      "\t\t\"Could\" (cluster_count=1)\n",
      "\t\t\tID=253   : size=5         : Could not obtain BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> from any node: java.io.IOException: No live nodes contain block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> after checking nodes = [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>], ignoredNodes = null No live nodes contain current block Block locations: <:IP:>:<:NUM:> <:IP:>:<:NUM:> Dead nodes: <:IP:>:<:NUM:> <:IP:>:<:NUM:>. Will get new block locations from namenode and retry...\n",
      "\t<L=51>\n",
      "\t\t\"Could\" (cluster_count=1)\n",
      "\t\t\tID=255   : size=1         : Could not obtain BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> from any node: java.io.IOException: No live nodes contain block BP-<:NUM:>-<:IP:><:NUM:>:blk <:NUM:> <:NUM:> after checking nodes = [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>], ignoredNodes = null No live nodes contain current block Block locations: <:IP:>:<:NUM:> <:IP:>:<:NUM:> Dead nodes: <:IP:>:<:NUM:> <:IP:>:<:NUM:> <:IP:>:<:NUM:>. Will get new block locations from namenode and retry...\n",
      "\t<L=27>\n",
      "\t\t\"java.net.NoRouteToHostException:\" (cluster_count=1)\n",
      "\t\t\tID=262   : size=14        : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "\t<L=22>\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=268   : size=12        : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: <:*:>\n",
      "\t<L=30>\n",
      "\t\t\"Exception\" (cluster_count=1)\n",
      "\t\t\tID=285   : size=2         : Exception cleaning up: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "\t<L=44>\n",
      "\t\t\"for\" (cluster_count=1)\n",
      "\t\t\tID=289   : size=3         : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "\t<L=37>\n",
      "\t\t\"Task:\" (cluster_count=1)\n",
      "\t\t\tID=290   : size=2         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "\t\t\"Diagnostics\" (cluster_count=1)\n",
      "\t\t\tID=291   : size=4         : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "total          : took    14.37 s (100.00%),    395,363 samples,   36.35 ms / 1000 samples,       27,508.28 hz\n",
      "mask           : took     7.35 s ( 51.14%),    395,363 samples,   18.59 ms / 1000 samples,       53,790.49 hz\n",
      "drain          : took     4.90 s ( 34.06%),    395,363 samples,   12.38 ms / 1000 samples,       80,753.37 hz\n",
      "tree_search    : took     1.66 s ( 11.52%),    395,363 samples,    4.19 ms / 1000 samples,      238,688.06 hz\n",
      "cluster_exist  : took     1.35 s (  9.38%),    395,061 samples,    3.41 ms / 1000 samples,      293,192.41 hz\n",
      "create_cluster : took     0.01 s (  0.05%),        302 samples,   25.41 ms / 1000 samples,       39,356.22 hz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_miner = mineModel(f = open(\"/Users/shuming/Downloads/merged_hadoop/merged.log\"),cfg='../data/drain3.ini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "def57a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_log = ['1445087491445_0005','1445087491445_0007', \n",
    "'1445175094696_0005','1445062781478_0011','1445062781478_0016','1445062781478_0019'\n",
    "'1445076437777_0002','1445076437777_0005','1445144423722_0021','1445144423722_0024'\n",
    "'1445182159119_0012']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30f89434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read from history task task <:NUM:> <:NUM:> m <:NUM:>\n",
      "47\n",
      "1445076437777_0005\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445087491445_0005\n",
      "1445076437777_0005\n",
      "1445087491445_0007\n",
      "1445094324383_0002\n",
      "1445087491445_0004\n",
      "1445087491445_0004\n",
      "1445087491445_0010\n",
      "1445094324383_0001\n",
      "1445087491445_0004\n",
      "1445094324383_0001\n",
      "1445182159119_0011\n",
      "1445062781478_0019\n",
      "1445087491445_0006\n",
      "1445182159119_0011\n",
      "1445087491445_0001\n",
      "1445144423722_0022\n",
      "1445182159119_0016\n",
      "1445076437777_0001\n",
      "1445076437777_0001\n",
      "1445144423722_0022\n",
      "1445087491445_0001\n",
      "1445182159119_0002\n",
      "1445182159119_0002\n",
      "1445182159119_0014\n",
      "1445144423722_0020\n",
      "1445094324383_0005\n",
      "1445144423722_0023\n",
      "1445144423722_0023\n",
      "1445094324383_0005\n",
      "1445087491445_0003\n",
      "1445087491445_0002\n",
      "1445076437777_0002\n",
      "1445182159119_0015\n",
      "1445182159119_0015\n",
      "1445094324383_0004\n",
      "1445087491445_0002\n",
      "1445062781478_0020\n",
      "1445182159119_0001\n",
      "1445062781478_0020\n",
      "1445062781478_0020\n",
      "1445076437777_0002\n",
      "1445144423722_0021\n",
      "1445182159119_0015\n",
      "1445087491445_0002\n",
      "1445094324383_0004\n",
      "1445087491445_0002\n",
      "1445182159119_0015\n",
      "1445087491445_0003\n",
      "1445144423722_0023\n",
      "1445087491445_0003\n",
      "1445094324383_0005\n",
      "1445094324383_0005\n",
      "1445144423722_0023\n",
      "1445144423722_0020\n",
      "1445144423722_0020\n",
      "1445182159119_0002\n",
      "1445182159119_0002\n",
      "1445182159119_0016\n",
      "1445144423722_0022\n",
      "1445076437777_0001\n",
      "1445087491445_0001\n",
      "1445087491445_0001\n",
      "1445076437777_0001\n",
      "1445144423722_0022\n",
      "1445182159119_0005\n",
      "1445062781478_0019\n",
      "1445182159119_0011\n",
      "1445182159119_0011\n",
      "1445087491445_0006\n",
      "1445062781478_0019\n",
      "1445094324383_0001\n",
      "1445087491445_0004\n",
      "1445094324383_0001\n",
      "1445087491445_0010\n",
      "1445087491445_0004\n",
      "1445062781478_0018\n",
      "1445087491445_0004\n",
      "1445094324383_0002\n",
      "1445087491445_0007\n",
      "\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445182159119_0012\n",
      "1445076437777_0005\n",
      "1445087491445_0005\n",
      "1445182159119_0012\n",
      "1445076437777_0005\n",
      "1445087491445_0005\n",
      "1445087491445_0005\n",
      "1445076437777_0005\n",
      "1445076437777_0004\n",
      "1445094324383_0002\n",
      "1445062781478_0018\n",
      "1445087491445_0004\n",
      "1445094324383_0001\n",
      "1445062781478_0018\n",
      "1445094324383_0001\n",
      "1445087491445_0004\n",
      "1445182159119_0004\n",
      "1445182159119_0011\n",
      "1445062781478_0019\n",
      "1445087491445_0006\n",
      "1445182159119_0011\n",
      "1445182159119_0005\n",
      "1445087491445_0001\n",
      "1445076437777_0001\n",
      "1445182159119_0016\n",
      "1445144423722_0022\n",
      "1445182159119_0016\n",
      "1445144423722_0022\n",
      "1445076437777_0001\n",
      "1445087491445_0001\n",
      "1445182159119_0002\n",
      "1445182159119_0002\n",
      "1445144423722_0020\n",
      "1445144423722_0020\n",
      "1445182159119_0003\n",
      "1445094324383_0005\n",
      "1445182159119_0014\n",
      "1445076437777_0003\n",
      "1445144423722_0023\n",
      "1445144423722_0023\n",
      "1445094324383_0005\n",
      "1445144423722_0021\n",
      "1445182159119_0015\n",
      "1445076437777_0002\n",
      "1445182159119_0015\n",
      "1445087491445_0002\n",
      "1445062781478_0020\n",
      "1445182159119_0001\n",
      "1445182159119_0001\n",
      "1445062781478_0020\n",
      "1445062781478_0020\n",
      "1445182159119_0015\n",
      "1445087491445_0002\n",
      "1445087491445_0002\n",
      "1445076437777_0002\n",
      "1445182159119_0015\n",
      "1445144423722_0021\n",
      "1445144423722_0023\n",
      "1445094324383_0005\n",
      "1445094324383_0005\n",
      "1445144423722_0023\n",
      "1445076437777_0003\n",
      "1445182159119_0014\n",
      "1445182159119_0003\n",
      "1445144423722_0020\n",
      "1445144423722_0020\n",
      "1445182159119_0002\n",
      "1445182159119_0002\n",
      "1445076437777_0001\n",
      "1445144423722_0022\n",
      "1445182159119_0016\n",
      "1445087491445_0001\n",
      "1445087491445_0001\n",
      "1445144423722_0022\n",
      "1445182159119_0016\n",
      "1445076437777_0001\n",
      "1445182159119_0005\n",
      "1445182159119_0011\n",
      "1445182159119_0011\n",
      "1445062781478_0019\n",
      "1445182159119_0004\n",
      "1445087491445_0004\n",
      "1445094324383_0001\n",
      "1445062781478_0018\n",
      "1445094324383_0001\n",
      "1445087491445_0004\n",
      "1445094324383_0002\n",
      "1445076437777_0004\n",
      "1445062781478_0018\n",
      "1445087491445_0005\n",
      "1445076437777_0005\n",
      "1445076437777_0005\n",
      "1445182159119_0012\n",
      "1445087491445_0005\n",
      "1445087491445_0005\n",
      "1445182159119_0012\n",
      "1445076437777_0005\n",
      "1445076437777_0005\n",
      "1445182159119_0012\n",
      "1445087491445_0005\n",
      "1445094324383_0002\n",
      "1445182159119_0013\n",
      "1445087491445_0010\n",
      "1445094324383_0002\n",
      "1445062781478_0018\n",
      "1445182159119_0004\n",
      "1445094324383_0001\n",
      "1445144423722_0024\n",
      "1445062781478_0018\n",
      "1445087491445_0004\n",
      "1445182159119_0005\n",
      "1445182159119_0005\n",
      "1445062781478_0019\n",
      "1445087491445_0006\n",
      "1445182159119_0011\n",
      "1445182159119_0011\n",
      "1445062781478_0019\n",
      "1445087491445_0006\n",
      "1445182159119_0002\n",
      "1445182159119_0002\n",
      "1445076437777_0001\n",
      "1445144423722_0022\n",
      "1445087491445_0001\n",
      "1445182159119_0016\n",
      "1445144423722_0022\n",
      "1445182159119_0003\n",
      "1445144423722_0020\n",
      "1445182159119_0017\n",
      "1445144423722_0020\n",
      "1445182159119_0014\n",
      "1445087491445_0003\n",
      "1445144423722_0023\n",
      "1445094324383_0005\n",
      "1445076437777_0003\n",
      "1445144423722_0023\n",
      "1445144423722_0020\n",
      "1445062781478_0020\n",
      "1445062781478_0020\n",
      "1445182159119_0001\n",
      "1445144423722_0021\n",
      "1445182159119_0015\n",
      "1445076437777_0002\n",
      "1445087491445_0002\n",
      "1445087491445_0002\n",
      "1445076437777_0002\n",
      "1445144423722_0021\n",
      "1445182159119_0015\n",
      "1445087491445_0002\n",
      "1445182159119_0015\n",
      "1445144423722_0021\n",
      "1445076437777_0002\n",
      "1445076437777_0002\n",
      "1445182159119_0015\n",
      "1445144423722_0021\n",
      "1445087491445_0002\n",
      "1445062781478_0020\n",
      "1445182159119_0001\n",
      "1445182159119_0001\n",
      "1445062781478_0020\n",
      "1445094324383_0005\n",
      "1445144423722_0020\n",
      "1445144423722_0023\n",
      "1445076437777_0003\n",
      "1445144423722_0023\n",
      "1445094324383_0005\n",
      "1445087491445_0003\n",
      "1445182159119_0014\n",
      "1445144423722_0020\n",
      "1445182159119_0017\n",
      "1445144423722_0020\n",
      "1445087491445_0001\n",
      "1445076437777_0001\n",
      "1445144423722_0022\n",
      "1445182159119_0016\n",
      "1445144423722_0022\n",
      "1445076437777_0001\n",
      "1445087491445_0001\n",
      "1445182159119_0002\n",
      "1445182159119_0002\n",
      "1445182159119_0011\n",
      "1445087491445_0006\n",
      "1445062781478_0019\n",
      "1445087491445_0006\n",
      "1445062781478_0019\n",
      "1445182159119_0011\n",
      "1445182159119_0005\n",
      "1445062781478_0018\n",
      "1445144423722_0024\n",
      "1445182159119_0004\n",
      "1445062781478_0018\n",
      "1445182159119_0013\n",
      "1445094324383_0002\n",
      "1445094324383_0002\n",
      "1445182159119_0012\n",
      "1445076437777_0005\n",
      "1445087491445_0005\n",
      "1445182159119_0012\n",
      "1445087491445_0005\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445076437777_0005\n",
      "1445182159119_0012\n",
      "1445182159119_0012\n",
      "1445076437777_0005\n",
      "1445087491445_0005\n",
      "1445087491445_0010\n",
      "1445087491445_0010\n",
      "1445087491445_0004\n",
      "1445094324383_0002\n",
      "1445094324383_0002\n",
      "1445087491445_0004\n",
      "1445087491445_0007\n",
      "1445094324383_0001\n",
      "1445182159119_0005\n",
      "1445182159119_0005\n",
      "1445062781478_0019\n",
      "1445087491445_0006\n",
      "1445182159119_0011\n",
      "1445062781478_0019\n",
      "1445182159119_0002\n",
      "1445182159119_0002\n",
      "1445144423722_0022\n",
      "1445182159119_0016\n",
      "1445076437777_0001\n",
      "1445087491445_0001\n",
      "1445087491445_0001\n",
      "1445182159119_0016\n",
      "1445182159119_0014\n",
      "1445144423722_0020\n",
      "1445144423722_0020\n",
      "1445087491445_0003\n",
      "1445144423722_0023\n",
      "1445087491445_0003\n",
      "1445094324383_0005\n",
      "1445094324383_0005\n",
      "1445182159119_0001\n",
      "1445062781478_0020\n",
      "1445062781478_0020\n",
      "1445182159119_0001\n",
      "1445076437777_0002\n",
      "1445182159119_0015\n",
      "1445144423722_0021\n",
      "1445087491445_0002\n",
      "1445094324383_0004\n",
      "1445087491445_0002\n",
      "1445182159119_0015\n",
      "1445144423722_0021\n",
      "1445076437777_0002\n",
      "1445087491445_0002\n",
      "1445076437777_0002\n",
      "1445144423722_0021\n",
      "1445182159119_0015\n",
      "1445144423722_0021\n",
      "1445076437777_0002\n",
      "1445094324383_0004\n",
      "1445087491445_0002\n",
      "1445062781478_0020\n",
      "1445182159119_0001\n",
      "1445182159119_0001\n",
      "1445062781478_0020\n",
      "1445094324383_0005\n",
      "1445144423722_0023\n",
      "1445144423722_0023\n",
      "1445094324383_0005\n",
      "1445087491445_0003\n",
      "1445087491445_0003\n",
      "1445144423722_0020\n",
      "1445144423722_0020\n",
      "1445087491445_0001\n",
      "1445182159119_0016\n",
      "1445144423722_0022\n",
      "1445076437777_0001\n",
      "1445076437777_0001\n",
      "1445182159119_0016\n",
      "1445144423722_0022\n",
      "1445087491445_0001\n",
      "1445182159119_0002\n",
      "1445182159119_0011\n",
      "1445087491445_0006\n",
      "1445062781478_0019\n",
      "1445087491445_0006\n",
      "1445062781478_0019\n",
      "1445182159119_0011\n",
      "1445182159119_0005\n",
      "1445182159119_0005\n",
      "1445094324383_0001\n",
      "1445087491445_0007\n",
      "1445087491445_0004\n",
      "1445062781478_0018\n",
      "1445094324383_0002\n",
      "1445094324383_0002\n",
      "1445087491445_0004\n",
      "1445087491445_0010\n",
      "1445087491445_0010\n",
      "1445076437777_0005\n",
      "1445182159119_0012\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445087491445_0005\n",
      "1445182159119_0012\n",
      "1445062781478_0014\n",
      "1445175094696_0003\n",
      "1445175094696_0003\n",
      "1445087491445_0009\n",
      "1445087491445_0008\n",
      "1445062781478_0017\n",
      "1445087491445_0008\n",
      "1445062781478_0017\n",
      "1445175094696_0004\n",
      "1445182159119_0019\n",
      "1445182159119_0019\n",
      "1445062781478_0013\n",
      "1445062781478_0013\n",
      "1445062781478_0013\n",
      "1445062781478_0013\n",
      "1445182159119_0019\n",
      "1445175094696_0004\n",
      "1445182159119_0019\n",
      "1445062781478_0017\n",
      "1445087491445_0008\n",
      "1445062781478_0017\n",
      "1445087491445_0008\n",
      "1445087491445_0009\n",
      "1445175094696_0003\n",
      "1445175094696_0003\n",
      "1445062781478_0014\n",
      "1445062781478_0014\n",
      "1445175094696_0002\n",
      "1445062781478_0014\n",
      "1445182159119_0020\n",
      "1445175094696_0003\n",
      "1445175094696_0003\n",
      "1445062781478_0016\n",
      "1445087491445_0008\n",
      "1445062781478_0017\n",
      "1445087491445_0008\n",
      "1445062781478_0017\n",
      "1445062781478_0013\n",
      "1445175094696_0004\n",
      "1445182159119_0019\n",
      "1445182159119_0019\n",
      "1445175094696_0004\n",
      "1445062781478_0012\n",
      "1445062781478_0013\n",
      "1445182159119_0018\n",
      "1445182159119_0018\n",
      "1445062781478_0013\n",
      "1445182159119_0019\n",
      "1445062781478_0012\n",
      "1445175094696_0004\n",
      "1445182159119_0019\n",
      "1445062781478_0013\n",
      "1445062781478_0017\n",
      "1445087491445_0008\n",
      "1445062781478_0017\n",
      "1445087491445_0008\n",
      "1445062781478_0016\n",
      "1445175094696_0003\n",
      "1445175094696_0003\n",
      "1445182159119_0020\n",
      "1445062781478_0014\n",
      "1445175094696_0002\n",
      "1445062781478_0014\n",
      "1445062781478_0014\n",
      "1445062781478_0015\n",
      "1445175094696_0003\n",
      "1445175094696_0001\n",
      "1445087491445_0008\n",
      "1445062781478_0017\n",
      "1445062781478_0017\n",
      "1445182159119_0018\n",
      "1445062781478_0011\n",
      "1445182159119_0019\n",
      "1445062781478_0012\n",
      "1445175094696_0004\n",
      "1445175094696_0004\n",
      "1445182159119_0018\n",
      "1445062781478_0013\n",
      "1445175094696_0005\n",
      "1445175094696_0005\n",
      "1445062781478_0013\n",
      "1445175094696_0004\n",
      "1445182159119_0019\n",
      "1445175094696_0004\n",
      "1445062781478_0012\n",
      "1445062781478_0011\n",
      "1445182159119_0018\n",
      "1445062781478_0017\n",
      "1445062781478_0017\n",
      "1445175094696_0001\n",
      "1445175094696_0003\n",
      "1445062781478_0015\n",
      "1445062781478_0014\n",
      "1445062781478_0014\n",
      "1445062781478_0014\n",
      "1445062781478_0014\n",
      "1445175094696_0003\n",
      "1445087491445_0009\n",
      "1445087491445_0008\n",
      "1445062781478_0017\n",
      "1445182159119_0018\n",
      "1445182159119_0019\n",
      "1445175094696_0004\n",
      "1445175094696_0004\n",
      "1445182159119_0019\n",
      "1445062781478_0013\n",
      "1445175094696_0004\n",
      "1445182159119_0019\n",
      "1445175094696_0004\n",
      "1445062781478_0017\n",
      "1445087491445_0008\n",
      "1445062781478_0017\n",
      "1445087491445_0009\n",
      "1445175094696_0003\n",
      "1445175094696_0003\n",
      "1445062781478_0014\n",
      "1445062781478_0014\n",
      "1445175094696_0002\n",
      "1445062781478_0014\n",
      "1445175094696_0002\n",
      "1445062781478_0015\n",
      "1445062781478_0015\n",
      "1445182159119_0020\n",
      "1445062781478_0016\n",
      "1445175094696_0003\n",
      "1445087491445_0009\n",
      "1445175094696_0001\n",
      "1445175094696_0001\n",
      "1445062781478_0013\n",
      "1445062781478_0011\n",
      "1445182159119_0019\n",
      "1445062781478_0011\n",
      "1445175094696_0004\n",
      "1445062781478_0012\n",
      "1445062781478_0012\n",
      "1445175094696_0005\n",
      "1445182159119_0018\n",
      "1445182159119_0018\n",
      "1445175094696_0005\n",
      "1445182159119_0018\n",
      "1445175094696_0005\n",
      "1445175094696_0005\n",
      "1445182159119_0018\n",
      "1445062781478_0012\n",
      "1445062781478_0012\n",
      "1445175094696_0004\n",
      "1445182159119_0019\n",
      "1445062781478_0011\n",
      "1445062781478_0011\n",
      "1445062781478_0013\n",
      "1445062781478_0013\n",
      "1445175094696_0001\n",
      "1445175094696_0001\n",
      "1445087491445_0009\n",
      "1445087491445_0009\n",
      "1445062781478_0016\n",
      "1445175094696_0003\n",
      "1445182159119_0020\n",
      "1445062781478_0015\n",
      "1445175094696_0002\n",
      "1445087491445_0008\n",
      "1445062781478_0014\n",
      "1445175094696_0002\n",
      "1445175094696_0002\n",
      "1445175094696_0002\n",
      "1445062781478_0015\n",
      "1445062781478_0015\n",
      "1445062781478_0016\n",
      "1445087491445_0009\n",
      "1445175094696_0003\n",
      "1445062781478_0016\n",
      "1445087491445_0009\n",
      "1445175094696_0001\n",
      "1445175094696_0001\n",
      "1445062781478_0013\n",
      "1445062781478_0011\n",
      "1445062781478_0011\n",
      "1445062781478_0012\n",
      "1445175094696_0005\n",
      "1445182159119_0018\n",
      "1445175094696_0005\n",
      "1445182159119_0018\n",
      "1445175094696_0005\n",
      "1445175094696_0005\n",
      "1445182159119_0018\n",
      "1445062781478_0012\n",
      "1445062781478_0012\n",
      "1445062781478_0011\n",
      "1445062781478_0011\n",
      "1445182159119_0019\n",
      "1445062781478_0013\n",
      "1445062781478_0013\n",
      "1445175094696_0001\n",
      "1445175094696_0001\n",
      "1445087491445_0009\n",
      "1445087491445_0009\n",
      "1445062781478_0016\n",
      "1445182159119_0020\n",
      "1445062781478_0015\n",
      "1445062781478_0015\n",
      "1445175094696_0002\n",
      "1445087491445_0008\n",
      "1445175094696_0002\n",
      "1445175094696_0002\n",
      "1445182159119_0020\n",
      "1445182159119_0020\n",
      "1445062781478_0015\n",
      "1445062781478_0015\n",
      "1445062781478_0016\n",
      "1445087491445_0009\n",
      "1445062781478_0016\n",
      "1445175094696_0003\n",
      "1445087491445_0009\n",
      "1445175094696_0001\n",
      "1445087491445_0008\n",
      "1445175094696_0001\n",
      "1445062781478_0013\n",
      "1445062781478_0013\n",
      "1445062781478_0011\n",
      "1445062781478_0012\n",
      "1445062781478_0012\n",
      "1445182159119_0018\n",
      "1445182159119_0018\n",
      "1445182159119_0018\n",
      "1445182159119_0018\n",
      "1445175094696_0005\n",
      "1445062781478_0012\n",
      "1445062781478_0012\n",
      "1445062781478_0011\n",
      "1445062781478_0013\n",
      "1445062781478_0013\n",
      "1445087491445_0008\n",
      "1445175094696_0001\n",
      "1445087491445_0009\n",
      "1445062781478_0016\n",
      "1445087491445_0009\n",
      "1445062781478_0016\n",
      "1445062781478_0015\n",
      "1445182159119_0020\n",
      "1445182159119_0020\n",
      "1445175094696_0002\n",
      "1445175094696_0002\n",
      "1445175094696_0002\n",
      "1445175094696_0002\n",
      "1445182159119_0020\n",
      "1445182159119_0020\n",
      "1445062781478_0015\n",
      "1445062781478_0015\n",
      "1445175094696_0003\n",
      "1445062781478_0016\n",
      "1445087491445_0009\n",
      "1445062781478_0016\n",
      "1445087491445_0009\n",
      "1445175094696_0001\n",
      "1445175094696_0001\n",
      "1445062781478_0017\n",
      "1445062781478_0013\n",
      "1445062781478_0013\n",
      "1445062781478_0011\n",
      "1445182159119_0019\n",
      "1445062781478_0012\n",
      "1445062781478_0013\n",
      "1445175094696_0005\n",
      "1445182159119_0018\n",
      "1445175094696_0005\n",
      "1445182159119_0018\n",
      "1445182159119_0018\n",
      "1445062781478_0013\n",
      "1445062781478_0012\n",
      "1445182159119_0019\n",
      "1445062781478_0012\n",
      "1445062781478_0011\n",
      "1445062781478_0013\n",
      "1445062781478_0013\n",
      "1445062781478_0017\n",
      "1445175094696_0001\n",
      "1445087491445_0009\n",
      "1445062781478_0016\n",
      "1445087491445_0009\n",
      "1445062781478_0016\n",
      "1445175094696_0003\n",
      "1445062781478_0015\n",
      "1445182159119_0020\n",
      "1445182159119_0020\n",
      "1445175094696_0002\n",
      "1445175094696_0002\n",
      "1445087491445_0005\n",
      "1445182159119_0012\n",
      "1445094324383_0003\n",
      "1445094324383_0003\n",
      "1445087491445_0010\n",
      "1445087491445_0010\n",
      "1445076437777_0004\n",
      "1445182159119_0013\n",
      "1445087491445_0004\n",
      "1445182159119_0013\n",
      "1445182159119_0004\n",
      "1445182159119_0004\n",
      "1445144423722_0024\n",
      "1445087491445_0007\n",
      "1445087491445_0007\n",
      "1445062781478_0018\n",
      "1445144423722_0024\n",
      "1445182159119_0005\n",
      "1445062781478_0019\n",
      "1445182159119_0002\n",
      "abel.txt\n",
      "1445182159119_0016\n",
      "1445087491445_0002\n",
      "1445182159119_0003\n",
      "1445182159119_0003\n",
      "1445182159119_0017\n",
      "1445144423722_0020\n",
      "1445144423722_0023\n",
      "1445182159119_0017\n",
      "1445087491445_0003\n",
      "1445182159119_0014\n",
      "1445144423722_0020\n",
      "1445076437777_0003\n",
      "1445087491445_0003\n",
      "1445062781478_0020\n",
      "1445182159119_0001\n",
      "1445094324383_0004\n",
      "1445076437777_0002\n",
      "1445144423722_0021\n",
      "1445076437777_0001\n",
      "1445182159119_0015\n",
      "1445094324383_0004\n",
      "1445094324383_0004\n",
      "1445182159119_0015\n",
      "1445144423722_0021\n",
      "1445076437777_0002\n",
      "1445182159119_0001\n",
      "1445062781478_0020\n",
      "1445182159119_0002\n",
      "1445076437777_0003\n",
      "1445144423722_0020\n",
      "1445182159119_0014\n",
      "1445087491445_0003\n",
      "1445087491445_0003\n",
      "1445182159119_0014\n",
      "1445182159119_0017\n",
      "1445144423722_0023\n",
      "1445144423722_0020\n",
      "1445182159119_0017\n",
      "1445182159119_0003\n",
      "1445182159119_0003\n",
      "1445182159119_0016\n",
      "1445182159119_0002\n",
      "1445062781478_0019\n",
      "1445182159119_0005\n",
      "1445062781478_0018\n",
      "1445087491445_0007\n",
      "1445144423722_0024\n",
      "1445144423722_0024\n",
      "1445087491445_0007\n",
      "1445182159119_0004\n",
      "1445182159119_0013\n",
      "1445076437777_0004\n",
      "1445087491445_0004\n",
      "1445087491445_0010\n",
      "1445087491445_0010\n",
      "1445094324383_0003\n",
      "1445182159119_0012\n",
      "1445094324383_0003\n",
      "1445087491445_0005\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445087491445_0005\n",
      "1445087491445_0010\n",
      "1445087491445_0010\n",
      "1445182159119_0013\n",
      "1445076437777_0004\n",
      "1445087491445_0004\n",
      "1445087491445_0004\n",
      "1445094324383_0002\n",
      "1445182159119_0013\n",
      "1445182159119_0004\n",
      "1445182159119_0004\n",
      "1445144423722_0024\n",
      "1445087491445_0007\n",
      "1445062781478_0018\n",
      "1445087491445_0007\n",
      "1445062781478_0018\n",
      "1445144423722_0024\n",
      "1445087491445_0006\n",
      "1445087491445_0001\n",
      "1445087491445_0001\n",
      "1445182159119_0003\n",
      "1445182159119_0017\n",
      "1445144423722_0023\n",
      "1445182159119_0017\n",
      "1445087491445_0003\n",
      "1445144423722_0020\n",
      "1445182159119_0014\n",
      "1445076437777_0003\n",
      "1445076437777_0003\n",
      "1445144423722_0020\n",
      "1445087491445_0003\n",
      "1445094324383_0005\n",
      "1445094324383_0004\n",
      "1445076437777_0001\n",
      "1445087491445_0002\n",
      "1445094324383_0004\n",
      "1445076437777_0001\n",
      "1445094324383_0004\n",
      "1445087491445_0002\n",
      "1445094324383_0004\n",
      "1445182159119_0002\n",
      "1445087491445_0002\n",
      "1445182159119_0014\n",
      "1445144423722_0020\n",
      "1445076437777_0003\n",
      "1445094324383_0005\n",
      "1445087491445_0003\n",
      "1445087491445_0003\n",
      "1445076437777_0003\n",
      "1445182159119_0014\n",
      "1445144423722_0020\n",
      "1445182159119_0017\n",
      "1445182159119_0017\n",
      "1445182159119_0003\n",
      "1445182159119_0003\n",
      "1445087491445_0001\n",
      "1445087491445_0006\n",
      "1445062781478_0018\n",
      "1445087491445_0007\n",
      "1445144423722_0024\n",
      "1445144423722_0024\n",
      "1445062781478_0018\n",
      "1445182159119_0004\n",
      "1445182159119_0004\n",
      "1445094324383_0002\n",
      "1445087491445_0004\n",
      "1445182159119_0013\n",
      "1445076437777_0004\n",
      "1445182159119_0013\n",
      "1445087491445_0004\n",
      "1445087491445_0010\n",
      "1445087491445_0010\n",
      "1445094324383_0003\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445087491445_0005\n",
      "1445087491445_0004\n",
      "1445182159119_0013\n",
      "1445076437777_0004\n",
      "1445076437777_0004\n",
      "1445182159119_0013\n",
      "1445087491445_0004\n",
      "1445087491445_0010\n",
      "1445087491445_0010\n",
      "1445087491445_0004\n",
      "1445062781478_0018\n",
      "1445094324383_0001\n",
      "1445087491445_0007\n",
      "1445062781478_0018\n",
      "1445182159119_0004\n",
      "1445087491445_0002\n",
      "1445087491445_0001\n",
      "1445087491445_0001\n",
      "1445182159119_0017\n",
      "1445144423722_0023\n",
      "1445182159119_0003\n",
      "1445182159119_0003\n",
      "1445144423722_0020\n",
      "1445182159119_0014\n",
      "1445076437777_0003\n",
      "1445094324383_0005\n",
      "1445087491445_0003\n",
      "1445087491445_0003\n",
      "1445076437777_0003\n",
      "1445182159119_0014\n",
      "1445094324383_0005\n",
      "1445094324383_0004\n",
      "1445087491445_0002\n",
      "1445094324383_0004\n",
      "1445094324383_0004\n",
      "1445087491445_0002\n",
      "1445094324383_0004\n",
      "1445094324383_0005\n",
      "1445087491445_0003\n",
      "1445182159119_0014\n",
      "1445076437777_0003\n",
      "1445182159119_0014\n",
      "1445087491445_0003\n",
      "1445094324383_0005\n",
      "1445182159119_0003\n",
      "1445182159119_0003\n",
      "1445182159119_0017\n",
      "1445144423722_0023\n",
      "1445087491445_0001\n",
      "1445087491445_0001\n",
      "1445087491445_0002\n",
      "1445182159119_0004\n",
      "1445144423722_0024\n",
      "1445062781478_0018\n",
      "1445087491445_0007\n",
      "1445094324383_0001\n",
      "1445087491445_0004\n",
      "1445087491445_0010\n",
      "1445087491445_0010\n",
      "1445182159119_0013\n",
      "1445076437777_0004\n",
      "1445087491445_0004\n",
      "1445087491445_0004\n",
      "1445076437777_0004\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445087491445_0006\n",
      "1445087491445_0005\n",
      "1445094324383_0003\n",
      "1445094324383_0003\n",
      "1445076437777_0005\n",
      "1445087491445_0005\n",
      "1445087491445_0004\n",
      "1445076437777_0004\n",
      "1445182159119_0013\n",
      "1445182159119_0013\n",
      "1445087491445_0004\n",
      "1445087491445_0010\n",
      "1445062781478_0018\n",
      "1445144423722_0024\n",
      "1445062781478_0018\n",
      "1445182159119_0004\n",
      "1445182159119_0004\n",
      "1445182159119_0011\n",
      "1445182159119_0016\n",
      "1445144423722_0022\n",
      "1445076437777_0001\n",
      "1445182159119_0002\n",
      "1445087491445_0002\n",
      "1445087491445_0001\n",
      "1445182159119_0002\n",
      "1445182159119_0017\n",
      "1445144423722_0020\n",
      "1445182159119_0003\n",
      "1445182159119_0003\n",
      "1445076437777_0003\n",
      "1445144423722_0023\n",
      "1445182159119_0014\n",
      "1445087491445_0003\n",
      "1445087491445_0003\n",
      "1445182159119_0014\n",
      "1445144423722_0020\n",
      "1445076437777_0003\n",
      "1445094324383_0005\n",
      "1445076437777_0001\n",
      "1445094324383_0004\n",
      "1445094324383_0004\n",
      "1445182159119_0015\n",
      "1445094324383_0004\n",
      "1445182159119_0015\n",
      "1445076437777_0001\n",
      "1445094324383_0004\n",
      "1445094324383_0005\n",
      "1445076437777_0003\n",
      "1445144423722_0020\n",
      "1445182159119_0014\n",
      "1445182159119_0014\n",
      "1445144423722_0023\n",
      "1445076437777_0003\n",
      "1445087491445_0003\n",
      "1445182159119_0003\n",
      "1445182159119_0003\n",
      "1445144423722_0020\n",
      "1445182159119_0017\n",
      "1445087491445_0001\n",
      "1445182159119_0002\n",
      "1445076437777_0001\n",
      "1445144423722_0022\n",
      "1445182159119_0016\n",
      "1445182159119_0011\n",
      "1445182159119_0004\n",
      "1445182159119_0004\n",
      "1445062781478_0018\n",
      "1445062781478_0018\n",
      "1445087491445_0007\n",
      "1445144423722_0024\n",
      "1445087491445_0010\n",
      "1445087491445_0010\n",
      "1445076437777_0004\n",
      "1445182159119_0013\n",
      "1445087491445_0004\n",
      "1445087491445_0004\n",
      "1445076437777_0004\n",
      "1445087491445_0005\n",
      "1445076437777_0005\n",
      "1445094324383_0003\n",
      "1445094324383_0003\n",
      "1445087491445_0006\n"
     ]
    }
   ],
   "source": [
    "label = []\n",
    "events = []\n",
    "identifier = []\n",
    "\n",
    "# open files one by one\n",
    "meragefiledir = '/Users/shuming/Downloads/merged_hadoop/'\n",
    "filenames=os.listdir(meragefiledir)  \n",
    "   \n",
    "for filename in filenames: \n",
    "    sequence = []\n",
    "    filepath=meragefiledir+filename   \n",
    "    if filename[10:28] in normal_log:\n",
    "        label.append(0)\n",
    "    else:\n",
    "        label.append(1) \n",
    "    for line in open(filepath):     \n",
    "        match = template_miner.match(line)\n",
    "        sequence.append('E'+match.cluster_id)\n",
    "    identifier(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0563bdb",
   "metadata": {},
   "source": [
    "### Parse Log files from HDFS\n",
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c5c5871b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Drain3 template miner\n",
      "Input (1): Receiving block blk_-1608999687919862906 src: /10.250.19.102:54106 dest: /10.250.19.102:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 1, \"cluster_size\": 1, \"template_mined\": \"Receiving block <:BLOCKID:> src: /10.250.19.102:54106 dest: /10.250.19.102:50010\", \"cluster_count\": 1}\n",
      "Input (2): BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.jar. blk_-1608999687919862906\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 2, \"cluster_size\": 1, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job 200811092030 0001/job.jar. <:BLOCKID:>\", \"cluster_count\": 2}\n",
      "Input (3): Receiving block blk_-1608999687919862906 src: /10.250.10.6:40524 dest: /10.250.10.6:50010\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 1, \"cluster_size\": 2, \"template_mined\": \"Receiving block <:BLOCKID:> src: <:*:> dest: <:*:>\", \"cluster_count\": 2}\n",
      "Input (5): PacketResponder 1 for block blk_-1608999687919862906 terminating\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 3, \"cluster_size\": 1, \"template_mined\": \"PacketResponder 1 for block <:BLOCKID:> terminating\", \"cluster_count\": 3}\n",
      "Input (6): PacketResponder 2 for block blk_-1608999687919862906 terminating\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 3, \"cluster_size\": 2, \"template_mined\": \"PacketResponder <:*:> for block <:BLOCKID:> terminating\", \"cluster_count\": 3}\n",
      "Input (7): Received block blk_-1608999687919862906 of size 91178 from /10.250.10.6\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 4, \"cluster_size\": 1, \"template_mined\": \"Received block <:BLOCKID:> of size 91178 from /10.250.10.6\", \"cluster_count\": 4}\n",
      "Input (8): Received block blk_-1608999687919862906 of size 91178 from /10.250.19.102\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 4, \"cluster_size\": 2, \"template_mined\": \"Received block <:BLOCKID:> of size 91178 from <:*:>\", \"cluster_count\": 4}\n",
      "Input (11): BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.10.6:50010 is added to blk_-1608999687919862906 size 91178\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 5, \"cluster_size\": 1, \"template_mined\": \"BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.10.6:50010 is added to <:BLOCKID:> size 91178\", \"cluster_count\": 5}\n",
      "Input (12): BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.209:50010 is added to blk_-1608999687919862906 size 91178\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 5, \"cluster_size\": 2, \"template_mined\": \"BLOCK* NameSystem.addStoredBlock: blockMap updated: <:*:> is added to <:BLOCKID:> size 91178\", \"cluster_count\": 5}\n",
      "Input (16): BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.split. blk_7503483334202473044\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 2, \"cluster_size\": 2, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job 200811092030 <:*:> <:BLOCKID:>\", \"cluster_count\": 5}\n",
      "Input (17): Received block blk_-1608999687919862906 src: /10.251.215.16:52002 dest: /10.251.215.16:50010 of size 91178\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 6, \"cluster_size\": 1, \"template_mined\": \"Received block <:BLOCKID:> src: /10.251.215.16:52002 dest: /10.251.215.16:50010 of size 91178\", \"cluster_count\": 6}\n",
      "Input (22): Received block blk_7503483334202473044 of size 233217 from /10.251.215.16\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 4, \"cluster_size\": 4, \"template_mined\": \"Received block <:BLOCKID:> of size <:*:> from <:*:>\", \"cluster_count\": 6}\n",
      "Input (25): 10.250.14.224:50010:Transmitted block blk_-1608999687919862906 to /10.251.215.16:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 7, \"cluster_size\": 1, \"template_mined\": \"10.250.14.224:50010:Transmitted block <:BLOCKID:> to /10.251.215.16:50010\", \"cluster_count\": 7}\n",
      "Input (26): Received block blk_-1608999687919862906 src: /10.250.14.224:35754 dest: /10.250.14.224:50010 of size 91178\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 6, \"cluster_size\": 2, \"template_mined\": \"Received block <:BLOCKID:> src: <:*:> dest: <:*:> of size 91178\", \"cluster_count\": 7}\n",
      "Input (30): 10.250.14.224:50010 Starting thread to transfer block blk_-1608999687919862906 to 10.251.215.16:50010, 10.251.71.193:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 8, \"cluster_size\": 1, \"template_mined\": \"10.250.14.224:50010 Starting thread to transfer block <:BLOCKID:> to 10.251.215.16:50010, 10.251.71.193:50010\", \"cluster_count\": 8}\n",
      "Input (31): BLOCK* ask 10.250.14.224:50010 to replicate blk_-1608999687919862906 to datanode(s) 10.251.215.16:50010 10.251.71.193:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 9, \"cluster_size\": 1, \"template_mined\": \"BLOCK* ask 10.250.14.224:50010 to replicate <:BLOCKID:> to datanode(s) 10.251.215.16:50010 10.251.71.193:50010\", \"cluster_count\": 9}\n",
      "Input (32): BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.106.10:50010 is added to blk_7503483334202473044 size 233217\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 5, \"cluster_size\": 4, \"template_mined\": \"BLOCK* NameSystem.addStoredBlock: blockMap updated: <:*:> is added to <:BLOCKID:> size <:*:>\", \"cluster_count\": 9}\n",
      "Input (47): 10.250.11.100:50010 Served block blk_-3544583377289625738 to /10.250.19.102\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 10, \"cluster_size\": 1, \"template_mined\": \"10.250.11.100:50010 Served block <:BLOCKID:> to /10.250.19.102\", \"cluster_count\": 10}\n",
      "Input (48): 10.251.111.209:50010 Served block blk_-1608999687919862906 to /10.250.19.102\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 10, \"cluster_size\": 2, \"template_mined\": \"<:*:> Served block <:BLOCKID:> to /10.250.19.102\", \"cluster_count\": 10}\n",
      "Input (49): BLOCK* NameSystem.allocateBlock: /user/root/rand/_logs/history/ip-10-250-19-102.ec2.internal_1226291400491_job_200811092030_0001_conf.xml. blk_-9073992586687739851\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 11, \"cluster_size\": 1, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: /user/root/rand/ logs/history/ip-10-250-19-102.ec2.internal 1226291400491 job 200811092030 0001 conf.xml. <:BLOCKID:>\", \"cluster_count\": 11}\n",
      "Input (61): BLOCK* ask 10.251.215.16:50010 to replicate blk_-1608999687919862906 to datanode(s) 10.251.74.79:50010 10.251.107.19:50010\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 9, \"cluster_size\": 2, \"template_mined\": \"BLOCK* ask <:*:> to replicate <:BLOCKID:> to datanode(s) <:*:> <:*:>\", \"cluster_count\": 11}\n",
      "Input (70): 10.251.39.179:50010 Served block blk_-3544583377289625738 to /10.250.18.114\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 10, \"cluster_size\": 4, \"template_mined\": \"<:*:> Served block <:BLOCKID:> to <:*:>\", \"cluster_count\": 11}\n",
      "Input (73): 10.251.215.16:50010:Transmitted block blk_-1608999687919862906 to /10.251.74.79:50010\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 7, \"cluster_size\": 2, \"template_mined\": \"<:*:> block <:BLOCKID:> to <:*:>\", \"cluster_count\": 11}\n",
      "Input (74): 10.251.215.16:50010 Starting thread to transfer block blk_-1608999687919862906 to 10.251.74.79:50010, 10.251.107.19:50010\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 8, \"cluster_size\": 2, \"template_mined\": \"<:*:> Starting thread to transfer block <:BLOCKID:> to <:*:> <:*:>\", \"cluster_count\": 11}\n",
      "Input (201): BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811092030_0001_m_000079_0/part-00079. blk_7854771516489510256\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 12, \"cluster_size\": 1, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: /user/root/rand/ temporary/ task 200811092030 0001 m 000079 0/part-00079. <:BLOCKID:>\", \"cluster_count\": 12}\n",
      "Input (271): BLOCK* ask 10.251.31.5:50010 to replicate blk_-1608999687919862906 to datanode(s) 10.251.90.64:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 13, \"cluster_size\": 1, \"template_mined\": \"BLOCK* ask 10.251.31.5:50010 to replicate <:BLOCKID:> to datanode(s) 10.251.90.64:50010\", \"cluster_count\": 13}\n",
      "Input (272): BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811092030_0001_m_000111_0/part-00111. blk_1717858812220360316\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 12, \"cluster_size\": 2, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: /user/root/rand/ temporary/ task 200811092030 0001 m <:*:> <:*:> <:BLOCKID:>\", \"cluster_count\": 13}\n",
      "Input (393): 10.251.31.5:50010 Starting thread to transfer block blk_-1608999687919862906 to 10.251.90.64:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 14, \"cluster_size\": 1, \"template_mined\": \"10.251.31.5:50010 Starting thread to transfer block <:BLOCKID:> to 10.251.90.64:50010\", \"cluster_count\": 14}\n",
      "Input (2082): Verification succeeded for blk_-9073992586687739851\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 15, \"cluster_size\": 1, \"template_mined\": \"Verification succeeded for <:BLOCKID:>\", \"cluster_count\": 15}\n",
      "Input (6336): writeBlock blk_-3102267849859399193 received exception java.net.SocketTimeoutException\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 16, \"cluster_size\": 1, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.net.SocketTimeoutException\", \"cluster_count\": 16}\n",
      "Input (6493): PacketResponder blk_-3102267849859399193 2 Exception java.io.EOFException\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 17, \"cluster_size\": 1, \"template_mined\": \"PacketResponder <:BLOCKID:> 2 Exception java.io.EOFException\", \"cluster_count\": 17}\n",
      "Processing line: 10000, rate 63537.3 lines/sec, 17 clusters so far.\n",
      "Processing line: 20000, rate 83036.0 lines/sec, 17 clusters so far.\n",
      "Input (28755): writeBlock blk_7648436334209344608 received exception java.io.IOException: Could not read from stream\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 18, \"cluster_size\": 1, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.io.IOException: Could not read from stream\", \"cluster_count\": 18}\n",
      "Processing line: 30000, rate 82722.5 lines/sec, 18 clusters so far.\n",
      "Processing line: 40000, rate 82568.9 lines/sec, 18 clusters so far.\n",
      "Processing line: 50000, rate 84146.6 lines/sec, 18 clusters so far.\n",
      "Processing line: 60000, rate 83263.9 lines/sec, 18 clusters so far.\n",
      "Processing line: 70000, rate 79212.8 lines/sec, 18 clusters so far.\n",
      "Input (72217): BLOCK* ask 10.251.43.21:50010 to replicate blk_-8213344449220111733 to datanode(s) 10.251.30.134:50010\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 13, \"cluster_size\": 2, \"template_mined\": \"BLOCK* ask <:*:> to replicate <:BLOCKID:> to datanode(s) <:*:>\", \"cluster_count\": 18}\n",
      "Input (72372): 10.251.43.21:50010 Starting thread to transfer block blk_-8213344449220111733 to 10.251.30.134:50010\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 14, \"cluster_size\": 2, \"template_mined\": \"<:*:> Starting thread to transfer block <:BLOCKID:> to <:*:>\", \"cluster_count\": 18}\n",
      "Input (72539): Received block blk_-8213344449220111733 src: /10.251.43.21:44237 dest: /10.251.43.21:50010 of size 3553474\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 6, \"cluster_size\": 8, \"template_mined\": \"Received block <:BLOCKID:> src: <:*:> dest: <:*:> of size <:*:>\", \"cluster_count\": 18}\n",
      "Input (72694): Deleting block blk_-8213344449220111733 file /mnt/hadoop/dfs/data/current/subdir39/blk_-8213344449220111733\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 7, \"cluster_size\": 6, \"template_mined\": \"<:*:> block <:BLOCKID:> <:*:> <:*:>\", \"cluster_count\": 18}\n",
      "Input (73506): Receiving empty packet for block blk_-3842070622043972712\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 19, \"cluster_size\": 1, \"template_mined\": \"Receiving empty packet for block <:BLOCKID:>\", \"cluster_count\": 19}\n",
      "Input (79121): BLOCK* NameSystem.allocateBlock: /user/root/rand/_logs/history/ip-10-250-19-102.ec2.internal_1226291400491_job_200811092030_0001_root_random-writer. blk_-7007586470894854585\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 20, \"cluster_size\": 1, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: /user/root/rand/ logs/history/ip-10-250-19-102.ec2.internal 1226291400491 job 200811092030 0001 root random-writer. <:BLOCKID:>\", \"cluster_count\": 20}\n",
      "Processing line: 80000, rate 77581.9 lines/sec, 20 clusters so far.\n",
      "Processing line: 90000, rate 79363.6 lines/sec, 20 clusters so far.\n",
      "Input (91149): Exception in receiveBlock for block blk_-3102267849859399193 java.io.IOException: Connection reset by peer\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 21, \"cluster_size\": 1, \"template_mined\": \"Exception in receiveBlock for block <:BLOCKID:> java.io.IOException: Connection reset by peer\", \"cluster_count\": 21}\n",
      "Input (91150): writeBlock blk_-3102267849859399193 received exception java.io.IOException: Connection reset by peer\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 22, \"cluster_size\": 1, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.io.IOException: Connection reset by peer\", \"cluster_count\": 22}\n",
      "Processing line: 100000, rate 79093.3 lines/sec, 22 clusters so far.\n",
      "Input (105299): BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for blk_3888635850409849568 on 10.251.107.227:50010 size 67108864\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 23, \"cluster_size\": 1, \"template_mined\": \"BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for <:BLOCKID:> on 10.251.107.227:50010 size 67108864\", \"cluster_count\": 23}\n",
      "Processing line: 110000, rate 80101.3 lines/sec, 23 clusters so far.\n",
      "Input (118679): PacketResponder blk_4241467193520768333 2 Exception java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.251.107.242:58971 remote=/10.251.123.33:50010]\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 24, \"cluster_size\": 1, \"template_mined\": \"PacketResponder <:BLOCKID:> 2 Exception java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.251.107.242:58971 remote=/10.251.123.33:50010]\", \"cluster_count\": 24}\n",
      "Processing line: 120000, rate 79546.2 lines/sec, 24 clusters so far.\n",
      "Input (123967): PacketResponder blk_4241467193520768333 1 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.251.123.33:39066 remote=/10.250.7.32:50010]. 59942 millis timeout left.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 25, \"cluster_size\": 1, \"template_mined\": \"PacketResponder <:BLOCKID:> 1 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.251.123.33:39066 remote=/10.250.7.32:50010]. 59942 millis timeout left.\", \"cluster_count\": 25}\n",
      "Input (124080): Exception in receiveBlock for block blk_4241467193520768333 java.io.EOFException\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 26, \"cluster_size\": 1, \"template_mined\": \"Exception in receiveBlock for block <:BLOCKID:> java.io.EOFException\", \"cluster_count\": 26}\n",
      "Input (124081): writeBlock blk_4241467193520768333 received exception java.io.EOFException\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 16, \"cluster_size\": 2, \"template_mined\": \"writeBlock <:BLOCKID:> received exception <:*:>\", \"cluster_count\": 26}\n",
      "Input (124082): PacketResponder 0 for block blk_4241467193520768333 Interrupted.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 3, \"cluster_size\": 27684, \"template_mined\": \"PacketResponder <:*:> for block <:BLOCKID:> <:*:>\", \"cluster_count\": 26}\n",
      "Input (124116): Changing block file offset of block blk_4241467193520768333 from 0 to 30867456 meta file offset to 241159\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 27, \"cluster_size\": 1, \"template_mined\": \"Changing block file offset of block <:BLOCKID:> from 0 to 30867456 meta file offset to 241159\", \"cluster_count\": 27}\n",
      "Input (126473): BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for blk_-8692505341300910301 on 10.251.43.192:50010 size 67108864\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 23, \"cluster_size\": 2, \"template_mined\": \"BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for <:BLOCKID:> on <:*:> size 67108864\", \"cluster_count\": 27}\n",
      "Processing line: 130000, rate 76895.2 lines/sec, 27 clusters so far.\n",
      "Processing line: 140000, rate 80677.7 lines/sec, 27 clusters so far.\n",
      "Processing line: 150000, rate 80444.0 lines/sec, 27 clusters so far.\n",
      "Processing line: 160000, rate 79143.2 lines/sec, 27 clusters so far.\n",
      "Processing line: 170000, rate 76366.7 lines/sec, 27 clusters so far.\n",
      "Processing line: 180000, rate 77784.1 lines/sec, 27 clusters so far.\n",
      "Processing line: 190000, rate 80413.8 lines/sec, 27 clusters so far.\n",
      "Processing line: 200000, rate 80622.3 lines/sec, 27 clusters so far.\n",
      "Processing line: 210000, rate 81057.7 lines/sec, 27 clusters so far.\n",
      "Processing line: 220000, rate 81037.9 lines/sec, 27 clusters so far.\n",
      "Processing line: 230000, rate 81055.1 lines/sec, 27 clusters so far.\n",
      "Processing line: 240000, rate 80777.4 lines/sec, 27 clusters so far.\n",
      "Input (246878): PacketResponder blk_3858821904894294369 2 Exception java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.250.13.188:58401 remote=/10.251.39.160:50010]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 24, \"cluster_size\": 2, \"template_mined\": \"PacketResponder <:BLOCKID:> 2 Exception java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected <:*:> <:*:>\", \"cluster_count\": 27}\n",
      "Input (248163): PacketResponder blk_3858821904894294369 1 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.251.39.160:57395 remote=/10.251.214.175:50010]. 59728 millis timeout left.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 25, \"cluster_size\": 2, \"template_mined\": \"PacketResponder <:BLOCKID:> 1 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected <:*:> <:*:> <:*:> millis timeout left.\", \"cluster_count\": 27}\n",
      "Input (248164): 10.250.13.188:50010:Exception writing block blk_3858821904894294369 to mirror 10.251.39.160:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 28, \"cluster_size\": 1, \"template_mined\": \"10.250.13.188:50010:Exception writing block <:BLOCKID:> to mirror 10.251.39.160:50010\", \"cluster_count\": 28}\n",
      "Input (248165): Exception in receiveBlock for block blk_3858821904894294369 java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.250.13.188:58401 remote=/10.251.39.160:50010]. 489996 millis timeout left.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 29, \"cluster_size\": 1, \"template_mined\": \"Exception in receiveBlock for block <:BLOCKID:> java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.250.13.188:58401 remote=/10.251.39.160:50010]. 489996 millis timeout left.\", \"cluster_count\": 29}\n",
      "Input (248166): writeBlock blk_3858821904894294369 received exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.250.13.188:58401 remote=/10.251.39.160:50010]. 489996 millis timeout left.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 30, \"cluster_size\": 1, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.250.13.188:58401 remote=/10.251.39.160:50010]. 489996 millis timeout left.\", \"cluster_count\": 30}\n",
      "Input (248325): PacketResponder blk_3858821904894294369 0 Exception java.io.IOException: Broken pipe\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 31, \"cluster_size\": 1, \"template_mined\": \"PacketResponder <:BLOCKID:> 0 Exception java.io.IOException: Broken pipe\", \"cluster_count\": 31}\n",
      "Input (248364): Changing block file offset of block blk_3858821904894294369 from 0 to 23724032 meta file offset to 185351\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 27, \"cluster_size\": 3, \"template_mined\": \"Changing block file offset of block <:BLOCKID:> from 0 to <:*:> meta file offset to <:*:>\", \"cluster_count\": 31}\n",
      "Processing line: 250000, rate 75161.5 lines/sec, 31 clusters so far.\n",
      "Processing line: 260000, rate 76975.7 lines/sec, 31 clusters so far.\n",
      "Processing line: 270000, rate 80961.7 lines/sec, 31 clusters so far.\n",
      "Processing line: 280000, rate 81555.4 lines/sec, 31 clusters so far.\n",
      "Input (282885): writeBlock blk_992101295951175683 received exception java.io.IOException: Block blk_992101295951175683 is valid, and cannot be written to.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 32, \"cluster_size\": 1, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.io.IOException: Block <:BLOCKID:> is valid, and cannot be written to.\", \"cluster_count\": 32}\n",
      "Input (282919): 10.251.194.147:50010:Failed to transfer blk_992101295951175683 to 10.251.214.112:50010 got java.io.IOException: Connection reset by peer\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 33, \"cluster_size\": 1, \"template_mined\": \"10.251.194.147:50010:Failed to transfer <:BLOCKID:> to 10.251.214.112:50010 got java.io.IOException: Connection reset by peer\", \"cluster_count\": 33}\n",
      "Processing line: 290000, rate 80305.9 lines/sec, 33 clusters so far.\n",
      "Processing line: 300000, rate 81339.2 lines/sec, 33 clusters so far.\n",
      "Processing line: 310000, rate 81350.4 lines/sec, 33 clusters so far.\n",
      "Processing line: 320000, rate 81470.3 lines/sec, 33 clusters so far.\n",
      "Processing line: 330000, rate 81559.4 lines/sec, 33 clusters so far.\n",
      "Input (331835): 10.251.39.242:50010:Failed to transfer blk_5862064168004546076 to 10.251.35.1:50010 got java.io.IOException: Connection reset by peer\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 33, \"cluster_size\": 2, \"template_mined\": \"<:*:> to transfer <:BLOCKID:> to <:*:> got java.io.IOException: Connection reset by peer\", \"cluster_count\": 33}\n",
      "Processing line: 340000, rate 79922.0 lines/sec, 33 clusters so far.\n",
      "Processing line: 350000, rate 81511.6 lines/sec, 33 clusters so far.\n",
      "Processing line: 360000, rate 81748.0 lines/sec, 33 clusters so far.\n",
      "Processing line: 370000, rate 82097.7 lines/sec, 33 clusters so far.\n",
      "Processing line: 380000, rate 81685.3 lines/sec, 33 clusters so far.\n",
      "Processing line: 390000, rate 82045.8 lines/sec, 33 clusters so far.\n",
      "Processing line: 400000, rate 82070.8 lines/sec, 33 clusters so far.\n",
      "Processing line: 410000, rate 82245.0 lines/sec, 33 clusters so far.\n",
      "Processing line: 420000, rate 82101.8 lines/sec, 33 clusters so far.\n",
      "Processing line: 430000, rate 82176.8 lines/sec, 33 clusters so far.\n",
      "Processing line: 440000, rate 81480.9 lines/sec, 33 clusters so far.\n",
      "Input (444883): BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for blk_-3174232733041654340 on 10.251.106.10:50010 size 44607007\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 23, \"cluster_size\": 28, \"template_mined\": \"BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for <:BLOCKID:> on <:*:> size <:*:>\", \"cluster_count\": 33}\n",
      "Processing line: 450000, rate 77467.7 lines/sec, 33 clusters so far.\n",
      "Processing line: 460000, rate 76819.0 lines/sec, 33 clusters so far.\n",
      "Processing line: 470000, rate 77592.8 lines/sec, 33 clusters so far.\n",
      "Input (472837): BLOCK* NameSystem.delete: blk_-1608999687919862906 is added to invalidSet of 10.250.10.6:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 34, \"cluster_size\": 1, \"template_mined\": \"BLOCK* NameSystem.delete: <:BLOCKID:> is added to invalidSet of 10.250.10.6:50010\", \"cluster_count\": 34}\n",
      "Input (472838): BLOCK* NameSystem.delete: blk_-1608999687919862906 is added to invalidSet of 10.250.14.224:50010\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 34, \"cluster_size\": 2, \"template_mined\": \"BLOCK* NameSystem.delete: <:BLOCKID:> is added to invalidSet of <:*:>\", \"cluster_count\": 34}\n",
      "Input (474740): Unexpected error trying to delete block blk_-1067131609371010449. BlockInfo not found in volumeMap.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 35, \"cluster_size\": 1, \"template_mined\": \"Unexpected error trying to delete block <:BLOCKID:>. BlockInfo not found in volumeMap.\", \"cluster_count\": 35}\n",
      "Input (478206): BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for blk_5398314277015661293 on 10.251.199.86:50010 size 67108864 But it does not belong to any file.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 36, \"cluster_size\": 1, \"template_mined\": \"BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for <:BLOCKID:> on 10.251.199.86:50010 size 67108864 But it does not belong to any file.\", \"cluster_count\": 36}\n",
      "Input (478208): BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for blk_5398314277015661293 on 10.251.122.79:50010 size 67108864 But it does not belong to any file.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 36, \"cluster_size\": 2, \"template_mined\": \"BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for <:BLOCKID:> on <:*:> size 67108864 But it does not belong to any file.\", \"cluster_count\": 36}\n",
      "Input (478315): BLOCK* NameSystem.allocateBlock: /user/root/sortrand/_logs/history/ip-10-250-19-102.ec2.internal_1226291400491_job_200811092030_0002_conf.xml. blk_-4615135864434101729\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 11, \"cluster_size\": 2, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: <:*:> logs/history/ip-10-250-19-102.ec2.internal 1226291400491 job 200811092030 <:*:> conf.xml. <:BLOCKID:>\", \"cluster_count\": 36}\n",
      "Processing line: 480000, rate 78740.1 lines/sec, 36 clusters so far.\n",
      "Input (481408): 10.251.71.68:50010:Got exception while serving blk_-8781759536960110370 to /10.250.17.225:\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 37, \"cluster_size\": 1, \"template_mined\": \"10.251.71.68:50010:Got exception while serving <:BLOCKID:> to /10.250.17.225:\", \"cluster_count\": 37}\n",
      "Input (481409): 10.251.27.63:50010:Got exception while serving blk_1732876454483854279 to /10.251.27.63:\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 37, \"cluster_size\": 2, \"template_mined\": \"<:*:> exception while serving <:BLOCKID:> to <:*:>\", \"cluster_count\": 37}\n",
      "Input (482608): BLOCK* NameSystem.allocateBlock: /user/root/sortrand/_logs/history/ip-10-250-19-102.ec2.internal_1226291400491_job_200811092030_0002_root_sorter. blk_-8950178633263590328\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 20, \"cluster_size\": 2, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: <:*:> logs/history/ip-10-250-19-102.ec2.internal 1226291400491 job 200811092030 <:*:> root <:*:> <:BLOCKID:>\", \"cluster_count\": 37}\n",
      "Processing line: 490000, rate 82719.9 lines/sec, 37 clusters so far.\n",
      "Processing line: 500000, rate 83789.2 lines/sec, 37 clusters so far.\n",
      "Processing line: 510000, rate 80159.2 lines/sec, 37 clusters so far.\n",
      "Processing line: 520000, rate 83946.7 lines/sec, 37 clusters so far.\n",
      "Processing line: 530000, rate 84184.3 lines/sec, 37 clusters so far.\n",
      "Processing line: 540000, rate 84582.4 lines/sec, 37 clusters so far.\n",
      "Processing line: 550000, rate 83283.9 lines/sec, 37 clusters so far.\n",
      "Processing line: 560000, rate 83180.9 lines/sec, 37 clusters so far.\n",
      "Processing line: 570000, rate 84269.3 lines/sec, 37 clusters so far.\n",
      "Processing line: 580000, rate 83309.1 lines/sec, 37 clusters so far.\n",
      "Processing line: 590000, rate 84629.5 lines/sec, 37 clusters so far.\n",
      "Processing line: 600000, rate 84653.7 lines/sec, 37 clusters so far.\n",
      "Processing line: 610000, rate 84417.9 lines/sec, 37 clusters so far.\n",
      "Input (615247): BLOCK* NameSystem.allocateBlock: /user/root/sortrand/_temporary/_task_200811092030_0002_r_000174_0/part-00174. blk_6048716917685366339\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 12, \"cluster_size\": 36296, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: <:*:> temporary/ task 200811092030 <:*:> <:*:> <:*:> <:*:> <:BLOCKID:>\", \"cluster_count\": 37}\n",
      "Processing line: 620000, rate 83958.8 lines/sec, 37 clusters so far.\n",
      "Processing line: 630000, rate 79391.6 lines/sec, 37 clusters so far.\n",
      "Processing line: 640000, rate 80256.7 lines/sec, 37 clusters so far.\n",
      "Processing line: 650000, rate 79900.2 lines/sec, 37 clusters so far.\n",
      "Processing line: 660000, rate 78255.9 lines/sec, 37 clusters so far.\n",
      "Processing line: 670000, rate 80116.0 lines/sec, 37 clusters so far.\n",
      "Processing line: 680000, rate 80715.0 lines/sec, 37 clusters so far.\n",
      "Processing line: 690000, rate 79377.0 lines/sec, 37 clusters so far.\n",
      "Processing line: 700000, rate 78676.4 lines/sec, 37 clusters so far.\n",
      "Processing line: 710000, rate 80789.9 lines/sec, 37 clusters so far.\n",
      "Processing line: 720000, rate 81015.7 lines/sec, 37 clusters so far.\n",
      "Processing line: 730000, rate 81286.4 lines/sec, 37 clusters so far.\n",
      "Processing line: 740000, rate 80010.2 lines/sec, 37 clusters so far.\n",
      "Processing line: 750000, rate 80682.8 lines/sec, 37 clusters so far.\n",
      "Processing line: 760000, rate 81101.1 lines/sec, 37 clusters so far.\n",
      "Processing line: 770000, rate 80519.2 lines/sec, 37 clusters so far.\n",
      "Processing line: 780000, rate 68148.6 lines/sec, 37 clusters so far.\n",
      "Processing line: 790000, rate 61572.6 lines/sec, 37 clusters so far.\n",
      "Input (791057): PacketResponder blk_-4567777441263358151 1 Exception java.io.IOException: Connection reset by peer\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 38, \"cluster_size\": 1, \"template_mined\": \"PacketResponder <:BLOCKID:> 1 Exception java.io.IOException: Connection reset by peer\", \"cluster_count\": 38}\n",
      "Input (791061): 10.251.199.225:50010:Exception writing block blk_-4567777441263358151 to mirror 10.251.107.227:50010\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 28, \"cluster_size\": 2, \"template_mined\": \"<:*:> writing block <:BLOCKID:> to mirror <:*:>\", \"cluster_count\": 38}\n",
      "Input (791062): Exception in receiveBlock for block blk_-4567777441263358151 java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.251.199.225:50760 remote=/10.251.107.227:50010]. 489959 millis timeout left.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 29, \"cluster_size\": 2, \"template_mined\": \"Exception in receiveBlock for block <:BLOCKID:> java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected <:*:> <:*:> <:*:> millis timeout left.\", \"cluster_count\": 38}\n",
      "Input (791063): writeBlock blk_-4567777441263358151 received exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.251.199.225:50760 remote=/10.251.107.227:50010]. 489959 millis timeout left.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 30, \"cluster_size\": 2, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected <:*:> <:*:> <:*:> millis timeout left.\", \"cluster_count\": 38}\n",
      "Processing line: 800000, rate 73700.7 lines/sec, 38 clusters so far.\n",
      "Processing line: 810000, rate 80435.7 lines/sec, 38 clusters so far.\n",
      "Processing line: 820000, rate 80355.0 lines/sec, 38 clusters so far.\n",
      "Processing line: 830000, rate 82265.1 lines/sec, 38 clusters so far.\n",
      "Processing line: 840000, rate 80537.8 lines/sec, 38 clusters so far.\n",
      "Processing line: 850000, rate 80242.6 lines/sec, 38 clusters so far.\n",
      "Processing line: 860000, rate 82804.8 lines/sec, 38 clusters so far.\n",
      "Processing line: 870000, rate 83303.5 lines/sec, 38 clusters so far.\n",
      "Processing line: 880000, rate 82786.8 lines/sec, 38 clusters so far.\n",
      "Processing line: 890000, rate 83245.9 lines/sec, 38 clusters so far.\n",
      "Processing line: 900000, rate 83066.2 lines/sec, 38 clusters so far.\n",
      "Processing line: 910000, rate 83121.4 lines/sec, 38 clusters so far.\n",
      "Processing line: 920000, rate 83255.0 lines/sec, 38 clusters so far.\n",
      "Processing line: 930000, rate 83009.7 lines/sec, 38 clusters so far.\n",
      "Processing line: 940000, rate 82882.4 lines/sec, 38 clusters so far.\n",
      "Processing line: 950000, rate 82722.5 lines/sec, 38 clusters so far.\n",
      "Processing line: 960000, rate 82787.0 lines/sec, 38 clusters so far.\n",
      "Processing line: 970000, rate 83200.2 lines/sec, 38 clusters so far.\n",
      "Processing line: 980000, rate 82661.7 lines/sec, 38 clusters so far.\n",
      "Processing line: 990000, rate 82596.1 lines/sec, 38 clusters so far.\n",
      "Processing line: 1000000, rate 83324.3 lines/sec, 38 clusters so far.\n",
      "Processing line: 1010000, rate 83405.0 lines/sec, 38 clusters so far.\n",
      "Processing line: 1020000, rate 79857.6 lines/sec, 38 clusters so far.\n",
      "Processing line: 1030000, rate 83090.2 lines/sec, 38 clusters so far.\n",
      "Processing line: 1040000, rate 83927.9 lines/sec, 38 clusters so far.\n",
      "Processing line: 1050000, rate 83606.7 lines/sec, 38 clusters so far.\n",
      "Processing line: 1060000, rate 85920.3 lines/sec, 38 clusters so far.\n",
      "Processing line: 1070000, rate 83884.2 lines/sec, 38 clusters so far.\n",
      "Processing line: 1080000, rate 83302.1 lines/sec, 38 clusters so far.\n",
      "Processing line: 1090000, rate 82805.5 lines/sec, 38 clusters so far.\n",
      "Processing line: 1100000, rate 82711.7 lines/sec, 38 clusters so far.\n",
      "Processing line: 1110000, rate 82889.9 lines/sec, 38 clusters so far.\n",
      "Processing line: 1120000, rate 83204.3 lines/sec, 38 clusters so far.\n",
      "Processing line: 1130000, rate 83155.8 lines/sec, 38 clusters so far.\n",
      "Processing line: 1140000, rate 83060.0 lines/sec, 38 clusters so far.\n",
      "Processing line: 1150000, rate 83054.4 lines/sec, 38 clusters so far.\n",
      "Processing line: 1160000, rate 82929.6 lines/sec, 38 clusters so far.\n",
      "Processing line: 1170000, rate 83044.2 lines/sec, 38 clusters so far.\n",
      "Processing line: 1180000, rate 82780.8 lines/sec, 38 clusters so far.\n",
      "Processing line: 1190000, rate 83190.4 lines/sec, 38 clusters so far.\n",
      "Processing line: 1200000, rate 83247.2 lines/sec, 38 clusters so far.\n",
      "Processing line: 1210000, rate 82466.0 lines/sec, 38 clusters so far.\n",
      "Processing line: 1220000, rate 83353.3 lines/sec, 38 clusters so far.\n",
      "Processing line: 1230000, rate 83235.7 lines/sec, 38 clusters so far.\n",
      "Processing line: 1240000, rate 83114.4 lines/sec, 38 clusters so far.\n",
      "Processing line: 1250000, rate 83070.3 lines/sec, 38 clusters so far.\n",
      "Processing line: 1260000, rate 83454.3 lines/sec, 38 clusters so far.\n",
      "Processing line: 1270000, rate 83011.5 lines/sec, 38 clusters so far.\n",
      "Processing line: 1280000, rate 83186.3 lines/sec, 38 clusters so far.\n",
      "Processing line: 1290000, rate 83315.2 lines/sec, 38 clusters so far.\n",
      "Processing line: 1300000, rate 83370.8 lines/sec, 38 clusters so far.\n",
      "Processing line: 1310000, rate 83136.0 lines/sec, 38 clusters so far.\n",
      "Processing line: 1320000, rate 83151.2 lines/sec, 38 clusters so far.\n",
      "Processing line: 1330000, rate 83248.7 lines/sec, 38 clusters so far.\n",
      "Processing line: 1340000, rate 83122.5 lines/sec, 38 clusters so far.\n",
      "Processing line: 1350000, rate 83034.5 lines/sec, 38 clusters so far.\n",
      "Processing line: 1360000, rate 82817.7 lines/sec, 38 clusters so far.\n",
      "Processing line: 1370000, rate 82968.1 lines/sec, 38 clusters so far.\n",
      "Input (1372949): Exception in receiveBlock for block blk_7008279672769077211 java.net.SocketTimeoutException: 490000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.251.42.191:43873 remote=/10.251.123.33:50010]\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 39, \"cluster_size\": 1, \"template_mined\": \"Exception in receiveBlock for block <:BLOCKID:> java.net.SocketTimeoutException: 490000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.251.42.191:43873 remote=/10.251.123.33:50010]\", \"cluster_count\": 39}\n",
      "Input (1373103): writeBlock blk_7008279672769077211 received exception java.net.SocketTimeoutException: 490000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.251.42.191:43873 remote=/10.251.123.33:50010]\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 40, \"cluster_size\": 1, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.net.SocketTimeoutException: 490000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.251.42.191:43873 remote=/10.251.123.33:50010]\", \"cluster_count\": 40}\n",
      "Input (1373106): PacketResponder blk_7008279672769077211 2 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.251.42.191:43873 remote=/10.251.123.33:50010]. 115663 millis timeout left.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 25, \"cluster_size\": 3, \"template_mined\": \"PacketResponder <:BLOCKID:> <:*:> Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected <:*:> <:*:> <:*:> millis timeout left.\", \"cluster_count\": 40}\n",
      "Processing line: 1380000, rate 81045.9 lines/sec, 40 clusters so far.\n",
      "Input (1389320): Exception in receiveBlock for block blk_7008279672769077211 java.nio.channels.ClosedByInterruptException\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 26, \"cluster_size\": 4, \"template_mined\": \"Exception in receiveBlock for block <:BLOCKID:> <:*:>\", \"cluster_count\": 40}\n",
      "Input (1389402): Reopen Block blk_7008279672769077211\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 41, \"cluster_size\": 1, \"template_mined\": \"Reopen Block <:BLOCKID:>\", \"cluster_count\": 41}\n",
      "Processing line: 1390000, rate 81030.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 1400000, rate 82480.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 1410000, rate 82996.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 1420000, rate 83476.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 1430000, rate 83035.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 1440000, rate 83198.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 1450000, rate 83041.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 1460000, rate 83284.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 1470000, rate 82601.0 lines/sec, 41 clusters so far.\n",
      "Processing line: 1480000, rate 81838.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 1490000, rate 82820.0 lines/sec, 41 clusters so far.\n",
      "Input (1496648): PacketResponder blk_8085135783040518166 1 Exception java.nio.channels.ClosedByInterruptException\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 17, \"cluster_size\": 3, \"template_mined\": \"PacketResponder <:BLOCKID:> <:*:> Exception <:*:>\", \"cluster_count\": 41}\n",
      "Processing line: 1500000, rate 82487.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 1510000, rate 82898.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 1520000, rate 82623.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 1530000, rate 83042.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 1540000, rate 83085.0 lines/sec, 41 clusters so far.\n",
      "Processing line: 1550000, rate 89654.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 1560000, rate 87966.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 1570000, rate 91991.0 lines/sec, 41 clusters so far.\n",
      "Processing line: 1580000, rate 92413.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 1590000, rate 92320.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 1600000, rate 92113.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 1610000, rate 92336.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 1620000, rate 91982.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 1630000, rate 92151.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 1640000, rate 92215.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 1650000, rate 91730.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 1660000, rate 91570.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 1670000, rate 88251.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 1680000, rate 89640.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 1690000, rate 90135.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 1700000, rate 90628.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 1710000, rate 90281.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 1720000, rate 90583.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 1730000, rate 90656.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 1740000, rate 90620.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 1750000, rate 90372.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 1760000, rate 90317.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 1770000, rate 90685.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 1780000, rate 96077.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 1790000, rate 90507.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 1800000, rate 90766.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 1810000, rate 90548.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 1820000, rate 90261.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 1830000, rate 89769.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 1840000, rate 90236.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 1850000, rate 90176.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 1860000, rate 90362.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 1870000, rate 90205.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 1880000, rate 93674.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 1890000, rate 91448.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 1900000, rate 90598.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 1910000, rate 90697.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 1920000, rate 90652.0 lines/sec, 41 clusters so far.\n",
      "Processing line: 1930000, rate 90784.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 1940000, rate 87631.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 1950000, rate 79680.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 1960000, rate 78761.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 1970000, rate 78520.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 1980000, rate 79626.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 1990000, rate 80055.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 2000000, rate 80352.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 2010000, rate 79785.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 2020000, rate 79960.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 2030000, rate 80009.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 2040000, rate 80101.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 2050000, rate 80006.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 2060000, rate 80033.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 2070000, rate 80512.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 2080000, rate 80058.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 2090000, rate 80565.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 2100000, rate 80645.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 2110000, rate 80778.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2120000, rate 80206.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 2130000, rate 80667.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 2140000, rate 80340.0 lines/sec, 41 clusters so far.\n",
      "Processing line: 2150000, rate 80283.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 2160000, rate 80234.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 2170000, rate 79846.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 2180000, rate 80156.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 2190000, rate 80275.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 2200000, rate 80128.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 2210000, rate 80668.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2220000, rate 80720.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 2230000, rate 80416.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 2240000, rate 79655.1 lines/sec, 41 clusters so far.\n",
      "Input (2247681): BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811101024_0001/job.jar. blk_-3907603481176056256\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 2, \"cluster_size\": 28, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job <:*:> <:*:> <:BLOCKID:>\", \"cluster_count\": 41}\n",
      "Input (2247854): BLOCK* NameSystem.allocateBlock: /user/root/rand/_logs/history/ip-10-250-19-102.ec2.internal_1226341485991_job_200811101024_0001_conf.xml. blk_8305330044595670869\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 11, \"cluster_size\": 10, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: <:*:> logs/history/ip-10-250-19-102.ec2.internal <:*:> job <:*:> <:*:> conf.xml. <:BLOCKID:>\", \"cluster_count\": 41}\n",
      "Input (2248504): BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811101024_0001_m_000000_0/part-00000. blk_6199125144269351162\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 12, \"cluster_size\": 108003, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: <:*:> temporary/ task <:*:> <:*:> <:*:> <:*:> <:*:> <:BLOCKID:>\", \"cluster_count\": 41}\n",
      "Processing line: 2250000, rate 80159.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 2260000, rate 86856.0 lines/sec, 41 clusters so far.\n",
      "Processing line: 2270000, rate 89615.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 2280000, rate 91030.7 lines/sec, 41 clusters so far.\n",
      "total          : took    24.51 s (100.00%),  2,284,176 samples,   10.73 ms / 1000 samples,       93,183.15 hz\n",
      "drain          : took    13.51 s ( 55.11%),  2,284,176 samples,    5.91 ms / 1000 samples,      169,098.97 hz\n",
      "mask           : took     6.09 s ( 24.86%),  2,284,176 samples,    2.67 ms / 1000 samples,      374,894.93 hz\n",
      "tree_search    : took     5.89 s ( 24.02%),  2,284,176 samples,    2.58 ms / 1000 samples,      387,880.20 hz\n",
      "cluster_exist  : took     3.13 s ( 12.77%),  2,284,135 samples,    1.37 ms / 1000 samples,      729,716.10 hz\n",
      "create_cluster : took     0.00 s (  0.00%),         41 samples,   15.07 ms / 1000 samples,       66,370.69 hz\n",
      "Processing line: 2290000, rate 91118.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2300000, rate 92109.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 2310000, rate 92852.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 2320000, rate 92784.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 2330000, rate 92365.0 lines/sec, 41 clusters so far.\n",
      "Processing line: 2340000, rate 92097.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 2350000, rate 92384.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2360000, rate 92822.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 2370000, rate 92531.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2380000, rate 90510.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 2390000, rate 91448.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 2400000, rate 92290.0 lines/sec, 41 clusters so far.\n",
      "Processing line: 2410000, rate 92833.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 2420000, rate 92424.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 2430000, rate 92353.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 2440000, rate 91996.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 2450000, rate 92487.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 2460000, rate 92289.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 2470000, rate 92039.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 2480000, rate 92424.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 2490000, rate 92332.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2500000, rate 92474.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 2510000, rate 92161.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 2520000, rate 91819.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2530000, rate 90828.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 2540000, rate 91293.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 2550000, rate 89183.0 lines/sec, 41 clusters so far.\n",
      "Processing line: 2560000, rate 88862.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2570000, rate 87496.0 lines/sec, 41 clusters so far.\n",
      "Processing line: 2580000, rate 86210.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 2590000, rate 84088.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2600000, rate 83591.0 lines/sec, 41 clusters so far.\n",
      "Processing line: 2610000, rate 83643.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2620000, rate 83408.5 lines/sec, 41 clusters so far.\n",
      "Input (2620400): BLOCK* Removing block blk_-3530301067936445915 from neededReplications as it does not belong to any file.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 42, \"cluster_size\": 1, \"template_mined\": \"BLOCK* Removing block <:BLOCKID:> from neededReplications as it does not belong to any file.\", \"cluster_count\": 42}\n",
      "Input (2621297): BLOCK* NameSystem.allocateBlock: /user/root/rand/_logs/history/ip-10-250-19-102.ec2.internal_1226341485991_job_200811101024_0001_root_random-writer. blk_-6652811131921977095\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 20, \"cluster_size\": 10, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: <:*:> logs/history/ip-10-250-19-102.ec2.internal <:*:> job <:*:> <:*:> root <:*:> <:BLOCKID:>\", \"cluster_count\": 42}\n",
      "Processing line: 2630000, rate 81429.8 lines/sec, 42 clusters so far.\n",
      "Input (2635992): BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for blk_-4109536386405623117 on 10.251.106.10:50010 size 3528482 But it does not belong to any file.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 36, \"cluster_size\": 24, \"template_mined\": \"BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for <:BLOCKID:> on <:*:> size <:*:> But it does not belong to any file.\", \"cluster_count\": 42}\n",
      "Processing line: 2640000, rate 77871.3 lines/sec, 42 clusters so far.\n",
      "Processing line: 2650000, rate 83195.4 lines/sec, 42 clusters so far.\n",
      "Input (2654215): PacketResponder blk_8516616325149469651 2 Exception java.io.IOException: Broken pipe\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 31, \"cluster_size\": 4, \"template_mined\": \"PacketResponder <:BLOCKID:> <:*:> Exception java.io.IOException: Broken pipe\", \"cluster_count\": 42}\n",
      "Processing line: 2660000, rate 82484.5 lines/sec, 42 clusters so far.\n",
      "Input (2663876): PendingReplicationMonitor timed out block blk_-5057834626410636236\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 43, \"cluster_size\": 1, \"template_mined\": \"PendingReplicationMonitor timed out block <:BLOCKID:>\", \"cluster_count\": 43}\n",
      "Processing line: 2670000, rate 82597.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 2680000, rate 82509.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 2690000, rate 83280.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 2700000, rate 83311.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 2710000, rate 81120.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 2720000, rate 83169.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 2730000, rate 83222.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 2740000, rate 83031.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 2750000, rate 83267.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 2760000, rate 83358.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 2770000, rate 82979.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 2780000, rate 83382.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 2790000, rate 83482.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 2800000, rate 83109.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 2810000, rate 82961.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 2820000, rate 83153.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 2830000, rate 83288.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 2840000, rate 83401.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 2850000, rate 81863.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 2860000, rate 82446.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 2870000, rate 83290.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 2880000, rate 81356.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 2890000, rate 81774.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 2900000, rate 82694.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 2910000, rate 83078.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 2920000, rate 83128.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 2930000, rate 81117.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 2940000, rate 81699.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 2950000, rate 83313.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 2960000, rate 83178.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 2970000, rate 82763.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 2980000, rate 82710.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 2990000, rate 82913.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3000000, rate 83065.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3010000, rate 83360.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3020000, rate 83411.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 3030000, rate 82655.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3040000, rate 86476.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 3050000, rate 83202.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3060000, rate 79757.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 3070000, rate 82995.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 3080000, rate 82818.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3090000, rate 82741.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 3100000, rate 82403.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 3110000, rate 83280.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 3120000, rate 81505.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 3130000, rate 83196.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 3140000, rate 82563.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3150000, rate 83086.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 3160000, rate 83305.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 3170000, rate 83256.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3180000, rate 80459.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 3190000, rate 82695.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3200000, rate 83379.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 3210000, rate 83242.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 3220000, rate 82502.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3230000, rate 83130.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 3240000, rate 83340.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 3250000, rate 83060.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 3260000, rate 83106.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 3270000, rate 82829.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3280000, rate 83104.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 3290000, rate 81114.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 3300000, rate 82209.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 3310000, rate 80397.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 3320000, rate 83097.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3330000, rate 83446.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 3340000, rate 83117.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 3350000, rate 82986.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3360000, rate 83061.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3370000, rate 82736.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3380000, rate 82151.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3390000, rate 82383.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3400000, rate 82444.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 3410000, rate 82133.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3420000, rate 82556.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 3430000, rate 82771.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3440000, rate 82596.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 3450000, rate 82752.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3460000, rate 82186.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 3470000, rate 82568.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 3480000, rate 82676.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3490000, rate 82004.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3500000, rate 82741.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 3510000, rate 82152.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3520000, rate 88073.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3530000, rate 86164.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 3540000, rate 85850.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3550000, rate 85879.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 3560000, rate 86111.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 3570000, rate 86091.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 3580000, rate 86233.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3590000, rate 85733.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 3600000, rate 85351.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 3610000, rate 85914.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3620000, rate 85825.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 3630000, rate 85915.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 3640000, rate 85669.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 3650000, rate 88743.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3660000, rate 82780.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 3670000, rate 82592.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 3680000, rate 82907.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 3690000, rate 82584.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 3700000, rate 82910.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3710000, rate 82650.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 3720000, rate 82748.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3730000, rate 80158.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3740000, rate 82651.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3750000, rate 82665.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 3760000, rate 82889.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3770000, rate 82745.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3780000, rate 82601.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 3790000, rate 82862.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3800000, rate 82535.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3810000, rate 82215.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 3820000, rate 82639.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3830000, rate 82586.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3840000, rate 81962.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3850000, rate 82360.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 3860000, rate 82637.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 3870000, rate 83132.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3880000, rate 83341.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 3890000, rate 82860.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3900000, rate 83065.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3910000, rate 82952.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3920000, rate 83437.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3930000, rate 82345.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3940000, rate 83368.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3950000, rate 83022.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3960000, rate 83144.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 3970000, rate 82549.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 3980000, rate 83340.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3990000, rate 83180.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 4000000, rate 83309.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 4010000, rate 83393.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 4020000, rate 83229.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 4030000, rate 82581.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 4040000, rate 82713.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 4050000, rate 83052.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 4060000, rate 82987.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 4070000, rate 84083.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 4080000, rate 85575.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 4090000, rate 90910.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4100000, rate 92409.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 4110000, rate 92008.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 4120000, rate 92421.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 4130000, rate 92435.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4140000, rate 92345.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 4150000, rate 92337.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 4160000, rate 92439.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 4170000, rate 92076.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 4180000, rate 91597.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 4190000, rate 89151.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 4200000, rate 91760.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 4210000, rate 87492.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 4220000, rate 81614.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 4230000, rate 79988.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 4240000, rate 80159.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4250000, rate 79650.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 4260000, rate 79852.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 4270000, rate 79741.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 4280000, rate 79760.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 4290000, rate 79716.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 4300000, rate 79478.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 4310000, rate 79316.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 4320000, rate 79706.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 4330000, rate 80087.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4340000, rate 79825.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 4350000, rate 80174.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 4360000, rate 80323.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 4370000, rate 80080.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4380000, rate 80247.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 4390000, rate 76780.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 4400000, rate 79696.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 4410000, rate 79590.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 4420000, rate 82297.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 4430000, rate 88642.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 4440000, rate 90379.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 4450000, rate 91762.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 4460000, rate 91160.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 4470000, rate 91371.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4480000, rate 91743.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 4490000, rate 92090.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 4500000, rate 92070.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 4510000, rate 92038.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 4520000, rate 92011.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 4530000, rate 86407.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4540000, rate 73141.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 4550000, rate 76995.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 4560000, rate 88844.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 4570000, rate 89661.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 4580000, rate 90712.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4590000, rate 90480.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 4600000, rate 89789.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 4610000, rate 89384.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 4620000, rate 87398.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4630000, rate 83892.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 4640000, rate 84552.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 4650000, rate 82516.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 4660000, rate 82021.1 lines/sec, 43 clusters so far.\n",
      "Input (4663237): writeBlock blk_-516029862489211143 received exception java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.251.197.161:44245 remote=/10.251.123.33:50010]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 40, \"cluster_size\": 2, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.net.SocketTimeoutException: <:*:> millis timeout while waiting for channel to be ready for <:*:> ch : java.nio.channels.SocketChannel[connected <:*:> remote=/10.251.123.33:50010]\", \"cluster_count\": 43}\n",
      "Processing line: 4670000, rate 79631.1 lines/sec, 43 clusters so far.\n",
      "Input (4673664): writeBlock blk_-4250706752040073149 received exception java.io.IOException: Interrupted receiveBlock\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 44, \"cluster_size\": 1, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.io.IOException: Interrupted receiveBlock\", \"cluster_count\": 44}\n",
      "Input (4673666): PacketResponder blk_-4250706752040073149 1 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 59985 millis timeout left.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 45, \"cluster_size\": 1, \"template_mined\": \"PacketResponder <:BLOCKID:> 1 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 59985 millis timeout left.\", \"cluster_count\": 45}\n",
      "Processing line: 4680000, rate 78712.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 4690000, rate 80043.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 4700000, rate 80832.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 4710000, rate 80528.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 4720000, rate 81070.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 4730000, rate 81695.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 4740000, rate 81385.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 4750000, rate 80992.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 4760000, rate 81620.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 4770000, rate 81519.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 4780000, rate 81173.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 4790000, rate 80969.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 4800000, rate 81622.0 lines/sec, 45 clusters so far.\n",
      "total          : took    51.15 s (100.00%),  4,809,281 samples,   10.64 ms / 1000 samples,       94,024.01 hz\n",
      "drain          : took    28.02 s ( 54.79%),  4,809,281 samples,    5.83 ms / 1000 samples,      171,613.02 hz\n",
      "mask           : took    12.84 s ( 25.11%),  4,809,281 samples,    2.67 ms / 1000 samples,      374,462.95 hz\n",
      "tree_search    : took    12.10 s ( 23.66%),  4,809,281 samples,    2.52 ms / 1000 samples,      397,323.12 hz\n",
      "cluster_exist  : took     6.54 s ( 12.78%),  4,809,236 samples,    1.36 ms / 1000 samples,      735,542.45 hz\n",
      "create_cluster : took     0.00 s (  0.00%),         45 samples,   15.17 ms / 1000 samples,       65,902.12 hz\n",
      "Processing line: 4810000, rate 81160.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 4820000, rate 81428.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 4830000, rate 81421.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 4840000, rate 81401.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 4850000, rate 81887.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 4860000, rate 81364.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 4870000, rate 81750.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 4880000, rate 81388.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 4890000, rate 81382.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 4900000, rate 81481.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 4910000, rate 81782.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 4920000, rate 80985.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 4930000, rate 81390.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 4940000, rate 81419.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 4950000, rate 81667.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 4960000, rate 81309.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 4970000, rate 81564.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 4980000, rate 81559.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 4990000, rate 81612.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 5000000, rate 82009.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 5010000, rate 81216.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5020000, rate 81903.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5030000, rate 81205.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 5040000, rate 81333.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5050000, rate 81544.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 5060000, rate 81281.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 5070000, rate 81594.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 5080000, rate 81676.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 5090000, rate 81594.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 5100000, rate 80615.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5110000, rate 78697.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5120000, rate 78879.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5130000, rate 78645.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5140000, rate 78249.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5150000, rate 78677.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5160000, rate 78572.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5170000, rate 78803.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 5180000, rate 78753.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5190000, rate 78670.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 5200000, rate 78927.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 5210000, rate 87125.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5220000, rate 89527.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 5230000, rate 90380.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 5240000, rate 91078.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 5250000, rate 90988.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5260000, rate 90948.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 5270000, rate 91268.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5280000, rate 90701.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5290000, rate 89680.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5300000, rate 89228.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5310000, rate 88086.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5320000, rate 83477.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5330000, rate 83927.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 5340000, rate 82444.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5350000, rate 81258.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 5360000, rate 81819.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5370000, rate 81069.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5380000, rate 81363.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5390000, rate 81673.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 5400000, rate 80321.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5410000, rate 80594.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5420000, rate 81979.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5430000, rate 81692.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5440000, rate 80933.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 5450000, rate 81661.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5460000, rate 80671.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 5470000, rate 81489.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5480000, rate 81667.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 5490000, rate 81299.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5500000, rate 81205.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 5510000, rate 81587.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5520000, rate 81467.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 5530000, rate 81067.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5540000, rate 81391.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 5550000, rate 81414.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 5560000, rate 81168.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 5570000, rate 81725.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5580000, rate 81543.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5590000, rate 81441.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 5600000, rate 81607.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5610000, rate 81616.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5620000, rate 81605.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5630000, rate 81547.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5640000, rate 81193.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 5650000, rate 81424.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 5660000, rate 81658.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 5670000, rate 81538.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 5680000, rate 81553.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5690000, rate 81514.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 5700000, rate 81332.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5710000, rate 81483.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 5720000, rate 81595.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 5730000, rate 81393.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5740000, rate 81750.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 5750000, rate 81960.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5760000, rate 81332.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5770000, rate 81682.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 5780000, rate 81697.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 5790000, rate 81051.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 5800000, rate 89887.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5810000, rate 89315.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 5820000, rate 89095.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5830000, rate 89419.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 5840000, rate 89331.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5850000, rate 89021.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 5860000, rate 89570.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5870000, rate 89430.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 5880000, rate 83553.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5890000, rate 78628.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 5900000, rate 78874.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5910000, rate 78763.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 5920000, rate 78680.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 5930000, rate 78940.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5940000, rate 78757.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5950000, rate 78855.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 5960000, rate 79023.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 5970000, rate 78710.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 5980000, rate 80330.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5990000, rate 89823.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 6000000, rate 90258.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6010000, rate 90869.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 6020000, rate 90875.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6030000, rate 90479.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6040000, rate 90702.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6050000, rate 91121.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6060000, rate 90505.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6070000, rate 90723.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 6080000, rate 89688.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 6090000, rate 85344.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 6100000, rate 82217.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6110000, rate 81768.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 6120000, rate 81437.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6130000, rate 81603.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6140000, rate 81766.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 6150000, rate 81457.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6160000, rate 81185.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6170000, rate 81728.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 6180000, rate 81619.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6190000, rate 81693.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6200000, rate 81546.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 6210000, rate 81667.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6220000, rate 81440.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 6230000, rate 81309.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 6240000, rate 80567.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6250000, rate 79728.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6260000, rate 81584.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6270000, rate 81498.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6280000, rate 81754.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6290000, rate 81592.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 6300000, rate 81536.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 6310000, rate 81657.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 6320000, rate 81796.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6330000, rate 81694.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6340000, rate 81359.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 6350000, rate 81835.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 6360000, rate 81487.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 6370000, rate 79279.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 6380000, rate 81104.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 6390000, rate 81513.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6400000, rate 81367.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 6410000, rate 81687.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 6420000, rate 81148.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6430000, rate 81523.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6440000, rate 81758.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6450000, rate 81672.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 6460000, rate 81868.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 6470000, rate 81756.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 6480000, rate 81709.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6490000, rate 81641.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6500000, rate 81687.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 6510000, rate 81846.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 6520000, rate 81145.2 lines/sec, 45 clusters so far.\n",
      "Input (6520468): PacketResponder blk_-4380884313697280861 1 Exception java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.251.107.50:40367 remote=/10.251.30.85:50010]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 24, \"cluster_size\": 7, \"template_mined\": \"PacketResponder <:BLOCKID:> <:*:> Exception java.net.SocketTimeoutException: <:*:> millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected <:*:> <:*:>\", \"cluster_count\": 45}\n",
      "Processing line: 6530000, rate 78935.9 lines/sec, 45 clusters so far.\n",
      "Input (6530498): writeBlock blk_1684134505299265593 received exception java.net.NoRouteToHostException: No route to host\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 22, \"cluster_size\": 26, \"template_mined\": \"writeBlock <:BLOCKID:> received exception <:*:> <:*:> <:*:> <:*:> <:*:>\", \"cluster_count\": 45}\n",
      "Processing line: 6540000, rate 79956.4 lines/sec, 45 clusters so far.\n",
      "Input (6549008): writeBlock blk_6189089042276997489 received exception java.io.IOException: Broken pipe\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 44, \"cluster_size\": 2, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.io.IOException: <:*:> <:*:>\", \"cluster_count\": 45}\n",
      "Processing line: 6550000, rate 80307.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6560000, rate 77730.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6570000, rate 85395.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6580000, rate 81091.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 6590000, rate 80100.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6600000, rate 81353.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6610000, rate 81543.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6620000, rate 81314.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 6630000, rate 81245.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6640000, rate 81255.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6650000, rate 81236.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 6660000, rate 81389.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 6670000, rate 80933.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6680000, rate 81574.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6690000, rate 81483.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 6700000, rate 81195.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6710000, rate 81449.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6720000, rate 81433.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6730000, rate 81404.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6740000, rate 81630.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 6750000, rate 81658.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 6760000, rate 81321.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6770000, rate 81310.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6780000, rate 81228.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6790000, rate 81522.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6800000, rate 81665.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6810000, rate 81560.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6820000, rate 81396.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 6830000, rate 81358.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6840000, rate 81441.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 6850000, rate 81317.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6860000, rate 81361.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 6870000, rate 81373.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 6880000, rate 81194.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 6890000, rate 81623.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6900000, rate 81502.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6910000, rate 81353.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 6920000, rate 81353.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6930000, rate 80781.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6940000, rate 81399.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6950000, rate 81539.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 6960000, rate 81306.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 6970000, rate 81026.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 6980000, rate 81164.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6990000, rate 81110.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 7000000, rate 81222.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 7010000, rate 81323.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 7020000, rate 81397.4 lines/sec, 45 clusters so far.\n",
      "Input (7023749): Adding an already existing block blk_-2074647664485597823\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 46, \"cluster_size\": 1, \"template_mined\": \"Adding an already existing block <:BLOCKID:>\", \"cluster_count\": 46}\n",
      "Processing line: 7030000, rate 80147.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7040000, rate 80731.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 7050000, rate 79223.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7060000, rate 78559.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7070000, rate 78181.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 7080000, rate 78530.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7090000, rate 78472.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 7100000, rate 78679.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 7110000, rate 78706.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 7120000, rate 78325.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7130000, rate 78491.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 7140000, rate 78574.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 7150000, rate 78843.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 7160000, rate 78761.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7170000, rate 78806.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 7180000, rate 78796.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7190000, rate 78879.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 7200000, rate 78822.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 7210000, rate 78812.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7220000, rate 78804.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7230000, rate 79074.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7240000, rate 78698.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 7250000, rate 78555.2 lines/sec, 46 clusters so far.\n",
      "total          : took    77.83 s (100.00%),  7,259,084 samples,   10.72 ms / 1000 samples,       93,270.49 hz\n",
      "drain          : took    42.71 s ( 54.88%),  7,259,084 samples,    5.88 ms / 1000 samples,      169,965.52 hz\n",
      "mask           : took    19.56 s ( 25.13%),  7,259,084 samples,    2.69 ms / 1000 samples,      371,150.28 hz\n",
      "tree_search    : took    18.57 s ( 23.85%),  7,259,084 samples,    2.56 ms / 1000 samples,      390,997.95 hz\n",
      "cluster_exist  : took     9.94 s ( 12.78%),  7,259,038 samples,    1.37 ms / 1000 samples,      730,040.24 hz\n",
      "create_cluster : took     0.00 s (  0.00%),         46 samples,   15.21 ms / 1000 samples,       65,736.96 hz\n",
      "Processing line: 7260000, rate 85609.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 7270000, rate 88460.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 7280000, rate 89749.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 7290000, rate 90228.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 7300000, rate 90536.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 7310000, rate 91103.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 7320000, rate 90058.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7330000, rate 90790.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 7340000, rate 90816.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 7350000, rate 90992.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7360000, rate 90956.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 7370000, rate 88370.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7380000, rate 90911.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 7390000, rate 90647.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 7400000, rate 91028.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 7410000, rate 90867.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 7420000, rate 90541.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7430000, rate 90784.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 7440000, rate 90323.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 7450000, rate 88902.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 7460000, rate 88273.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 7470000, rate 86881.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7480000, rate 85030.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 7490000, rate 82741.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 7500000, rate 81474.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 7510000, rate 81601.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 7520000, rate 81361.1 lines/sec, 46 clusters so far.\n",
      "Input (7527735): PacketResponder blk_2928690555251770634 2 Exception java.io.IOException: Connection reset by peer\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 38, \"cluster_size\": 3, \"template_mined\": \"PacketResponder <:BLOCKID:> <:*:> Exception java.io.IOException: Connection reset by peer\", \"cluster_count\": 46}\n",
      "Processing line: 7530000, rate 80747.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7540000, rate 80403.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 7550000, rate 81211.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7560000, rate 81557.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7570000, rate 80360.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7580000, rate 81223.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7590000, rate 81067.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 7600000, rate 81542.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7610000, rate 80100.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7620000, rate 80735.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 7630000, rate 81480.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 7640000, rate 81435.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 7650000, rate 81271.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7660000, rate 81494.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 7670000, rate 79872.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 7680000, rate 81394.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 7690000, rate 81447.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 7700000, rate 81285.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 7710000, rate 81453.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7720000, rate 81462.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 7730000, rate 81194.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 7740000, rate 81259.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 7750000, rate 81093.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 7760000, rate 81546.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 7770000, rate 79083.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7780000, rate 81247.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7790000, rate 81476.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7800000, rate 81441.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 7810000, rate 81680.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 7820000, rate 81400.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 7830000, rate 81112.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 7840000, rate 81205.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7850000, rate 81453.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 7860000, rate 81256.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7870000, rate 81230.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 7880000, rate 81494.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7890000, rate 81980.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 7900000, rate 81000.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7910000, rate 81793.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 7920000, rate 81505.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7930000, rate 81020.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7940000, rate 81112.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7950000, rate 84784.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7960000, rate 81049.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 7970000, rate 81068.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 7980000, rate 80984.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 7990000, rate 81109.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 8000000, rate 81176.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 8010000, rate 81296.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 8020000, rate 81285.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 8030000, rate 81271.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 8040000, rate 81273.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 8050000, rate 81493.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 8060000, rate 81373.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 8070000, rate 81148.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 8080000, rate 81473.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 8090000, rate 81363.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 8100000, rate 80815.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 8110000, rate 81272.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 8120000, rate 81446.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 8130000, rate 81120.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 8140000, rate 81339.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 8150000, rate 81376.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 8160000, rate 81304.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 8170000, rate 81314.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 8180000, rate 81319.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 8190000, rate 81321.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 8200000, rate 81384.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 8210000, rate 81531.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 8220000, rate 81397.4 lines/sec, 46 clusters so far.\n",
      "Input (8226043): writeBlock blk_-2308563252795628677 received exception java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.251.90.134:46191 remote=/10.250.13.188:50010]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 40, \"cluster_size\": 9, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.net.SocketTimeoutException: <:*:> millis timeout while waiting for channel to be ready for <:*:> ch : java.nio.channels.SocketChannel[connected <:*:> <:*:>\", \"cluster_count\": 46}\n",
      "Processing line: 8230000, rate 79877.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 8240000, rate 80409.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 8250000, rate 81687.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 8260000, rate 80950.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 8270000, rate 80800.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 8280000, rate 81335.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 8290000, rate 81279.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 8300000, rate 81474.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 8310000, rate 81300.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 8320000, rate 81445.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 8330000, rate 81410.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 8340000, rate 81245.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 8350000, rate 81375.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 8360000, rate 81341.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 8370000, rate 81520.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 8380000, rate 81535.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 8390000, rate 81475.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 8400000, rate 81662.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 8410000, rate 81571.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 8420000, rate 80980.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 8430000, rate 78725.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 8440000, rate 78681.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 8450000, rate 78729.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 8460000, rate 78920.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 8470000, rate 78175.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 8480000, rate 78297.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 8490000, rate 78625.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 8500000, rate 78384.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 8510000, rate 78632.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 8520000, rate 78581.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 8530000, rate 86244.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 8540000, rate 89522.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 8550000, rate 89669.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 8560000, rate 90602.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 8570000, rate 90807.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 8580000, rate 91175.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 8590000, rate 90579.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 8600000, rate 90396.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 8610000, rate 90177.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 8620000, rate 89025.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 8630000, rate 87930.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 8640000, rate 85162.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 8650000, rate 82525.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 8660000, rate 81766.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 8670000, rate 80759.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 8680000, rate 81123.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 8690000, rate 80411.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 8700000, rate 81401.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 8710000, rate 79701.6 lines/sec, 46 clusters so far.\n",
      "Input (8718961): Exception in receiveBlock for block blk_6224343649004202692 java.net.SocketTimeoutException: 490000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.251.127.243:44092 remote=/10.251.107.50:50010]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 39, \"cluster_size\": 2, \"template_mined\": \"Exception in receiveBlock for block <:BLOCKID:> java.net.SocketTimeoutException: 490000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected <:*:> <:*:>\", \"cluster_count\": 46}\n",
      "Processing line: 8720000, rate 79867.8 lines/sec, 46 clusters so far.\n",
      "Input (8729166): Exception in receiveBlock for block blk_-6994808880344424033 java.io.IOException: Broken pipe\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 47, \"cluster_size\": 1, \"template_mined\": \"Exception in receiveBlock for block <:BLOCKID:> java.io.IOException: Broken pipe\", \"cluster_count\": 47}\n",
      "Processing line: 8730000, rate 77069.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 8740000, rate 80446.8 lines/sec, 47 clusters so far.\n",
      "Input (8746310): PacketResponder blk_-4723951162006187997 1 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 59799 millis timeout left.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 45, \"cluster_size\": 2, \"template_mined\": \"PacketResponder <:BLOCKID:> 1 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. <:*:> millis timeout left.\", \"cluster_count\": 47}\n",
      "Processing line: 8750000, rate 79531.7 lines/sec, 47 clusters so far.\n",
      "Input (8757961): PacketResponder blk_8006271611835981128 1 Exception java.io.IOException: The stream is closed\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 38, \"cluster_size\": 9, \"template_mined\": \"PacketResponder <:BLOCKID:> <:*:> Exception java.io.IOException: <:*:> <:*:> <:*:> <:*:>\", \"cluster_count\": 47}\n",
      "Processing line: 8760000, rate 76927.7 lines/sec, 47 clusters so far.\n",
      "Input (8760413): Exception in receiveBlock for block blk_584730932939516842 java.net.SocketTimeoutException: 485000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.250.19.227:55473 remote=/10.251.107.50:50010]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 39, \"cluster_size\": 4, \"template_mined\": \"Exception in receiveBlock for block <:BLOCKID:> java.net.SocketTimeoutException: <:*:> millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected <:*:> <:*:>\", \"cluster_count\": 47}\n",
      "Processing line: 8770000, rate 76650.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 8780000, rate 80316.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 8790000, rate 80176.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 8800000, rate 81368.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 8810000, rate 78379.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 8820000, rate 79275.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 8830000, rate 80151.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 8840000, rate 79883.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 8850000, rate 80743.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 8860000, rate 80887.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 8870000, rate 80234.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 8880000, rate 80732.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 8890000, rate 81113.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 8900000, rate 81443.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 8910000, rate 81187.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 8920000, rate 81288.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 8930000, rate 81532.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 8940000, rate 80699.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 8950000, rate 79484.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 8960000, rate 81472.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 8970000, rate 79077.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 8980000, rate 81297.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 8990000, rate 81317.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 9000000, rate 81228.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 9010000, rate 81509.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9020000, rate 81589.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9030000, rate 81179.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9040000, rate 81253.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9050000, rate 81306.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 9060000, rate 81480.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9070000, rate 81196.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9080000, rate 81292.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 9090000, rate 81197.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 9100000, rate 81396.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 9110000, rate 81624.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9120000, rate 79402.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 9130000, rate 78237.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9140000, rate 78167.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 9150000, rate 78479.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9160000, rate 78483.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9170000, rate 78521.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 9180000, rate 78569.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9190000, rate 78856.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9200000, rate 78787.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9210000, rate 78026.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9220000, rate 79330.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 9230000, rate 87163.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 9240000, rate 89927.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 9250000, rate 90316.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9260000, rate 90877.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 9270000, rate 91221.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9280000, rate 90927.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9290000, rate 90807.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9300000, rate 90706.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9310000, rate 90535.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 9320000, rate 89648.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9330000, rate 87665.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9340000, rate 85058.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 9350000, rate 83000.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9360000, rate 81929.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9370000, rate 81252.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 9380000, rate 81620.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9390000, rate 81376.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 9400000, rate 81240.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 9410000, rate 81299.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9420000, rate 81358.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 9430000, rate 80761.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9440000, rate 79895.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 9450000, rate 80906.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 9460000, rate 79999.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 9470000, rate 81481.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9480000, rate 81323.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9490000, rate 81253.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9500000, rate 80818.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9510000, rate 81655.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9520000, rate 81279.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9530000, rate 80997.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9540000, rate 81219.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 9550000, rate 81191.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 9560000, rate 81099.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 9570000, rate 81175.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9580000, rate 81588.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9590000, rate 81578.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 9600000, rate 81233.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9610000, rate 81155.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 9620000, rate 81487.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 9630000, rate 81176.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 9640000, rate 81421.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9650000, rate 81538.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9660000, rate 81205.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 9670000, rate 80983.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9680000, rate 81501.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 9690000, rate 79592.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9700000, rate 81087.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 9710000, rate 81600.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 9720000, rate 81416.1 lines/sec, 47 clusters so far.\n",
      "total          : took   104.47 s (100.00%),  9,720,954 samples,   10.75 ms / 1000 samples,       93,052.82 hz\n",
      "drain          : took    57.27 s ( 54.83%),  9,720,954 samples,    5.89 ms / 1000 samples,      169,724.91 hz\n",
      "mask           : took    26.31 s ( 25.19%),  9,720,954 samples,    2.71 ms / 1000 samples,      369,427.30 hz\n",
      "tree_search    : took    24.85 s ( 23.79%),  9,720,954 samples,    2.56 ms / 1000 samples,      391,116.56 hz\n",
      "cluster_exist  : took    13.36 s ( 12.79%),  9,720,907 samples,    1.37 ms / 1000 samples,      727,753.57 hz\n",
      "create_cluster : took     0.00 s (  0.00%),         47 samples,   15.27 ms / 1000 samples,       65,470.70 hz\n",
      "Processing line: 9730000, rate 80395.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9740000, rate 81044.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9750000, rate 81537.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9760000, rate 81193.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 9770000, rate 81453.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 9780000, rate 81634.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9790000, rate 81448.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9800000, rate 81301.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9810000, rate 79842.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 9820000, rate 78658.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9830000, rate 75831.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 9840000, rate 75554.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9850000, rate 78445.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9860000, rate 78753.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 9870000, rate 78680.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 9880000, rate 78594.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9890000, rate 78324.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 9900000, rate 78966.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9910000, rate 81285.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 9920000, rate 87744.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9930000, rate 89893.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9940000, rate 90470.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 9950000, rate 90351.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9960000, rate 91200.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 9970000, rate 90826.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9980000, rate 90795.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 9990000, rate 90630.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 10000000, rate 89501.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 10010000, rate 88922.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 10020000, rate 86869.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 10030000, rate 84354.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 10040000, rate 82538.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10050000, rate 81693.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10060000, rate 81483.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 10070000, rate 81606.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 10080000, rate 81361.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10090000, rate 81356.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10100000, rate 81029.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 10110000, rate 81602.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10120000, rate 81282.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10130000, rate 81226.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10140000, rate 81248.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 10150000, rate 79963.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 10160000, rate 81792.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10170000, rate 81139.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10180000, rate 81229.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 10190000, rate 81582.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10200000, rate 81469.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10210000, rate 81792.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 10220000, rate 81623.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 10230000, rate 81721.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 10240000, rate 80939.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 10250000, rate 81382.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 10260000, rate 81196.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 10270000, rate 81304.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10280000, rate 81475.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 10290000, rate 81302.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10300000, rate 81345.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10310000, rate 81445.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10320000, rate 81088.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 10330000, rate 80275.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10340000, rate 81522.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10350000, rate 81364.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10360000, rate 81600.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 10370000, rate 81047.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10380000, rate 81186.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 10390000, rate 81615.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 10400000, rate 81228.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10410000, rate 81345.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10420000, rate 81254.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 10430000, rate 81131.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10440000, rate 81063.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10450000, rate 81242.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10460000, rate 81202.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 10470000, rate 81011.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10480000, rate 81294.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10490000, rate 81336.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10500000, rate 83382.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10510000, rate 81422.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 10520000, rate 81458.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 10530000, rate 81289.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 10540000, rate 81021.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10550000, rate 81139.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10560000, rate 80077.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 10570000, rate 81084.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 10580000, rate 81222.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10590000, rate 81373.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 10600000, rate 81346.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10610000, rate 81087.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10620000, rate 81255.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 10630000, rate 81244.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10640000, rate 81326.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 10650000, rate 81124.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 10660000, rate 81236.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10670000, rate 81327.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10680000, rate 81368.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10690000, rate 81577.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10700000, rate 81423.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10710000, rate 80774.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10720000, rate 81259.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10730000, rate 81250.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10740000, rate 81479.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 10750000, rate 81464.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 10760000, rate 81522.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10770000, rate 81110.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10780000, rate 81248.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 10790000, rate 81139.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10800000, rate 81227.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 10810000, rate 81209.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 10820000, rate 81251.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10830000, rate 81490.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10840000, rate 81347.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 10850000, rate 81114.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10860000, rate 81437.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10870000, rate 80654.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 10880000, rate 81357.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10890000, rate 81689.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 10900000, rate 81321.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10910000, rate 81014.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10920000, rate 81070.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10930000, rate 80709.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 10940000, rate 81192.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10950000, rate 79859.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 10960000, rate 81343.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10970000, rate 80281.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 10980000, rate 78481.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 10990000, rate 78396.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 11000000, rate 78704.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 11010000, rate 78774.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 11020000, rate 78655.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 11030000, rate 78526.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 11040000, rate 78356.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 11050000, rate 77554.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 11060000, rate 78559.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 11070000, rate 79083.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 11080000, rate 90541.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 11090000, rate 90322.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 11100000, rate 90437.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 11110000, rate 90453.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 11120000, rate 90530.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 11130000, rate 90578.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 11140000, rate 90801.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 11150000, rate 90469.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 11160000, rate 90657.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 11170000, rate 90767.9 lines/sec, 47 clusters so far.\n",
      "--- Done processing file in 135.44 sec. Total of 11175629 lines, rate 82512.2 lines/sec, 47 clusters\n",
      "ID=1     : size=1723232   : Receiving block <:BLOCKID:> src: <:*:> dest: <:*:>\n",
      "ID=5     : size=1719741   : BLOCK* NameSystem.addStoredBlock: blockMap updated: <:*:> is added to <:BLOCKID:> size <:*:>\n",
      "ID=3     : size=1706728   : PacketResponder <:*:> for block <:BLOCKID:> <:*:>\n",
      "ID=4     : size=1706514   : Received block <:BLOCKID:> of size <:*:> from <:*:>\n",
      "ID=7     : size=1408984   : <:*:> block <:BLOCKID:> <:*:> <:*:>\n",
      "ID=34    : size=1396174   : BLOCK* NameSystem.delete: <:BLOCKID:> is added to invalidSet of <:*:>\n",
      "ID=12    : size=574940    : BLOCK* NameSystem.allocateBlock: <:*:> temporary/ task <:*:> <:*:> <:*:> <:*:> <:*:> <:BLOCKID:>\n",
      "ID=10    : size=428726    : <:*:> Served block <:BLOCKID:> to <:*:>\n",
      "ID=37    : size=356207    : <:*:> exception while serving <:BLOCKID:> to <:*:>\n",
      "ID=15    : size=120036    : Verification succeeded for <:BLOCKID:>\n",
      "ID=6     : size=7097      : Received block <:BLOCKID:> src: <:*:> dest: <:*:> of size <:*:>\n",
      "ID=13    : size=6837      : BLOCK* ask <:*:> to replicate <:BLOCKID:> to datanode(s) <:*:>\n",
      "ID=14    : size=6837      : <:*:> Starting thread to transfer block <:BLOCKID:> to <:*:>\n",
      "ID=35    : size=5545      : Unexpected error trying to delete block <:BLOCKID:>. BlockInfo not found in volumeMap.\n",
      "ID=18    : size=3226      : writeBlock <:BLOCKID:> received exception java.io.IOException: Could not read from stream\n",
      "ID=19    : size=1464      : Receiving empty packet for block <:BLOCKID:>\n",
      "ID=36    : size=1288      : BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for <:BLOCKID:> on <:*:> size <:*:> But it does not belong to any file.\n",
      "ID=23    : size=975       : BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for <:BLOCKID:> on <:*:> size <:*:>\n",
      "ID=8     : size=165       : <:*:> Starting thread to transfer block <:BLOCKID:> to <:*:> <:*:>\n",
      "ID=9     : size=165       : BLOCK* ask <:*:> to replicate <:BLOCKID:> to datanode(s) <:*:> <:*:>\n",
      "ID=22    : size=76        : writeBlock <:BLOCKID:> received exception <:*:> <:*:> <:*:> <:*:> <:*:>\n",
      "ID=21    : size=75        : Exception in receiveBlock for block <:BLOCKID:> java.io.IOException: Connection reset by peer\n",
      "ID=2     : size=73        : BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job <:*:> <:*:> <:BLOCKID:>\n",
      "ID=16    : size=67        : writeBlock <:BLOCKID:> received exception <:*:>\n",
      "ID=27    : size=65        : Changing block file offset of block <:BLOCKID:> from 0 to <:*:> meta file offset to <:*:>\n",
      "ID=26    : size=56        : Exception in receiveBlock for block <:BLOCKID:> <:*:>\n",
      "ID=43    : size=47        : PendingReplicationMonitor timed out block <:BLOCKID:>\n",
      "ID=28    : size=34        : <:*:> writing block <:BLOCKID:> to mirror <:*:>\n",
      "ID=25    : size=33        : PacketResponder <:BLOCKID:> <:*:> Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected <:*:> <:*:> <:*:> millis timeout left.\n",
      "ID=17    : size=29        : PacketResponder <:BLOCKID:> <:*:> Exception <:*:>\n",
      "ID=11    : size=24        : BLOCK* NameSystem.allocateBlock: <:*:> logs/history/ip-10-250-19-102.ec2.internal <:*:> job <:*:> <:*:> conf.xml. <:BLOCKID:>\n",
      "ID=20    : size=24        : BLOCK* NameSystem.allocateBlock: <:*:> logs/history/ip-10-250-19-102.ec2.internal <:*:> job <:*:> <:*:> root <:*:> <:BLOCKID:>\n",
      "ID=24    : size=22        : PacketResponder <:BLOCKID:> <:*:> Exception java.net.SocketTimeoutException: <:*:> millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected <:*:> <:*:>\n",
      "ID=29    : size=16        : Exception in receiveBlock for block <:BLOCKID:> java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected <:*:> <:*:> <:*:> millis timeout left.\n",
      "ID=44    : size=16        : writeBlock <:BLOCKID:> received exception java.io.IOException: <:*:> <:*:>\n",
      "ID=40    : size=15        : writeBlock <:BLOCKID:> received exception java.net.SocketTimeoutException: <:*:> millis timeout while waiting for channel to be ready for <:*:> ch : java.nio.channels.SocketChannel[connected <:*:> <:*:>\n",
      "ID=38    : size=13        : PacketResponder <:BLOCKID:> <:*:> Exception java.io.IOException: <:*:> <:*:> <:*:> <:*:>\n",
      "ID=46    : size=10        : Adding an already existing block <:BLOCKID:>\n",
      "ID=32    : size=9         : writeBlock <:BLOCKID:> received exception java.io.IOException: Block <:BLOCKID:> is valid, and cannot be written to.\n",
      "ID=33    : size=9         : <:*:> to transfer <:BLOCKID:> to <:*:> got java.io.IOException: Connection reset by peer\n",
      "ID=30    : size=7         : writeBlock <:BLOCKID:> received exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected <:*:> <:*:> <:*:> millis timeout left.\n",
      "ID=31    : size=6         : PacketResponder <:BLOCKID:> <:*:> Exception java.io.IOException: Broken pipe\n",
      "ID=39    : size=5         : Exception in receiveBlock for block <:BLOCKID:> java.net.SocketTimeoutException: <:*:> millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected <:*:> <:*:>\n",
      "ID=41    : size=5         : Reopen Block <:BLOCKID:>\n",
      "ID=45    : size=5         : PacketResponder <:BLOCKID:> 1 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. <:*:> millis timeout left.\n",
      "ID=42    : size=4         : BLOCK* Removing block <:BLOCKID:> from neededReplications as it does not belong to any file.\n",
      "ID=47    : size=3         : Exception in receiveBlock for block <:BLOCKID:> java.io.IOException: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "mine_template = mineModel(f = open(\"../DATA/HDFS.log\"),cfg='../data/drain3_hdfs.ini')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f7dca",
   "metadata": {},
   "source": [
    "#### Get Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5e27e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = []\n",
    "templates = []\n",
    "identifier = []\n",
    "\n",
    "f = open(\"../DATA/HDFS.log\")\n",
    "lines = f.readlines()     \n",
    "\n",
    "for line in lines:   \n",
    "   line = line.partition(': ')[2]\n",
    "   match = mine_template.match(line)\n",
    "   event.append('E'+ str(match.cluster_id))\n",
    "   templates.append(match.get_template())\n",
    "   ps = template_miner.extract_parameters(match.get_template(),line,False)\n",
    "   for p in ps:\n",
    "      if p.mask_name == 'BLOCKID':\n",
    "         identifier.append(p.value)\n",
    "         break\n",
    "\n",
    "df = pd.DataFrame({'event': event,'template': templates,'identifier':identifier})\n",
    "df.to_csv('../data/hdfs_templates.csv')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "261b4fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Drain3 template miner\n"
     ]
    }
   ],
   "source": [
    "config = TemplateMinerConfig()\n",
    "config.load(\"../data/drain3.ini\")\n",
    "config.profiling_enabled = True\n",
    "template_miner = TemplateMiner(config=config)   \n",
    "\n",
    "log_line = \"Receiving block blk_-1608999687919862906 src: /10.250.19.102:54106 dest: /10.250.19.102:50010\"\n",
    "\n",
    "result = template_miner.add_log_message(log_line)\n",
    "\n",
    "match = template_miner.match(log_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41b29845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID=1     : size=1         : Receiving block <:BLOCKID:> src: /<:IP:>:<:NUM:> dest: /<:IP:>:<:NUM:>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['blk -1608999687919862906',\n",
       " '10.250.19.102',\n",
       " '54106',\n",
       " '10.250.19.102',\n",
       " '50010']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(match)\n",
    "template_miner.get_parameter_list(match.get_template(),log_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8dc96dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blk -1608999687919862906\n"
     ]
    }
   ],
   "source": [
    "ps = template_miner.extract_parameters(match.get_template(),log_line,False)\n",
    "for p in ps:\n",
    "    if p.mask_name == 'BLOCKID':\n",
    "        print(p.value)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7cd2205e5de8df8d67c6695cefbd519d929d2d761d1221a3acfc20267f908436"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
