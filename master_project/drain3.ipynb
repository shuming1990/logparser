{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0381c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c78a1bf",
   "metadata": {},
   "source": [
    "### mining log template model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "642cca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from os.path import dirname\n",
    "\n",
    "def mineModel(f, cfg):\n",
    "    config = TemplateMinerConfig()\n",
    "    config.load(cfg)\n",
    "    config.profiling_enabled = True\n",
    "    template_miner = TemplateMiner(config=config)   \n",
    "    lines = f.readlines()              \n",
    "\n",
    "    line_count = 0\n",
    "    start_time = time.time()\n",
    "    batch_start_time = start_time\n",
    "    batch_size = 10000\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(message)s')\n",
    "\n",
    "    for line in lines:\n",
    "        s = line\n",
    "        line = line.rstrip()\n",
    "        line = line.partition(\": \")[2]\n",
    "        line = line.replace('blk_','blk#')\n",
    "        if line == '' : continue\n",
    "        result = template_miner.add_log_message(line)\n",
    "        line_count += 1\n",
    "        if line_count % batch_size == 0:\n",
    "            time_took = time.time() - batch_start_time\n",
    "            rate = batch_size / time_took\n",
    "            logger.info(f\"Processing line: {line_count}, rate {rate:.1f} lines/sec, \"\n",
    "                        f\"{len(template_miner.drain.clusters)} clusters so far.\")\n",
    "            batch_start_time = time.time()\n",
    "        if result[\"change_type\"] != \"none\":\n",
    "            result_json = json.dumps(result)\n",
    "            logger.info(f\"Input ({line_count}): \" + line)\n",
    "            logger.info(\"Result: \" + result_json)\n",
    "\n",
    "    time_took = time.time() - start_time\n",
    "    rate = line_count / time_took\n",
    "    logger.info(f\"--- Done processing file in {time_took:.2f} sec. Total of {line_count} lines, rate {rate:.1f} lines/sec, \"\n",
    "                f\"{len(template_miner.drain.clusters)} clusters\")\n",
    "\n",
    "    sorted_clusters = sorted(template_miner.drain.clusters, key=lambda it: it.size, reverse=True)\n",
    "    for cluster in sorted_clusters:\n",
    "        logger.info(cluster)\n",
    "    \n",
    "    return template_miner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb7818",
   "metadata": {},
   "source": [
    "### Parse log files from Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2dd0a7",
   "metadata": {},
   "source": [
    "#### Move all files to a single directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b3c0ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shuming/Downloads/Hadoop/abnormal_label.txt\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000028.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000026.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000027.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000025.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000024.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000023.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0005/container_1445087491445_0005_01_000022.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0018/container_1445062781478_0018_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0002/container_1445087491445_0002_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0020/container_1445062781478_0020_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0016/container_1445062781478_0016_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0011/container_1445062781478_0011_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0020/container_1445144423722_0020_02_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0005/container_1445175094696_0005_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0002/container_1445175094696_0002_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0017/container_1445062781478_0017_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000024.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000023.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000022.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0003/container_1445087491445_0003_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000022.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000023.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_02_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0004/container_1445087491445_0004_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0019/container_1445062781478_0019_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0003/container_1445175094696_0003_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0004/container_1445175094696_0004_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0021/container_1445144423722_0021_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0005/container_1445182159119_0005_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0002/container_1445182159119_0002_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0001/container_1445094324383_0001_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0003/container_1445182159119_0003_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0004/container_1445182159119_0004_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0019/container_1445182159119_0019_02_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0017/container_1445182159119_0017_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0011/container_1445182159119_0011_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0016/container_1445182159119_0016_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0020/container_1445182159119_0020_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0018/container_1445182159119_0018_02_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0005/container_1445076437777_0005_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0002/container_1445076437777_0002_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000022.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0010/container_1445087491445_0010_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0003/container_1445076437777_0003_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0004/container_1445076437777_0004_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0024/container_1445144423722_0024_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0023/container_1445144423722_0023_02_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445175094696_0001/container_1445175094696_0001_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000024.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000022.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0001/container_1445087491445_0001_01_000023.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0006/container_1445087491445_0006_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0012/container_1445062781478_0012_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0015/container_1445062781478_0015_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0008/container_1445087491445_0008_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445144423722_0022/container_1445144423722_0022_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0014/container_1445062781478_0014_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0009/container_1445087491445_0009_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445062781478_0013/container_1445062781478_0013_02_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445087491445_0007/container_1445087491445_0007_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0001/container_1445182159119_0001_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0004/container_1445094324383_0004_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0003/container_1445094324383_0003_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0002/container_1445094324383_0002_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000021.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000020.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000022.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445094324383_0005/container_1445094324383_0005_01_000023.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0013/container_1445182159119_0013_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_02_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_02_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_02_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0014/container_1445182159119_0014_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000017.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000019.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000018.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0015/container_1445182159119_0015_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445182159119_0012/container_1445182159119_0012_01_000008.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000011.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000005.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000004.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000010.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000006.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000007.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000003.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000002.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000015.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_02_000016.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_02_000001.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_02_000014.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000009.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_02_000013.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_02_000012.log\n",
      "/Users/shuming/Downloads/Hadoop/application_1445076437777_0001/container_1445076437777_0001_01_000008.log\n",
      "copy complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "source_path = os.path.abspath(r'/Users/shuming/Downloads/Hadoop')     # source directory\n",
    "target_path = os.path.abspath(r'/Users/shuming/Downloads/merged_hadoop/')    # target directory\n",
    "\n",
    "if not os.path.exists(target_path):     # creat target directory if it doesn't exist \n",
    "    os.makedirs(target_path)\n",
    "\n",
    "if os.path.exists(source_path):    \n",
    "    \n",
    "    for root, dirs, files in os.walk(source_path):\n",
    "        for file in files:\n",
    "            src_file = os.path.join(root, file)\n",
    "            shutil.copy(src_file, target_path)\n",
    "            print(src_file)\n",
    "\n",
    "print('copy complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1d21e6",
   "metadata": {},
   "source": [
    "#### Merged all files into one log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad3f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os module consists of many functions about file and directory processing\n",
    "import os  \n",
    "meragefiledir = '/Users/shuming/Downloads/merged_hadoop/'\n",
    "filenames=os.listdir(meragefiledir)  \n",
    "# target file path\n",
    "file=open('/Users/shuming/Downloads/merged_hadoop/merged.log','w')  \n",
    "   \n",
    "for filename in filenames:  \n",
    "    if(filename != 'merged.log'):\n",
    "        filepath=meragefiledir+filename    \n",
    "        for line in open(filepath):  \n",
    "            file.writelines(line)  \n",
    "        file.write('\\n')  \n",
    " \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55c5fc4",
   "metadata": {},
   "source": [
    "#### Train Drain3 model with existing logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4b98610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Drain3 template miner\n",
      "Input (1): loaded properties from hadoop-metrics2.properties\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 1, \"cluster_size\": 1, \"template_mined\": \"loaded properties from hadoop-metrics2.properties\", \"cluster_count\": 1}\n",
      "Input (2): Scheduled snapshot period at 10 second(s).\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 2, \"cluster_size\": 1, \"template_mined\": \"Scheduled snapshot period at <:NUM:> second(s).\", \"cluster_count\": 2}\n",
      "Input (3): MapTask metrics system started\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 3, \"cluster_size\": 1, \"template_mined\": \"MapTask metrics system started\", \"cluster_count\": 3}\n",
      "Input (4): Executing with tokens:\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 4, \"cluster_size\": 1, \"template_mined\": \"Executing with tokens:\", \"cluster_count\": 4}\n",
      "Input (5): Kind: mapreduce.job, Service: job_1445076437777_0005, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@666adef3)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 5, \"cluster_size\": 1, \"template_mined\": \"Kind: mapreduce.job, Service: job <:NUM:> <:NUM:>, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@666adef3)\", \"cluster_count\": 5}\n",
      "Input (6): Sleeping for 0ms before retrying again. Got null now.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 6, \"cluster_size\": 1, \"template_mined\": \"Sleeping for 0ms before retrying again. Got null now.\", \"cluster_count\": 6}\n",
      "Input (7): mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application_1445076437777_0005\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 7, \"cluster_size\": 1, \"template_mined\": \"mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application <:NUM:> <:NUM:>\", \"cluster_count\": 7}\n",
      "Input (8): session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 8, \"cluster_size\": 1, \"template_mined\": \"session.id is deprecated. Instead, use dfs.metrics.session-id\", \"cluster_count\": 8}\n",
      "Input (9): ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 9, \"cluster_size\": 1, \"template_mined\": \"ProcfsBasedProcessTree currently is supported only on Linux.\", \"cluster_count\": 9}\n",
      "Input (10):  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6270c836\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 10, \"cluster_size\": 1, \"template_mined\": \"Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6270c836\", \"cluster_count\": 10}\n",
      "Input (11): Processing split: hdfs://msra-sa-41:9000/pageinput2.txt:268435456+134217728\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 11, \"cluster_size\": 1, \"template_mined\": \"Processing split: hdfs://msra-sa-<:NUM:>:<:NUM:>/pageinput2.txt:<:NUM:>+<:NUM:>\", \"cluster_count\": 11}\n",
      "Input (12): (EQUATOR) 0 kvi 26214396(104857584)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 12, \"cluster_size\": 1, \"template_mined\": \"(EQUATOR) <:NUM:> kvi <:NUM:>(<:NUM:>)\", \"cluster_count\": 12}\n",
      "Input (13): mapreduce.task.io.sort.mb: 100\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 13, \"cluster_size\": 1, \"template_mined\": \"mapreduce.task.io.sort.mb: <:NUM:>\", \"cluster_count\": 13}\n",
      "Input (14): soft limit at 83886080\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 14, \"cluster_size\": 1, \"template_mined\": \"soft limit at <:NUM:>\", \"cluster_count\": 14}\n",
      "Input (15): bufstart = 0; bufvoid = 104857600\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 15, \"cluster_size\": 1, \"template_mined\": \"bufstart = <:NUM:>; bufvoid = <:NUM:>\", \"cluster_count\": 15}\n",
      "Input (16): kvstart = 26214396; length = 6553600\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 16, \"cluster_size\": 1, \"template_mined\": \"kvstart = <:NUM:>; length = <:NUM:>\", \"cluster_count\": 16}\n",
      "Input (17): Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 17, \"cluster_size\": 1, \"template_mined\": \"Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\", \"cluster_count\": 17}\n",
      "Input (18): Spilling map output\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 18, \"cluster_size\": 1, \"template_mined\": \"Spilling map output\", \"cluster_count\": 18}\n",
      "Input (19): bufstart = 0; bufend = 48249276; bufvoid = 104857600\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 19, \"cluster_size\": 1, \"template_mined\": \"bufstart = <:NUM:>; bufend = <:NUM:>; bufvoid = <:NUM:>\", \"cluster_count\": 19}\n",
      "Input (20): kvstart = 26214396(104857584); kvend = 17305200(69220800); length = 8909197/6553600\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 20, \"cluster_size\": 1, \"template_mined\": \"kvstart = <:NUM:>(<:NUM:>); kvend = <:NUM:>(<:NUM:>); length = <:NUM:>/<:NUM:>\", \"cluster_count\": 20}\n",
      "Input (22): Finished spill 0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 21, \"cluster_size\": 1, \"template_mined\": \"Finished spill <:NUM:>\", \"cluster_count\": 21}\n",
      "Input (23): (RESET) equator 57318028 kv 14329500(57318000) kvi 12129788(48519152)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 22, \"cluster_size\": 1, \"template_mined\": \"(RESET) equator <:NUM:> kv <:NUM:>(<:NUM:>) kvi <:NUM:>(<:NUM:>)\", \"cluster_count\": 22}\n",
      "Input (60): Starting flush of map output\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 23, \"cluster_size\": 1, \"template_mined\": \"Starting flush of map output\", \"cluster_count\": 23}\n",
      "Input (65): Merging 8 sorted segments\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 24, \"cluster_size\": 1, \"template_mined\": \"Merging <:NUM:> sorted segments\", \"cluster_count\": 24}\n",
      "Input (66): Down to the last merge-pass, with 8 segments left of total size: 288688442 bytes\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 25, \"cluster_size\": 1, \"template_mined\": \"Down to the last merge-pass, with <:NUM:> segments left of total size: <:NUM:> bytes\", \"cluster_count\": 25}\n",
      "Input (67): Task:attempt_1445076437777_0005_m_000002_0 is done. And is in the process of committing\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 26, \"cluster_size\": 1, \"template_mined\": \"Task:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> is done. And is in the process of committing\", \"cluster_count\": 26}\n",
      "Input (68): Task 'attempt_1445076437777_0005_m_000002_0' done.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 27, \"cluster_size\": 1, \"template_mined\": \"Task 'attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>' done.\", \"cluster_count\": 27}\n",
      "Input (73): Kind: mapreduce.job, Service: job_1445087491445_0005, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@7f0eb4b4)\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 5, \"cluster_size\": 2, \"template_mined\": \"Kind: mapreduce.job, Service: job <:NUM:> <:NUM:>, Ident: <:*:>\", \"cluster_count\": 27}\n",
      "Input (78):  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6ad3381f\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 10, \"cluster_size\": 2, \"template_mined\": \"Using ResourceCalculatorProcessTree : <:*:>\", \"cluster_count\": 27}\n",
      "Input (79): Processing split: hdfs://msra-sa-41:9000/wordcount2.txt:402653184+134217728\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 11, \"cluster_size\": 2, \"template_mined\": \"Processing split: <:*:>\", \"cluster_count\": 27}\n",
      "Input (121): I/O error constructing remote block reader.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 28, \"cluster_size\": 1, \"template_mined\": \"I/O error constructing remote block reader.\", \"cluster_count\": 28}\n",
      "Input (122): Connection timed out: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 29, \"cluster_size\": 1, \"template_mined\": \"Connection timed out: no further information\", \"cluster_count\": 29}\n",
      "Input (123): Failed to connect to /10.86.169.121:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 30, \"cluster_size\": 1, \"template_mined\": \"Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information\", \"cluster_count\": 30}\n",
      "Input (125): Successfully connected to /10.190.173.170:50010 for BP-1347369012-10.190.173.170-1444972147527:blk#1073742826_2022\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 31, \"cluster_size\": 1, \"template_mined\": \"Successfully connected to /<:IP:>:<:NUM:> for BP-<:NUM:>-<:IP:><:NUM:>:blk#<:NUM:> <:NUM:>\", \"cluster_count\": 31}\n",
      "Input (214): Connection refused: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 32, \"cluster_size\": 1, \"template_mined\": \"Connection refused: no further information\", \"cluster_count\": 32}\n",
      "Input (215): Failed to connect to /10.86.169.121:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection refused: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 33, \"cluster_size\": 1, \"template_mined\": \"Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.ConnectException: Connection refused: no further information\", \"cluster_count\": 33}\n",
      "Input (296): Stopping MapTask metrics system...\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 34, \"cluster_size\": 1, \"template_mined\": \"Stopping MapTask metrics system...\", \"cluster_count\": 34}\n",
      "Input (297): MapTask metrics system stopped.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 3, \"cluster_size\": 6, \"template_mined\": \"MapTask metrics system <:*:>\", \"cluster_count\": 34}\n",
      "Input (298): MapTask metrics system shutdown complete.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 35, \"cluster_size\": 1, \"template_mined\": \"MapTask metrics system shutdown complete.\", \"cluster_count\": 35}\n",
      "Input (299): Created MRAppMaster for application appattempt_1445087491445_0007_000002\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 36, \"cluster_size\": 1, \"template_mined\": \"Created MRAppMaster for application appattempt <:NUM:> <:NUM:> <:NUM:>\", \"cluster_count\": 36}\n",
      "Input (301): Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 7 cluster_timestamp: 1445087491445 } attemptId: 2 } keyId: -1547346236)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 37, \"cluster_size\": 1, \"template_mined\": \"Kind: YARN AM RM TOKEN, Service: , Ident: (appAttemptId { application id { id: <:NUM:> cluster timestamp: <:NUM:> } attemptId: <:NUM:> } keyId: <:NUM:>)\", \"cluster_count\": 37}\n",
      "Input (302): Using mapred newApiCommitter.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 38, \"cluster_size\": 1, \"template_mined\": \"Using mapred newApiCommitter.\", \"cluster_count\": 38}\n",
      "Input (303): OutputCommitter set in config null\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 39, \"cluster_size\": 1, \"template_mined\": \"OutputCommitter set in config null\", \"cluster_count\": 39}\n",
      "Input (304): OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 40, \"cluster_size\": 1, \"template_mined\": \"OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\", \"cluster_count\": 40}\n",
      "Input (305): Registering class org.apache.hadoop.mapreduce.jobhistory.EventType for class org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 41, \"cluster_size\": 1, \"template_mined\": \"Registering class org.apache.hadoop.mapreduce.jobhistory.EventType for class org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler\", \"cluster_count\": 41}\n",
      "Input (306): Registering class org.apache.hadoop.mapreduce.v2.app.job.event.JobEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 41, \"cluster_size\": 2, \"template_mined\": \"Registering class <:*:> for class <:*:>\", \"cluster_count\": 41}\n",
      "Input (313): Default file system [hdfs://msra-sa-41:9000]\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 42, \"cluster_size\": 1, \"template_mined\": \"Default file system [hdfs://msra-sa-<:NUM:>:<:NUM:>]\", \"cluster_count\": 42}\n",
      "Input (316): Emitting job history data to the timeline server is not enabled\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 43, \"cluster_size\": 1, \"template_mined\": \"Emitting job history data to the timeline server is not enabled\", \"cluster_count\": 43}\n",
      "Input (317): Recovery is enabled. Will try to recover from previous life on best effort basis.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 44, \"cluster_size\": 1, \"template_mined\": \"Recovery is enabled. Will try to recover from previous life on best effort basis.\", \"cluster_count\": 44}\n",
      "Input (319): Previous history file is at hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007/job_1445087491445_0007_1.jhist\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 45, \"cluster_size\": 1, \"template_mined\": \"Previous history file is at hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist\", \"cluster_count\": 45}\n",
      "Input (320): Read from history task task_1445087491445_0007_m_000001\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 46, \"cluster_size\": 1, \"template_mined\": \"Read from history task task <:NUM:> <:NUM:> m <:NUM:>\", \"cluster_count\": 46}\n",
      "Input (330): Read completed tasks from history 10\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 47, \"cluster_size\": 1, \"template_mined\": \"Read completed tasks from history <:NUM:>\", \"cluster_count\": 47}\n",
      "Input (334): MRAppMaster metrics system started\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 48, \"cluster_size\": 1, \"template_mined\": \"MRAppMaster metrics system started\", \"cluster_count\": 48}\n",
      "Input (335): Adding job token for job_1445087491445_0007 to jobTokenSecretManager\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 49, \"cluster_size\": 1, \"template_mined\": \"Adding job token for job <:NUM:> <:NUM:> to jobTokenSecretManager\", \"cluster_count\": 49}\n",
      "Input (336): Not uberizing job_1445087491445_0007 because: not enabled; too many maps; too much input;\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 50, \"cluster_size\": 1, \"template_mined\": \"Not uberizing job <:NUM:> <:NUM:> because: not enabled; too many maps; too much input;\", \"cluster_count\": 50}\n",
      "Input (337): Input size for job job_1445087491445_0007 = 1313861632. Number of splits = 10\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 51, \"cluster_size\": 1, \"template_mined\": \"Input size for job job <:NUM:> <:NUM:> = <:NUM:>. Number of splits = <:NUM:>\", \"cluster_count\": 51}\n",
      "Input (338): Number of reduces for job job_1445087491445_0007 = 1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 52, \"cluster_size\": 1, \"template_mined\": \"Number of reduces for job job <:NUM:> <:NUM:> = <:NUM:>\", \"cluster_count\": 52}\n",
      "Input (339): job_1445087491445_0007Job Transitioned from NEW to INITED\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 53, \"cluster_size\": 1, \"template_mined\": \"job <:NUM:> 0007Job Transitioned from NEW to INITED\", \"cluster_count\": 53}\n",
      "Input (340): MRAppMaster launching normal, non-uberized, multi-container job job_1445087491445_0007.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 54, \"cluster_size\": 1, \"template_mined\": \"MRAppMaster launching normal, non-uberized, multi-container job job <:NUM:> <:NUM:>.\", \"cluster_count\": 54}\n",
      "Input (341): Using callQueue class java.util.concurrent.LinkedBlockingQueue\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 55, \"cluster_size\": 1, \"template_mined\": \"Using callQueue class java.util.concurrent.LinkedBlockingQueue\", \"cluster_count\": 55}\n",
      "Input (342): Starting Socket Reader #1 for port 24281\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 56, \"cluster_size\": 1, \"template_mined\": \"Starting Socket Reader #<:NUM:> for port <:NUM:>\", \"cluster_count\": 56}\n",
      "Input (343): Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 57, \"cluster_size\": 1, \"template_mined\": \"Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server\", \"cluster_count\": 57}\n",
      "Input (344): IPC Server listener on 24281: starting\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 58, \"cluster_size\": 1, \"template_mined\": \"IPC Server listener on <:NUM:>: starting\", \"cluster_count\": 58}\n",
      "Input (345): IPC Server Responder: starting\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 59, \"cluster_size\": 1, \"template_mined\": \"IPC Server Responder: starting\", \"cluster_count\": 59}\n",
      "Input (346): Instantiated MRClientService at MSRA-SA-41.fareast.corp.microsoft.com/10.190.173.170:24281\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 60, \"cluster_size\": 1, \"template_mined\": \"Instantiated MRClientService at MSRA-SA-<:NUM:>.fareast.corp.microsoft.com/<:IP:>:<:NUM:>\", \"cluster_count\": 60}\n",
      "Input (347): Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 61, \"cluster_size\": 1, \"template_mined\": \"Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\", \"cluster_count\": 61}\n",
      "Input (348): Http request log for http.requests.mapreduce is not defined\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 62, \"cluster_size\": 1, \"template_mined\": \"Http request log for http.requests.mapreduce is not defined\", \"cluster_count\": 62}\n",
      "Input (349): Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 63, \"cluster_size\": 1, \"template_mined\": \"Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\", \"cluster_count\": 63}\n",
      "Input (350): Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context mapreduce\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 64, \"cluster_size\": 1, \"template_mined\": \"Added filter AM PROXY FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context mapreduce\", \"cluster_count\": 64}\n",
      "Input (351): Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context static\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 64, \"cluster_size\": 2, \"template_mined\": \"Added filter AM PROXY FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context <:*:>\", \"cluster_count\": 64}\n",
      "Input (352): adding path spec: /mapreduce/*\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 65, \"cluster_size\": 1, \"template_mined\": \"adding path spec: /mapreduce/*\", \"cluster_count\": 65}\n",
      "Input (353): adding path spec: /ws/*\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 65, \"cluster_size\": 2, \"template_mined\": \"adding path spec: <:*:>\", \"cluster_count\": 65}\n",
      "Input (354): Jetty bound to port 24289\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 66, \"cluster_size\": 1, \"template_mined\": \"Jetty bound to port <:NUM:>\", \"cluster_count\": 66}\n",
      "Input (355): jetty-6.1.26\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 67, \"cluster_size\": 1, \"template_mined\": \"jetty-<:NUM:>.<:NUM:>.<:NUM:>\", \"cluster_count\": 67}\n",
      "Input (356): Extract jar:file:/D:/hadoop-2.6.0-localbox/share/hadoop/yarn/hadoop-yarn-common-2.6.0-SNAPSHOT.jar!/webapps/mapreduce to C:\\Users\\msrabi\\AppData\\Local\\Temp\\2\\Jetty_0_0_0_0_24289_mapreduce____.6ucjbs\\webapp\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 68, \"cluster_size\": 1, \"template_mined\": \"Extract jar:file:/D:/hadoop-<:NUM:>.<:NUM:>.<:NUM:>-localbox/share/hadoop/yarn/hadoop-yarn-common-<:NUM:>.<:NUM:>.<:NUM:>-SNAPSHOT.jar!/webapps/mapreduce to C:\\\\Users\\\\msrabi\\\\AppData\\\\Local\\\\Temp\\\\<:NUM:>\\\\Jetty <:NUM:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> mapreduce .6ucjbs\\\\webapp\", \"cluster_count\": 68}\n",
      "Input (357): Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:24289\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 69, \"cluster_size\": 1, \"template_mined\": \"Started HttpServer2$SelectChannelConnectorWithSafeStartup@<:IP:>:<:NUM:>\", \"cluster_count\": 69}\n",
      "Input (358): Web app /mapreduce started at 24289\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 70, \"cluster_size\": 1, \"template_mined\": \"Web app /mapreduce started at <:NUM:>\", \"cluster_count\": 70}\n",
      "Input (359): Registered webapp guice modules\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 71, \"cluster_size\": 1, \"template_mined\": \"Registered webapp guice modules\", \"cluster_count\": 71}\n",
      "Input (360): JOB_CREATE job_1445087491445_0007\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 72, \"cluster_size\": 1, \"template_mined\": \"JOB CREATE job <:NUM:> <:NUM:>\", \"cluster_count\": 72}\n",
      "Input (365): nodeBlacklistingEnabled:true\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 73, \"cluster_size\": 1, \"template_mined\": \"nodeBlacklistingEnabled:true\", \"cluster_count\": 73}\n",
      "Input (366): maxTaskFailuresPerNode is 3\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 74, \"cluster_size\": 1, \"template_mined\": \"maxTaskFailuresPerNode is <:NUM:>\", \"cluster_count\": 74}\n",
      "Input (367): blacklistDisablePercent is 33\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 75, \"cluster_size\": 1, \"template_mined\": \"blacklistDisablePercent is <:NUM:>\", \"cluster_count\": 75}\n",
      "Input (368): Connecting to ResourceManager at MSRA-SA-41/10.190.173.170:8030\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 76, \"cluster_size\": 1, \"template_mined\": \"Connecting to ResourceManager at MSRA-SA-<:NUM:>/<:IP:>:<:NUM:>\", \"cluster_count\": 76}\n",
      "Input (369): maxContainerCapability: <memory:8192, vCores:32>\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 77, \"cluster_size\": 1, \"template_mined\": \"maxContainerCapability: <memory:<:NUM:>, vCores:<:NUM:>>\", \"cluster_count\": 77}\n",
      "Input (370): queue: default\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 78, \"cluster_size\": 1, \"template_mined\": \"queue: default\", \"cluster_count\": 78}\n",
      "Input (371): Upper limit on the thread pool size is 500\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 79, \"cluster_size\": 1, \"template_mined\": \"Upper limit on the thread pool size is <:NUM:>\", \"cluster_count\": 79}\n",
      "Input (372): yarn.client.max-cached-nodemanagers-proxies : 0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 80, \"cluster_size\": 1, \"template_mined\": \"yarn.client.max-cached-nodemanagers-proxies : <:NUM:>\", \"cluster_count\": 80}\n",
      "Input (373): job_1445087491445_0007Job Transitioned from INITED to SETUP\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 53, \"cluster_size\": 2, \"template_mined\": \"job <:NUM:> 0007Job Transitioned from <:*:> to <:*:>\", \"cluster_count\": 80}\n",
      "Input (374): Processing the event EventType: JOB_SETUP\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 81, \"cluster_size\": 1, \"template_mined\": \"Processing the event EventType: JOB SETUP\", \"cluster_count\": 81}\n",
      "Input (376): Recovering task task_1445087491445_0007_m_000000 from prior app attempt, status was SUCCEEDED\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 82, \"cluster_size\": 1, \"template_mined\": \"Recovering task task <:NUM:> <:NUM:> m <:NUM:> from prior app attempt, status was SUCCEEDED\", \"cluster_count\": 82}\n",
      "Input (377): Resolved MININT-75DGDAM1.fareast.corp.microsoft.com to /default-rack\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 83, \"cluster_size\": 1, \"template_mined\": \"Resolved MININT-75DGDAM1.fareast.corp.microsoft.com to /default-rack\", \"cluster_count\": 83}\n",
      "Input (378): Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 83, \"cluster_size\": 2, \"template_mined\": \"Resolved <:*:> to /default-rack\", \"cluster_count\": 83}\n",
      "Input (380): TaskAttempt: [attempt_1445087491445_0007_m_000000_0] using containerId: [container_1445087491445_0007_01_000002 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:30535]\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 84, \"cluster_size\": 1, \"template_mined\": \"TaskAttempt: [attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>] using containerId: [container <:NUM:> <:NUM:> <:NUM:> <:NUM:> on NM: [MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>]\", \"cluster_count\": 84}\n",
      "Input (381): attempt_1445087491445_0007_m_000000_0 TaskAttempt Transitioned from NEW to SUCCEEDED\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 85, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> TaskAttempt Transitioned from NEW to SUCCEEDED\", \"cluster_count\": 85}\n",
      "Input (382): Task succeeded with attempt attempt_1445087491445_0007_m_000000_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 86, \"cluster_size\": 1, \"template_mined\": \"Task succeeded with attempt attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 86}\n",
      "Input (383): task_1445087491445_0007_m_000000 Task Transitioned from NEW to SUCCEEDED\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 87, \"cluster_size\": 1, \"template_mined\": \"task <:NUM:> <:NUM:> m <:NUM:> Task Transitioned from NEW to SUCCEEDED\", \"cluster_count\": 87}\n",
      "Input (393): Event Writer setup for JobId: job_1445087491445_0007, File: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007/job_1445087491445_0007_2.jhist\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 88, \"cluster_size\": 1, \"template_mined\": \"Event Writer setup for JobId: job <:NUM:> <:NUM:>, File: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist\", \"cluster_count\": 88}\n",
      "Input (457): task_1445087491445_0007_r_000000 Task Transitioned from NEW to SCHEDULED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 87, \"cluster_size\": 11, \"template_mined\": \"task <:NUM:> <:NUM:> <:*:> <:NUM:> Task Transitioned from NEW to <:*:>\", \"cluster_count\": 88}\n",
      "Input (458): Num completed Tasks: 1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 89, \"cluster_size\": 1, \"template_mined\": \"Num completed Tasks: <:NUM:>\", \"cluster_count\": 89}\n",
      "Input (468): attempt_1445087491445_0007_r_000000_1000 TaskAttempt Transitioned from NEW to UNASSIGNED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 85, \"cluster_size\": 11, \"template_mined\": \"attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from NEW to <:*:>\", \"cluster_count\": 89}\n",
      "Input (469): reduceResourceRequest:<memory:1024, vCores:1>\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 90, \"cluster_size\": 1, \"template_mined\": \"reduceResourceRequest:<memory:<:NUM:>, vCores:<:NUM:>>\", \"cluster_count\": 90}\n",
      "Input (470): Before Scheduling: PendingReds:1 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:10 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 91, \"cluster_size\": 1, \"template_mined\": \"Before Scheduling: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\", \"cluster_count\": 91}\n",
      "Input (471): Recalculating schedule, headroom=<memory:16384, vCores:-26>\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 92, \"cluster_size\": 1, \"template_mined\": \"Recalculating schedule, headroom=<memory:<:NUM:>, vCores:<:NUM:>>\", \"cluster_count\": 92}\n",
      "Input (472): Reduce slow start threshold reached. Scheduling reduces.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 93, \"cluster_size\": 1, \"template_mined\": \"Reduce slow start threshold reached. Scheduling reduces.\", \"cluster_count\": 93}\n",
      "Input (473): All maps assigned. Ramping up all remaining reduces:1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 94, \"cluster_size\": 1, \"template_mined\": \"All maps assigned. Ramping up all remaining reduces:<:NUM:>\", \"cluster_count\": 94}\n",
      "Input (474): After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:1 AssignedMaps:0 AssignedReds:0 CompletedMaps:10 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 95, \"cluster_size\": 1, \"template_mined\": \"After Scheduling: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\", \"cluster_count\": 95}\n",
      "Input (475): getResources() for application_1445087491445_0007: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:16384, vCores:-26> knownNMs=6\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 96, \"cluster_size\": 1, \"template_mined\": \"getResources() for application <:NUM:> <:NUM:>: ask=<:NUM:> release= <:NUM:> newContainers=<:NUM:> finishedContainers=<:NUM:> resourcelimit=<memory:<:NUM:>, vCores:<:NUM:>> knownNMs=<:NUM:>\", \"cluster_count\": 96}\n",
      "Input (476): Got allocated containers 1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 97, \"cluster_size\": 1, \"template_mined\": \"Got allocated containers <:NUM:>\", \"cluster_count\": 97}\n",
      "Input (477): Assigned to reduce\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 98, \"cluster_size\": 1, \"template_mined\": \"Assigned to reduce\", \"cluster_count\": 98}\n",
      "Input (478): Assigned container container_1445087491445_0007_02_000003 to attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 99, \"cluster_size\": 1, \"template_mined\": \"Assigned container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> to attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 99}\n",
      "Input (481): The job-jar file on the remote FS is hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007/job.jar\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 100, \"cluster_size\": 1, \"template_mined\": \"The job-jar file on the remote FS is hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job.jar\", \"cluster_count\": 100}\n",
      "Input (482): The job-conf file on the remote FS is /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007/job.xml\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 100, \"cluster_size\": 2, \"template_mined\": \"The <:*:> file on the remote FS is <:*:> <:NUM:> <:*:>\", \"cluster_count\": 100}\n",
      "Input (483): Adding #0 tokens and #1 secret keys for NM use for launching container\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 101, \"cluster_size\": 1, \"template_mined\": \"Adding #<:NUM:> tokens and #<:NUM:> secret keys for NM use for launching container\", \"cluster_count\": 101}\n",
      "Input (484): Size of containertokens_dob is 1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 102, \"cluster_size\": 1, \"template_mined\": \"Size of containertokens dob is <:NUM:>\", \"cluster_count\": 102}\n",
      "Input (485): Putting shuffle token in serviceData\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 103, \"cluster_size\": 1, \"template_mined\": \"Putting shuffle token in serviceData\", \"cluster_count\": 103}\n",
      "Input (486): attempt_1445087491445_0007_r_000000_1000 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 85, \"cluster_size\": 12, \"template_mined\": \"attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> to <:*:>\", \"cluster_count\": 103}\n",
      "Input (487): Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445087491445_0007_02_000003 taskAttempt attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 104, \"cluster_size\": 1, \"template_mined\": \"Processing the event EventType: CONTAINER REMOTE LAUNCH for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> taskAttempt attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 104}\n",
      "Input (488): Launching attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 105, \"cluster_size\": 1, \"template_mined\": \"Launching attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 105}\n",
      "Input (489): Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:49130\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 106, \"cluster_size\": 1, \"template_mined\": \"Opening proxy : MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>\", \"cluster_count\": 106}\n",
      "Input (490): Shuffle port returned by ContainerManager for attempt_1445087491445_0007_r_000000_1000 : 13562\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 107, \"cluster_size\": 1, \"template_mined\": \"Shuffle port returned by ContainerManager for attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> : <:NUM:>\", \"cluster_count\": 107}\n",
      "Input (491): TaskAttempt: [attempt_1445087491445_0007_r_000000_1000] using containerId: [container_1445087491445_0007_02_000003 on NM: [MSRA-SA-39.fareast.corp.microsoft.com:49130]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 84, \"cluster_size\": 11, \"template_mined\": \"TaskAttempt: [attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>] using containerId: [container <:NUM:> <:NUM:> <:NUM:> <:NUM:> on NM: [MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>]\", \"cluster_count\": 107}\n",
      "Input (493): ATTEMPT_START task_1445087491445_0007_r_000000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 108, \"cluster_size\": 1, \"template_mined\": \"ATTEMPT START task <:NUM:> <:NUM:> r <:NUM:>\", \"cluster_count\": 108}\n",
      "Input (494): task_1445087491445_0007_r_000000 Task Transitioned from SCHEDULED to RUNNING\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 87, \"cluster_size\": 12, \"template_mined\": \"task <:NUM:> <:NUM:> <:*:> <:NUM:> Task Transitioned from <:*:> to <:*:>\", \"cluster_count\": 108}\n",
      "Input (496): Auth successful for job_1445087491445_0007 (auth:SIMPLE)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 109, \"cluster_size\": 1, \"template_mined\": \"Auth successful for job <:NUM:> <:NUM:> (auth:SIMPLE)\", \"cluster_count\": 109}\n",
      "Input (497): JVM with ID : jvm_1445087491445_0007_r_000003 asked for a task\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 110, \"cluster_size\": 1, \"template_mined\": \"JVM with ID : jvm <:NUM:> <:NUM:> r <:NUM:> asked for a task\", \"cluster_count\": 110}\n",
      "Input (498): JVM with ID: jvm_1445087491445_0007_r_000003 given task: attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 111, \"cluster_size\": 1, \"template_mined\": \"JVM with ID: jvm <:NUM:> <:NUM:> r <:NUM:> given task: attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 111}\n",
      "Input (499): MapCompletionEvents request from attempt_1445087491445_0007_r_000000_1000. startIndex 0 maxEvents 10000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 112, \"cluster_size\": 1, \"template_mined\": \"MapCompletionEvents request from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>. startIndex <:NUM:> maxEvents <:NUM:>\", \"cluster_count\": 112}\n",
      "Input (505): Progress of TaskAttempt attempt_1445087491445_0007_r_000000_1000 is : 0.13333334\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 113, \"cluster_size\": 1, \"template_mined\": \"Progress of TaskAttempt attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> is : <:NUM:>.<:NUM:>\", \"cluster_count\": 113}\n",
      "Input (592): DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk#1073743028_2240\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 114, \"cluster_size\": 1, \"template_mined\": \"DFSOutputStream ResponseProcessor exception for block BP-<:NUM:>-<:IP:><:NUM:>:blk#<:NUM:> <:NUM:>\", \"cluster_count\": 114}\n",
      "Input (593): Bad response ERROR for block BP-1347369012-10.190.173.170-1444972147527:blk#1073743028_2240 from datanode 10.86.169.121:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 115, \"cluster_size\": 1, \"template_mined\": \"Bad response ERROR for block BP-<:NUM:>-<:IP:><:NUM:>:blk#<:NUM:> <:NUM:> from datanode <:IP:>:<:NUM:>\", \"cluster_count\": 115}\n",
      "Input (594): Error Recovery for block BP-1347369012-10.190.173.170-1444972147527:blk#1073743028_2240 in pipeline 10.190.173.170:50010, 10.86.169.121:50010: bad datanode 10.86.169.121:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 116, \"cluster_size\": 1, \"template_mined\": \"Error Recovery for block BP-<:NUM:>-<:IP:><:NUM:>:blk#<:NUM:> <:NUM:> in pipeline <:IP:>:<:NUM:>, <:IP:>:<:NUM:>: bad datanode <:IP:>:<:NUM:>\", \"cluster_count\": 116}\n",
      "Input (711): Commit-pending state update from attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 117, \"cluster_size\": 1, \"template_mined\": \"Commit-pending state update from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 117}\n",
      "Input (712): attempt_1445087491445_0007_r_000000_1000 TaskAttempt Transitioned from RUNNING to COMMIT_PENDING\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 118, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> TaskAttempt Transitioned from RUNNING to COMMIT PENDING\", \"cluster_count\": 118}\n",
      "Input (713): attempt_1445087491445_0007_r_000000_1000 given a go for committing the task output.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 119, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> given a go for committing the task output.\", \"cluster_count\": 119}\n",
      "Input (714): Commit go/no-go request from attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 120, \"cluster_size\": 1, \"template_mined\": \"Commit go/no-go request from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 120}\n",
      "Input (715): Result of canCommit for attempt_1445087491445_0007_r_000000_1000:true\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 121, \"cluster_size\": 1, \"template_mined\": \"Result of canCommit for attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>:true\", \"cluster_count\": 121}\n",
      "Input (717): Done acknowledgement from attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 122, \"cluster_size\": 1, \"template_mined\": \"Done acknowledgement from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 122}\n",
      "Input (718): attempt_1445087491445_0007_r_000000_1000 TaskAttempt Transitioned from COMMIT_PENDING to SUCCESS_CONTAINER_CLEANUP\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 123, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> TaskAttempt Transitioned from COMMIT PENDING to SUCCESS CONTAINER CLEANUP\", \"cluster_count\": 123}\n",
      "Input (719): Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445087491445_0007_02_000003 taskAttempt attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 104, \"cluster_size\": 2, \"template_mined\": \"Processing the event EventType: CONTAINER REMOTE <:*:> for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> taskAttempt attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 123}\n",
      "Input (720): KILLING attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 124, \"cluster_size\": 1, \"template_mined\": \"KILLING attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 124}\n",
      "Input (722): attempt_1445087491445_0007_r_000000_1000 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 119, \"cluster_size\": 2, \"template_mined\": \"attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:>\", \"cluster_count\": 124}\n",
      "Input (723): Task succeeded with attempt attempt_1445087491445_0007_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 86, \"cluster_size\": 11, \"template_mined\": \"Task succeeded with attempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 124}\n",
      "Input (727): Processing the event EventType: JOB_COMMIT\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 81, \"cluster_size\": 2, \"template_mined\": \"Processing the event EventType: JOB <:*:>\", \"cluster_count\": 124}\n",
      "Input (728): Calling handler for JobFinishedEvent\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 125, \"cluster_size\": 1, \"template_mined\": \"Calling handler for JobFinishedEvent\", \"cluster_count\": 125}\n",
      "Input (730): We are finishing cleanly so this is the last retry\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 126, \"cluster_size\": 1, \"template_mined\": \"We are finishing cleanly so this is the last retry\", \"cluster_count\": 126}\n",
      "Input (731): Notify RMCommunicator isAMLastRetry: true\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 127, \"cluster_size\": 1, \"template_mined\": \"Notify RMCommunicator isAMLastRetry: true\", \"cluster_count\": 127}\n",
      "Input (732): RMCommunicator notified that shouldUnregistered is: true\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 128, \"cluster_size\": 1, \"template_mined\": \"RMCommunicator notified that shouldUnregistered is: true\", \"cluster_count\": 128}\n",
      "Input (733): Notify JHEH isAMLastRetry: true\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 127, \"cluster_size\": 2, \"template_mined\": \"Notify <:*:> isAMLastRetry: true\", \"cluster_count\": 128}\n",
      "Input (734): JobHistoryEventHandler notified that forceJobCompletion is true\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 129, \"cluster_size\": 1, \"template_mined\": \"JobHistoryEventHandler notified that forceJobCompletion is true\", \"cluster_count\": 129}\n",
      "Input (735): Calling stop for all the services\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 130, \"cluster_size\": 1, \"template_mined\": \"Calling stop for all the services\", \"cluster_count\": 130}\n",
      "Input (736): Stopping JobHistoryEventHandler. Size of the outstanding queue size is 0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 131, \"cluster_size\": 1, \"template_mined\": \"Stopping JobHistoryEventHandler. Size of the outstanding queue size is <:NUM:>\", \"cluster_count\": 131}\n",
      "Input (738): Copying hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007/job_1445087491445_0007_2.jhist to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 132, \"cluster_size\": 1, \"template_mined\": \"Copying hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist to hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\", \"cluster_count\": 132}\n",
      "Input (739): Copied to done location: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 133, \"cluster_size\": 1, \"template_mined\": \"Copied to done location: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\", \"cluster_count\": 133}\n",
      "Input (740): Copying hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007/job_1445087491445_0007_2_conf.xml to hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 134, \"cluster_size\": 1, \"template_mined\": \"Copying hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:> conf.xml to hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\", \"cluster_count\": 134}\n",
      "Input (742): Moved tmp to done: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 135, \"cluster_size\": 1, \"template_mined\": \"Moved tmp to done: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\", \"cluster_count\": 135}\n",
      "Input (745): Stopped JobHistoryEventHandler. super.stop()\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 136, \"cluster_size\": 1, \"template_mined\": \"Stopped JobHistoryEventHandler. super.stop()\", \"cluster_count\": 136}\n",
      "Input (746): Setting job diagnostics to\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 137, \"cluster_size\": 1, \"template_mined\": \"Setting job diagnostics to\", \"cluster_count\": 137}\n",
      "Input (747): History url is http://MSRA-SA-41.fareast.corp.microsoft.com:19888/jobhistory/job/job_1445087491445_0007\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 138, \"cluster_size\": 1, \"template_mined\": \"History url is http://MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>/jobhistory/job/job <:NUM:> <:NUM:>\", \"cluster_count\": 138}\n",
      "Input (748): Waiting for application to be successfully unregistered.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 139, \"cluster_size\": 1, \"template_mined\": \"Waiting for application to be successfully unregistered.\", \"cluster_count\": 139}\n",
      "Input (749): Final Stats: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:1 CompletedMaps:10 CompletedReds:1 ContAlloc:1 ContRel:0 HostLocal:0 RackLocal:0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 140, \"cluster_size\": 1, \"template_mined\": \"Final Stats: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\", \"cluster_count\": 140}\n",
      "Input (750): Deleting staging directory hdfs://msra-sa-41:9000 /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445087491445_0007\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 141, \"cluster_size\": 1, \"template_mined\": \"Deleting staging directory hdfs://msra-sa-<:NUM:>:<:NUM:> /tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>\", \"cluster_count\": 141}\n",
      "Input (751): Stopping server on 24300\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 142, \"cluster_size\": 1, \"template_mined\": \"Stopping server on <:NUM:>\", \"cluster_count\": 142}\n",
      "Input (752): Stopping IPC Server listener on 24300\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 143, \"cluster_size\": 1, \"template_mined\": \"Stopping IPC Server listener on <:NUM:>\", \"cluster_count\": 143}\n",
      "Input (753): TaskHeartbeatHandler thread interrupted\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 144, \"cluster_size\": 1, \"template_mined\": \"TaskHeartbeatHandler thread interrupted\", \"cluster_count\": 144}\n",
      "Input (754): Stopping IPC Server Responder\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 145, \"cluster_size\": 1, \"template_mined\": \"Stopping IPC Server Responder\", \"cluster_count\": 145}\n",
      "Input (1439): job_1445182159119_0016Job Transitioned from NEW to INITED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 53, \"cluster_size\": 6, \"template_mined\": \"job <:NUM:> <:*:> Transitioned from <:*:> to <:*:>\", \"cluster_count\": 145}\n",
      "Input (1456): Extract jar:file:/D:/hadoop-2.6.0-localbox/share/hadoop/yarn/hadoop-yarn-common-2.6.0-SNAPSHOT.jar!/webapps/mapreduce to C:\\Users\\msrabi\\AppData\\Local\\Temp\\2\\Jetty_0_0_0_0_55793_mapreduce____.11rul4\\webapp\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 68, \"cluster_size\": 2, \"template_mined\": \"Extract jar:file:/D:/hadoop-<:NUM:>.<:NUM:>.<:NUM:>-localbox/share/hadoop/yarn/hadoop-yarn-common-<:NUM:>.<:NUM:>.<:NUM:>-SNAPSHOT.jar!/webapps/mapreduce to C:\\\\Users\\\\msrabi\\\\AppData\\\\Local\\\\Temp\\\\<:NUM:>\\\\Jetty <:NUM:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> mapreduce <:*:>\", \"cluster_count\": 145}\n",
      "Input (1468): Connecting to ResourceManager at msra-sa-41/10.190.173.170:8030\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 76, \"cluster_size\": 2, \"template_mined\": \"Connecting to ResourceManager at <:*:>\", \"cluster_count\": 145}\n",
      "Input (1518): mapResourceRequest:<memory:1024, vCores:1>\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 146, \"cluster_size\": 1, \"template_mined\": \"mapResourceRequest:<memory:<:NUM:>, vCores:<:NUM:>>\", \"cluster_count\": 146}\n",
      "Input (1524): Reduce slow start threshold not met. completedMapsForReduceSlowstart 1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 147, \"cluster_size\": 1, \"template_mined\": \"Reduce slow start threshold not met. completedMapsForReduceSlowstart <:NUM:>\", \"cluster_count\": 147}\n",
      "Input (1526): Assigned container container_1445182159119_0016_01_000002 to attempt_1445182159119_0016_m_000000_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 99, \"cluster_size\": 2, \"template_mined\": \"Assigned container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> to attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 147}\n",
      "Input (1546): Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445182159119_0016_01_000002 taskAttempt attempt_1445182159119_0016_m_000000_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 104, \"cluster_size\": 3, \"template_mined\": \"Processing the event EventType: CONTAINER REMOTE <:*:> for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> taskAttempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 147}\n",
      "Input (1550): Launching attempt_1445182159119_0016_m_000003_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 105, \"cluster_size\": 2, \"template_mined\": \"Launching attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 147}\n",
      "Input (1558): Shuffle port returned by ContainerManager for attempt_1445182159119_0016_m_000002_0 : 13562\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 107, \"cluster_size\": 2, \"template_mined\": \"Shuffle port returned by ContainerManager for attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> : <:NUM:>\", \"cluster_count\": 147}\n",
      "Input (1570): ATTEMPT_START task_1445182159119_0016_m_000003\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 108, \"cluster_size\": 2, \"template_mined\": \"ATTEMPT START task <:NUM:> <:NUM:> <:*:> <:NUM:>\", \"cluster_count\": 147}\n",
      "Input (1593): Opening proxy : MININT-FNANLI5.fareast.corp.microsoft.com:59190\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 106, \"cluster_size\": 7, \"template_mined\": \"Opening proxy : <:*:>\", \"cluster_count\": 147}\n",
      "Input (1595): TaskAttempt: [attempt_1445182159119_0016_m_000004_0] using containerId: [container_1445182159119_0016_01_000006 on NM: [MININT-FNANLI5.fareast.corp.microsoft.com:59190]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 84, \"cluster_size\": 16, \"template_mined\": \"TaskAttempt: [attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>] using containerId: [container <:NUM:> <:NUM:> <:NUM:> <:NUM:> on NM: <:*:>\", \"cluster_count\": 147}\n",
      "Input (1617): JVM with ID : jvm_1445182159119_0016_m_000002 asked for a task\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 110, \"cluster_size\": 2, \"template_mined\": \"JVM with ID : jvm <:NUM:> <:NUM:> <:*:> <:NUM:> asked for a task\", \"cluster_count\": 147}\n",
      "Input (1618): JVM with ID: jvm_1445182159119_0016_m_000002 given task: attempt_1445182159119_0016_m_000000_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 111, \"cluster_size\": 2, \"template_mined\": \"JVM with ID: jvm <:NUM:> <:NUM:> <:*:> <:NUM:> given task: attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 147}\n",
      "Input (1696): Cannot assign container Container: [ContainerId: container_1445182159119_0016_01_000012, NodeId: MININT-75DGDAM1.fareast.corp.microsoft.com:58081, NodeHttpAddress: MININT-75DGDAM1.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.86.165.66:58081 }, ] for a map as either  container memory less than required <memory:1024, vCores:1> or no pending map tasks - maps.isEmpty=true\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 148, \"cluster_size\": 1, \"template_mined\": \"Cannot assign container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: MININT-75DGDAM1.fareast.corp.microsoft.com:<:NUM:>, NodeHttpAddress: MININT-75DGDAM1.fareast.corp.microsoft.com:<:NUM:>, Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] for a map as either container memory less than required <memory:<:NUM:>, vCores:<:NUM:>> or no pending map tasks - maps.isEmpty=true\", \"cluster_count\": 148}\n",
      "Input (1697): Cannot assign container Container: [ContainerId: container_1445182159119_0016_01_000013, NodeId: MININT-FNANLI5.fareast.corp.microsoft.com:59190, NodeHttpAddress: MININT-FNANLI5.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.86.169.121:59190 }, ] for a map as either  container memory less than required <memory:1024, vCores:1> or no pending map tasks - maps.isEmpty=true\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 148, \"cluster_size\": 2, \"template_mined\": \"Cannot assign container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] for a map as either container memory less than required <memory:<:NUM:>, vCores:<:NUM:>> or no pending map tasks - maps.isEmpty=true\", \"cluster_count\": 148}\n",
      "Input (1705): Received completed container container_1445182159119_0016_01_000013\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 149, \"cluster_size\": 1, \"template_mined\": \"Received completed container container <:NUM:> <:NUM:> <:NUM:> <:NUM:>\", \"cluster_count\": 149}\n",
      "Input (1706): Container complete event for unknown container id container_1445182159119_0016_01_000013\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 150, \"cluster_size\": 1, \"template_mined\": \"Container complete event for unknown container id container <:NUM:> <:NUM:> <:NUM:> <:NUM:>\", \"cluster_count\": 150}\n",
      "Input (1719): Progress of TaskAttempt attempt_1445182159119_0016_m_000000_0 is : 0.10291508\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 113, \"cluster_size\": 164, \"template_mined\": \"Progress of TaskAttempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> is : <:NUM:>.<:NUM:>\", \"cluster_count\": 150}\n",
      "Input (2063): Socket Reader #1 for port 55796: readAndProcess from client 10.86.165.66 threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 151, \"cluster_size\": 1, \"template_mined\": \"Socket Reader #<:NUM:> for port <:NUM:>: readAndProcess from client <:IP:> threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]\", \"cluster_count\": 151}\n",
      "Input (2064): An existing connection was forcibly closed by the remote host\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 152, \"cluster_size\": 1, \"template_mined\": \"An existing connection was forcibly closed by the remote host\", \"cluster_count\": 152}\n",
      "Input (2081): Done acknowledgement from attempt_1445182159119_0016_m_000003_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 122, \"cluster_size\": 2, \"template_mined\": \"Done acknowledgement from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 152}\n",
      "Input (2082): attempt_1445182159119_0016_m_000003_0 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 153, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> TaskAttempt Transitioned from RUNNING to SUCCESS CONTAINER CLEANUP\", \"cluster_count\": 153}\n",
      "Input (2084): KILLING attempt_1445182159119_0016_m_000003_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 124, \"cluster_size\": 2, \"template_mined\": \"KILLING attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 153}\n",
      "Input (2086): attempt_1445182159119_0016_m_000003_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 153, \"cluster_size\": 2, \"template_mined\": \"attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> <:*:> <:*:> <:*:> <:*:>\", \"cluster_count\": 153}\n",
      "Input (2090): DefaultSpeculator.addSpeculativeAttempt -- we are speculating task_1445182159119_0016_m_000004\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 154, \"cluster_size\": 1, \"template_mined\": \"DefaultSpeculator.addSpeculativeAttempt -- we are speculating task <:NUM:> <:NUM:> m <:NUM:>\", \"cluster_count\": 154}\n",
      "Input (2091): We launched 1 speculations.  Sleeping 15000 milliseconds.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 155, \"cluster_size\": 1, \"template_mined\": \"We launched <:NUM:> speculations. Sleeping <:NUM:> milliseconds.\", \"cluster_count\": 155}\n",
      "Input (2092): Scheduling a redundant attempt for task task_1445182159119_0016_m_000004\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 156, \"cluster_size\": 1, \"template_mined\": \"Scheduling a redundant attempt for task task <:NUM:> <:NUM:> m <:NUM:>\", \"cluster_count\": 156}\n",
      "Input (2101): completedMapPercent 0.1 totalResourceLimit:<memory:10240, vCores:-17> finalMapResourceLimit:<memory:9216, vCores:-16> finalReduceResourceLimit:<memory:1024, vCores:-1> netScheduledMapResource:<memory:11264, vCores:11> netScheduledReduceResource:<memory:0, vCores:0>\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 157, \"cluster_size\": 1, \"template_mined\": \"completedMapPercent <:NUM:>.<:NUM:> totalResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> finalMapResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> finalReduceResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> netScheduledMapResource:<memory:<:NUM:>, vCores:<:NUM:>> netScheduledReduceResource:<memory:<:NUM:>, vCores:<:NUM:>>\", \"cluster_count\": 157}\n",
      "Input (2102): Ramping up 1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 158, \"cluster_size\": 1, \"template_mined\": \"Ramping up <:NUM:>\", \"cluster_count\": 158}\n",
      "Input (2119): Diagnostics report from attempt_1445182159119_0016_m_000003_0: Container killed by the ApplicationMaster.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 159, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Container killed by the ApplicationMaster.\", \"cluster_count\": 159}\n",
      "Input (2594): Issuing kill to other attempt attempt_1445182159119_0016_m_000004_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 160, \"cluster_size\": 1, \"template_mined\": \"Issuing kill to other attempt attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 160}\n",
      "Input (2621): attempt_1445182159119_0016_m_000004_0 TaskAttempt Transitioned from KILL_CONTAINER_CLEANUP to KILL_TASK_CLEANUP\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 161, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> TaskAttempt Transitioned from KILL CONTAINER CLEANUP to KILL TASK CLEANUP\", \"cluster_count\": 161}\n",
      "Input (2622): Processing the event EventType: TASK_ABORT\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 81, \"cluster_size\": 4, \"template_mined\": \"Processing the event EventType: <:*:> <:*:>\", \"cluster_count\": 161}\n",
      "Input (2623): Could not delete hdfs://msra-sa-41:9000/pageout/out1/_temporary/1/_temporary/attempt_1445182159119_0016_m_000004_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 162, \"cluster_size\": 1, \"template_mined\": \"Could not delete hdfs://msra-sa-<:NUM:>:<:NUM:>/pageout/out1/ temporary/<:NUM:>/ temporary/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 162}\n",
      "Input (3073): Retrying connect to server: MININT-75DGDAM1.fareast.corp.microsoft.com/10.86.165.66:58081. Already tried 0 time(s); maxRetries=45\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 163, \"cluster_size\": 1, \"template_mined\": \"Retrying connect to server: MININT-75DGDAM1.fareast.corp.microsoft.com/<:IP:>:<:NUM:>. Already tried <:NUM:> time(s); maxRetries=<:NUM:>\", \"cluster_count\": 163}\n",
      "Input (3436): Retrying connect to server: MININT-75DGDAM1.fareast.corp.microsoft.com/10.86.165.66:58081. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 164, \"cluster_size\": 1, \"template_mined\": \"Retrying connect to server: MININT-75DGDAM1.fareast.corp.microsoft.com/<:IP:>:<:NUM:>. Already tried <:NUM:> time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=<:NUM:>, sleepTime=<:NUM:> MILLISECONDS)\", \"cluster_count\": 164}\n",
      "Input (3679): Diagnostics report from attempt_1445182159119_0016_m_000009_0: AttemptID:attempt_1445182159119_0016_m_000009_0 Timed out after 600 secs\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 165, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: AttemptID:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> Timed out after <:NUM:> secs\", \"cluster_count\": 165}\n",
      "Input (3759): Killing taskAttempt:attempt_1445182159119_0016_m_000005_0 because it is running on unusable node:MININT-75DGDAM1.fareast.corp.microsoft.com:58081\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 166, \"cluster_size\": 1, \"template_mined\": \"Killing taskAttempt:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> because it is running on unusable node:MININT-75DGDAM1.fareast.corp.microsoft.com:<:NUM:>\", \"cluster_count\": 166}\n",
      "Input (3766): Diagnostics report from attempt_1445182159119_0016_m_000009_0: Container released on a *lost* node\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 167, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Container released on a *lost* node\", \"cluster_count\": 167}\n",
      "Input (5287): Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"04DN8IQ/10.86.164.9\"; destination host is: \"minint-fnanli5.fareast.corp.microsoft.com\":52839;\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 168, \"cluster_size\": 1, \"template_mined\": \"Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \\\"04DN8IQ/<:IP:>\\\"; destination host is: \\\"minint-fnanli5.fareast.corp.microsoft.com\\\":<:NUM:>;\", \"cluster_count\": 168}\n",
      "Input (5288): java.io.IOException: An existing connection was forcibly closed by the remote host\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 169, \"cluster_size\": 1, \"template_mined\": \"java.io.IOException: An existing connection was forcibly closed by the remote host\", \"cluster_count\": 169}\n",
      "Input (5291): Retrying connect to server: minint-fnanli5.fareast.corp.microsoft.com/10.86.169.121:52839. Already tried 0 time(s); maxRetries=45\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 163, \"cluster_size\": 12, \"template_mined\": \"Retrying connect to server: <:*:> Already tried <:NUM:> time(s); maxRetries=<:NUM:>\", \"cluster_count\": 169}\n",
      "Input (5863): ReduceTask metrics system started\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 170, \"cluster_size\": 1, \"template_mined\": \"ReduceTask metrics system started\", \"cluster_count\": 170}\n",
      "Input (5871): Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@64bfc93b\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 171, \"cluster_size\": 1, \"template_mined\": \"Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@64bfc93b\", \"cluster_count\": 171}\n",
      "Input (5872): MergerManager: memoryLimit=130652568, maxSingleShuffleLimit=32663142, mergeThreshold=86230696, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 172, \"cluster_size\": 1, \"template_mined\": \"MergerManager: memoryLimit=<:NUM:>, maxSingleShuffleLimit=<:NUM:>, mergeThreshold=<:NUM:>, ioSortFactor=<:NUM:>, memToMemMergeOutputsThreshold=<:NUM:>\", \"cluster_count\": 172}\n",
      "Input (5874): attempt_1445094324383_0005_r_000000_0: Got 6 new map-outputs\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 173, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>: Got <:NUM:> new map-outputs\", \"cluster_count\": 173}\n",
      "Input (5875): Assigning MSRA-SA-41.fareast.corp.microsoft.com:13562 with 6 to fetcher#4\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 174, \"cluster_size\": 1, \"template_mined\": \"Assigning MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:> with <:NUM:> to fetcher#<:NUM:>\", \"cluster_count\": 174}\n",
      "Input (5876): assigned 6 of 6 to MSRA-SA-41.fareast.corp.microsoft.com:13562 to fetcher#4\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 175, \"cluster_size\": 1, \"template_mined\": \"assigned <:NUM:> of <:NUM:> to MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:> to fetcher#<:NUM:>\", \"cluster_count\": 175}\n",
      "Input (5877): for url=13562/mapOutput?job=job_1445094324383_0005&reduce=0&map=attempt_1445094324383_0005_m_000000_0,attempt_1445094324383_0005_m_000005_0,attempt_1445094324383_0005_m_000003_0,attempt_1445094324383_0005_m_000001_0,attempt_1445094324383_0005_m_000004_0,attempt_1445094324383_0005_m_000002_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 176, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 176}\n",
      "Input (5878): attempt_1445094324383_0005_m_000000_0: Shuffling to disk since 216988123 is greater than maxSingleShuffleLimit (32663142)\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 177, \"cluster_size\": 1, \"template_mined\": \"attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Shuffling to disk since <:NUM:> is greater than maxSingleShuffleLimit (<:NUM:>)\", \"cluster_count\": 177}\n",
      "Input (5879): fetcher#4 about to shuffle output of map attempt_1445094324383_0005_m_000000_0 decomp: 216988123 len: 216988127 to DISK\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 178, \"cluster_size\": 1, \"template_mined\": \"fetcher#<:NUM:> about to shuffle output of map attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> decomp: <:NUM:> len: <:NUM:> to DISK\", \"cluster_count\": 178}\n",
      "Input (5880): Ignoring obsolete output of KILLED map-task: 'attempt_1445094324383_0005_m_000006_1'\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 179, \"cluster_size\": 1, \"template_mined\": \"Ignoring obsolete output of KILLED map-task: 'attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>'\", \"cluster_count\": 179}\n",
      "Input (6147): Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@49c051a1\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 171, \"cluster_size\": 2, \"template_mined\": \"Using ShuffleConsumerPlugin: <:*:>\", \"cluster_count\": 179}\n",
      "Input (6153): for url=13562/mapOutput?job=job_1445094324383_0004&reduce=0&map=attempt_1445094324383_0004_m_000001_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 180, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 180}\n",
      "Input (6254): Instantiated MRClientService at MININT-FNANLI5.fareast.corp.microsoft.com/10.86.169.121:61543\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 60, \"cluster_size\": 3, \"template_mined\": \"Instantiated MRClientService at <:*:>\", \"cluster_count\": 180}\n",
      "Input (6266): Extract jar:file:/D:/hadoop-2.6.0-localbox/share/hadoop/yarn/hadoop-yarn-common-2.6.0-SNAPSHOT.jar!/webapps/mapreduce to C:\\Users\\msrabi\\AppData\\Local\\Temp\\Jetty_0_0_0_0_61550_mapreduce____.mrek2k\\webapp\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 68, \"cluster_size\": 3, \"template_mined\": \"Extract jar:file:/D:/hadoop-<:NUM:>.<:NUM:>.<:NUM:>-localbox/share/hadoop/yarn/hadoop-yarn-common-<:NUM:>.<:NUM:>.<:NUM:>-SNAPSHOT.jar!/webapps/mapreduce to <:*:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> mapreduce <:*:>\", \"cluster_count\": 180}\n",
      "Input (7161): attempt_1445182159119_0001_r_000000_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 153, \"cluster_size\": 48, \"template_mined\": \"attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> <:*:> <:*:> <:*:> <:*:>\", \"cluster_count\": 180}\n",
      "Input (7170): Diagnostics report from attempt_1445182159119_0001_r_000000_0: Container killed by the ApplicationMaster.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 159, \"cluster_size\": 23, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: Container killed by the ApplicationMaster.\", \"cluster_count\": 180}\n",
      "Input (7189): History url is http://MININT-FNANLI5.fareast.corp.microsoft.com:19888/jobhistory/job/job_1445182159119_0001\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 138, \"cluster_size\": 2, \"template_mined\": \"History url is <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 180}\n",
      "Input (8175): Could not delete hdfs://msra-sa-41:9000/pageout/out5/_temporary/1/_temporary/attempt_1445062781478_0020_m_000001_1\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 162, \"cluster_size\": 4, \"template_mined\": \"Could not delete <:*:> temporary/<:NUM:>/ temporary/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 180}\n",
      "Input (9422): Diagnostics report from attempt_1445076437777_0002_m_000006_0:\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 181, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>:\", \"cluster_count\": 181}\n",
      "Processing line: 10000, rate 7197.5 lines/sec, 181 clusters so far.\n",
      "Input (10449): Slow ReadProcessor read fields took 54719ms (threshold=30000ms); ack: seqno: -2 status: SUCCESS status: ERROR downstreamAckTimeNanos: 0, targets: [10.86.164.15:50010, 10.86.169.121:50010]\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 182, \"cluster_size\": 1, \"template_mined\": \"Slow ReadProcessor read fields took 54719ms (threshold=30000ms); ack: seqno: <:NUM:> status: SUCCESS status: ERROR downstreamAckTimeNanos: <:NUM:>, targets: [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>]\", \"cluster_count\": 182}\n",
      "Input (10453): In stop, writing event TASK_FINISHED\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 183, \"cluster_size\": 1, \"template_mined\": \"In stop, writing event TASK FINISHED\", \"cluster_count\": 183}\n",
      "Input (10454): In stop, writing event JOB_FINISHED\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 183, \"cluster_size\": 2, \"template_mined\": \"In stop, writing event <:*:> FINISHED\", \"cluster_count\": 183}\n",
      "Input (13311): Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MININT-75DGDAM1/10.86.165.66\"; destination host is: \"minint-fnanli5.fareast.corp.microsoft.com\":52839;\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 168, \"cluster_size\": 2, \"template_mined\": \"Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: \\\"minint-fnanli5.fareast.corp.microsoft.com\\\":<:NUM:>;\", \"cluster_count\": 183}\n",
      "Input (16143): Read 172334808 bytes from map-output for attempt_1445094324383_0001_m_000009_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 184, \"cluster_size\": 1, \"template_mined\": \"Read <:NUM:> bytes from map-output for attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 184}\n",
      "Input (16144): MSRA-SA-39.fareast.corp.microsoft.com:13562 freed by fetcher#1 in 5943ms\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 185, \"cluster_size\": 1, \"template_mined\": \"MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:> freed by fetcher#<:NUM:> in 5943ms\", \"cluster_count\": 185}\n",
      "Input (16147): for url=13562/mapOutput?job=job_1445094324383_0001&reduce=0&map=attempt_1445094324383_0001_m_000001_0,attempt_1445094324383_0001_m_000008_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 186, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 186}\n",
      "Input (16151): MSRA-SA-41.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 8239ms\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 185, \"cluster_size\": 2, \"template_mined\": \"MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:> freed by fetcher#<:NUM:> in <:*:>\", \"cluster_count\": 186}\n",
      "Input (16154): for url=13562/mapOutput?job=job_1445094324383_0001&reduce=0&map=attempt_1445094324383_0001_m_000003_0,attempt_1445094324383_0001_m_000006_0,attempt_1445094324383_0001_m_000005_0,attempt_1445094324383_0001_m_000004_0,attempt_1445094324383_0001_m_000007_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 187, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 187}\n",
      "Input (16183): EventFetcher is interrupted.. Returning\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 188, \"cluster_size\": 1, \"template_mined\": \"EventFetcher is interrupted.. Returning\", \"cluster_count\": 188}\n",
      "Input (16185): finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 189, \"cluster_size\": 1, \"template_mined\": \"finalMerge called with <:NUM:> in-memory map-outputs and <:NUM:> on-disk map-outputs\", \"cluster_count\": 189}\n",
      "Input (16186): Merging 10 files, 2125289789 bytes from disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 190, \"cluster_size\": 1, \"template_mined\": \"Merging <:NUM:> files, <:NUM:> bytes from disk\", \"cluster_count\": 190}\n",
      "Input (16187): Merging 0 segments, 0 bytes from memory into reduce\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 191, \"cluster_size\": 1, \"template_mined\": \"Merging <:NUM:> segments, <:NUM:> bytes from memory into reduce\", \"cluster_count\": 191}\n",
      "Input (16190): mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 192, \"cluster_size\": 1, \"template_mined\": \"mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\", \"cluster_count\": 192}\n",
      "Input (16191): Task:attempt_1445094324383_0001_r_000000_0 is done. And is in the process of committing\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 26, \"cluster_size\": 36, \"template_mined\": \"Task:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> is done. And is in the process of committing\", \"cluster_count\": 192}\n",
      "Input (16192): Task attempt_1445094324383_0001_r_000000_0 is allowed to commit now\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 193, \"cluster_size\": 1, \"template_mined\": \"Task attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> is allowed to commit now\", \"cluster_count\": 193}\n",
      "Input (16193): Saved output of task 'attempt_1445094324383_0001_r_000000_0' to hdfs://msra-sa-41:9000/out/out4/_temporary/1/task_1445094324383_0001_r_000000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 194, \"cluster_size\": 1, \"template_mined\": \"Saved output of task 'attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>' to hdfs://msra-sa-<:NUM:>:<:NUM:>/out/out4/ temporary/<:NUM:>/task <:NUM:> <:NUM:> r <:NUM:>\", \"cluster_count\": 194}\n",
      "Input (16194): Task 'attempt_1445094324383_0001_r_000000_0' done.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 27, \"cluster_size\": 36, \"template_mined\": \"Task 'attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>' done.\", \"cluster_count\": 194}\n",
      "Input (17131): for url=13562/mapOutput?job=job_1445087491445_0007&reduce=0&map=attempt_1445087491445_0007_m_000006_0,attempt_1445087491445_0007_m_000007_0,attempt_1445087491445_0007_m_000008_0,attempt_1445087491445_0007_m_000009_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 195, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 195}\n",
      "Input (17170): Exception in createBlockOutputStream\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 196, \"cluster_size\": 1, \"template_mined\": \"Exception in createBlockOutputStream\", \"cluster_count\": 196}\n",
      "Input (17171): Bad connect ack with firstBadLink as 10.86.169.121:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 197, \"cluster_size\": 1, \"template_mined\": \"Bad connect ack with firstBadLink as <:IP:>:<:NUM:>\", \"cluster_count\": 197}\n",
      "Input (17172): Abandoning BP-1347369012-10.190.173.170-1444972147527:blk#1073743056_2272\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 198, \"cluster_size\": 1, \"template_mined\": \"Abandoning BP-<:NUM:>-<:IP:><:NUM:>:blk#<:NUM:> <:NUM:>\", \"cluster_count\": 198}\n",
      "Input (17173): Excluding datanode 10.86.169.121:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 199, \"cluster_size\": 1, \"template_mined\": \"Excluding datanode <:IP:>:<:NUM:>\", \"cluster_count\": 199}\n",
      "Input (17176): Saved output of task 'attempt_1445087491445_0007_r_000000_1000' to hdfs://msra-sa-41:9000/out/out1/_temporary/2/task_1445087491445_0007_r_000000\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 194, \"cluster_size\": 2, \"template_mined\": \"Saved output of task 'attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>' to <:*:> temporary/<:NUM:>/task <:NUM:> <:NUM:> r <:NUM:>\", \"cluster_count\": 199}\n",
      "Processing line: 20000, rate 11408.4 lines/sec, 199 clusters so far.\n",
      "Input (20064): Killing taskAttempt:attempt_1445094324383_0002_r_000000_0 because it is running on unusable node:04DN8IQ.fareast.corp.microsoft.com:55452\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 166, \"cluster_size\": 4, \"template_mined\": \"Killing taskAttempt:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> because it is running on unusable <:*:>\", \"cluster_count\": 199}\n",
      "Input (20068): Diagnostics report from attempt_1445094324383_0002_r_000000_0: Container released on a *lost* node\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 167, \"cluster_size\": 4, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: Container released on a *lost* node\", \"cluster_count\": 199}\n",
      "Input (20072): attempt_1445094324383_0002_r_000000_0 TaskAttempt Transitioned from KILL_CONTAINER_CLEANUP to KILL_TASK_CLEANUP\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 161, \"cluster_size\": 24, \"template_mined\": \"attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from KILL CONTAINER CLEANUP to KILL TASK CLEANUP\", \"cluster_count\": 199}\n",
      "Input (20716): Could not delete hdfs://msra-sa-41:9000/out/out1/_temporary/1/_temporary/attempt_1445094324383_0002_r_000000_1\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 162, \"cluster_size\": 24, \"template_mined\": \"Could not delete <:*:> temporary/<:NUM:>/ temporary/attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\", \"cluster_count\": 199}\n",
      "Input (20949): Assigning 04DN8IQ.fareast.corp.microsoft.com:13562 with 1 to fetcher#4\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 174, \"cluster_size\": 14, \"template_mined\": \"Assigning <:*:> with <:NUM:> to fetcher#<:NUM:>\", \"cluster_count\": 199}\n",
      "Input (20951): assigned 1 of 1 to 04DN8IQ.fareast.corp.microsoft.com:13562 to fetcher#4\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 175, \"cluster_size\": 14, \"template_mined\": \"assigned <:NUM:> of <:NUM:> to <:*:> to fetcher#<:NUM:>\", \"cluster_count\": 199}\n",
      "Input (20962): 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#4 in 857ms\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 200, \"cluster_size\": 1, \"template_mined\": \"04DN8IQ.fareast.corp.microsoft.com:<:NUM:> freed by fetcher#<:NUM:> in 857ms\", \"cluster_count\": 200}\n",
      "Input (21000): 04DN8IQ.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 603ms\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 200, \"cluster_size\": 2, \"template_mined\": \"04DN8IQ.fareast.corp.microsoft.com:<:NUM:> freed by fetcher#<:NUM:> in <:*:>\", \"cluster_count\": 200}\n",
      "Input (22988): Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MININT-FNANLI5/127.0.0.1\"; destination host is: \"10.190.173.170\":29630;\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 168, \"cluster_size\": 4, \"template_mined\": \"Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\", \"cluster_count\": 200}\n",
      "Input (22990): Retrying connect to server: 10.190.173.170/10.190.173.170:29630. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 164, \"cluster_size\": 151, \"template_mined\": \"Retrying connect to server: <:*:> Already tried <:NUM:> time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=<:NUM:>, sleepTime=<:NUM:> MILLISECONDS)\", \"cluster_count\": 200}\n",
      "Input (23000): Communication exception: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to 10.190.173.170:29630 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 201, \"cluster_size\": 1, \"template_mined\": \"Communication exception: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to <:IP:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 201}\n",
      "Input (23001): java.net.NoRouteToHostException: No route to host: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 202, \"cluster_size\": 1, \"template_mined\": \"java.net.NoRouteToHostException: No route to host: no further information\", \"cluster_count\": 202}\n",
      "Input (23014): Process Thread Dump: Communication exception\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 203, \"cluster_size\": 1, \"template_mined\": \"Process Thread Dump: Communication exception\", \"cluster_count\": 203}\n",
      "Input (23015): WAITING\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 204, \"cluster_size\": 1, \"template_mined\": \"WAITING\", \"cluster_count\": 204}\n",
      "Input (23016): 0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 205, \"cluster_size\": 1, \"template_mined\": \"<:NUM:>\", \"cluster_count\": 205}\n",
      "Input (23018): TIMED_WAITING\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 206, \"cluster_size\": 1, \"template_mined\": \"TIMED WAITING\", \"cluster_count\": 206}\n",
      "Input (23021): RUNNABLE\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 207, \"cluster_size\": 1, \"template_mined\": \"RUNNABLE\", \"cluster_count\": 207}\n",
      "Input (23051): Last retry, killing attempt_1445144423722_0022_m_000004_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 208, \"cluster_size\": 1, \"template_mined\": \"Last retry, killing attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 208}\n",
      "Input (23143): for url=13562/mapOutput?job=job_1445144423722_0022&reduce=0&map=attempt_1445144423722_0022_m_000002_0,attempt_1445144423722_0022_m_000000_0,attempt_1445144423722_0022_m_000001_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 209, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 209}\n",
      "Input (24261): Runnning cleanup for the task\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 210, \"cluster_size\": 1, \"template_mined\": \"Runnning cleanup for the task\", \"cluster_count\": 210}\n",
      "Input (27128): Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 211, \"cluster_size\": 1, \"template_mined\": \"Address change detected. Old: msra-sa-<:NUM:>/<:IP:>:<:NUM:> New: msra-sa-<:NUM:>:<:NUM:>\", \"cluster_count\": 211}\n",
      "Input (27129): Failed to renew lease for [DFSClient_NONMAPREDUCE_483047941_1] for 46 seconds.  Will retry shortly ...\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 212, \"cluster_size\": 1, \"template_mined\": \"Failed to renew lease for [DFSClient NONMAPREDUCE <:NUM:> <:NUM:>] for <:NUM:> seconds. Will retry shortly ...\", \"cluster_count\": 212}\n",
      "Input (27130): No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 213, \"cluster_size\": 1, \"template_mined\": \"No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 213}\n",
      "Input (27200): ERROR IN CONTACTING RM.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 214, \"cluster_size\": 1, \"template_mined\": \"ERROR IN CONTACTING RM.\", \"cluster_count\": 214}\n",
      "Input (27201): Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MININT-FNANLI5/127.0.0.1\"; destination host is: \"msra-sa-41\":8030;\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 215, \"cluster_size\": 1, \"template_mined\": \"Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \\\"MININT-FNANLI5/<:IP:>\\\"; destination host is: \\\"msra-sa-<:NUM:>\\\":<:NUM:>;\", \"cluster_count\": 215}\n",
      "Input (27214): Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \"MININT-FNANLI5/127.0.0.1\"; destination host is: \"msra-sa-41\":8030;\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 216, \"cluster_size\": 1, \"template_mined\": \"Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \\\"MININT-FNANLI5/<:IP:>\\\"; destination host is: \\\"msra-sa-<:NUM:>\\\":<:NUM:>;\", \"cluster_count\": 216}\n",
      "Input (27215): java.io.IOException: Couldn't set up IO streams\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 217, \"cluster_size\": 1, \"template_mined\": \"java.io.IOException: Couldn't set up IO streams\", \"cluster_count\": 217}\n",
      "Input (27216): java.nio.channels.UnresolvedAddressException\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 218, \"cluster_size\": 1, \"template_mined\": \"java.nio.channels.UnresolvedAddressException\", \"cluster_count\": 218}\n",
      "Input (28202): Ignoring obsolete output of FAILED map-task: 'attempt_1445182159119_0002_m_000007_0'\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 179, \"cluster_size\": 3, \"template_mined\": \"Ignoring obsolete output of <:*:> map-task: 'attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>'\", \"cluster_count\": 218}\n",
      "Input (28247): Exception in getting events\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 219, \"cluster_size\": 1, \"template_mined\": \"Exception in getting events\", \"cluster_count\": 219}\n",
      "Input (28248): Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MSRA-SA-41/10.190.173.170\"; destination host is: \"minint-fnanli5.fareast.corp.microsoft.com\":52839;\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 215, \"cluster_size\": 2, \"template_mined\": \"Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\", \"cluster_count\": 219}\n",
      "Input (29067): MININT-FNANLI5.fareast.corp.microsoft.com:13562 freed by fetcher#5 in 36889ms\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 200, \"cluster_size\": 10, \"template_mined\": \"<:*:> freed by fetcher#<:NUM:> in <:*:>\", \"cluster_count\": 219}\n",
      "Processing line: 30000, rate 11351.6 lines/sec, 219 clusters so far.\n",
      "Input (34601): TaskAttempt killed because it ran on unusable node 04DN8IQ.fareast.corp.microsoft.com:55452. AttemptId:attempt_1445087491445_0004_m_000002_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 220, \"cluster_size\": 1, \"template_mined\": \"TaskAttempt killed because it ran on unusable node 04DN8IQ.fareast.corp.microsoft.com:<:NUM:>. AttemptId:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 220}\n",
      "Input (35191): TaskAttempt killed because it ran on unusable node MININT-FNANLI5.fareast.corp.microsoft.com:55629. AttemptId:attempt_1445087491445_0004_m_000005_0\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 220, \"cluster_size\": 7, \"template_mined\": \"TaskAttempt killed because it ran on unusable node <:*:> AttemptId:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 220}\n",
      "Input (35202): Ramping down all scheduled reduces:0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 221, \"cluster_size\": 1, \"template_mined\": \"Ramping down all scheduled reduces:<:NUM:>\", \"cluster_count\": 221}\n",
      "Input (35203): Going to preempt 1 due to lack of space for maps\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 222, \"cluster_size\": 1, \"template_mined\": \"Going to preempt <:NUM:> due to lack of space for maps\", \"cluster_count\": 222}\n",
      "Input (35204): Preempting attempt_1445087491445_0004_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 223, \"cluster_size\": 1, \"template_mined\": \"Preempting attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 223}\n",
      "Input (35220): Reduce preemption successful attempt_1445087491445_0004_r_000000_1000\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 224, \"cluster_size\": 1, \"template_mined\": \"Reduce preemption successful attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\", \"cluster_count\": 224}\n",
      "Input (37871): Merging 4 intermediate segments out of a total of 13\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 225, \"cluster_size\": 1, \"template_mined\": \"Merging <:NUM:> intermediate segments out of a total of <:NUM:>\", \"cluster_count\": 225}\n",
      "Input (37905): Communication exception: java.net.ConnectException: Call From MSRA-SA-39/172.22.149.145 to minint-fnanli5.fareast.corp.microsoft.com:49594 failed on connection exception: java.net.ConnectException: Connection timed out: no further information; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 226, \"cluster_size\": 1, \"template_mined\": \"Communication exception: java.net.ConnectException: Call From MSRA-SA-<:NUM:>/<:IP:> to minint-fnanli5.fareast.corp.microsoft.com:<:NUM:> failed on connection exception: java.net.ConnectException: Connection timed out: no further information; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused\", \"cluster_count\": 226}\n",
      "Input (37906): java.net.ConnectException: Connection timed out: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 227, \"cluster_size\": 1, \"template_mined\": \"java.net.ConnectException: Connection timed out: no further information\", \"cluster_count\": 227}\n",
      "Input (38054): Stopping ReduceTask metrics system...\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 34, \"cluster_size\": 56, \"template_mined\": \"Stopping <:*:> metrics system...\", \"cluster_count\": 227}\n",
      "Input (38055): ReduceTask metrics system stopped.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 170, \"cluster_size\": 18, \"template_mined\": \"ReduceTask metrics system <:*:>\", \"cluster_count\": 227}\n",
      "Input (38056): ReduceTask metrics system shutdown complete.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 228, \"cluster_size\": 1, \"template_mined\": \"ReduceTask metrics system shutdown complete.\", \"cluster_count\": 228}\n",
      "Processing line: 40000, rate 15805.4 lines/sec, 228 clusters so far.\n",
      "Input (45644): Slow ReadProcessor read fields took 48944ms (threshold=30000ms); ack: seqno: -2 status: SUCCESS status: ERROR downstreamAckTimeNanos: 0, targets: [172.22.149.145:50010, 10.86.169.121:50010]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 182, \"cluster_size\": 2, \"template_mined\": \"Slow ReadProcessor read fields took <:*:> (threshold=30000ms); ack: seqno: <:NUM:> status: SUCCESS status: ERROR downstreamAckTimeNanos: <:NUM:>, targets: [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>]\", \"cluster_count\": 228}\n",
      "Input (47647): Releasing unassigned and invalid container Container: [ContainerId: container_1445076437777_0005_01_000011, NodeId: MININT-75DGDAM1.fareast.corp.microsoft.com:53425, NodeHttpAddress: MININT-75DGDAM1.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.86.165.66:53425 }, ]. RM may have assignment issues\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 229, \"cluster_size\": 1, \"template_mined\": \"Releasing unassigned and invalid container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: MININT-75DGDAM1.fareast.corp.microsoft.com:<:NUM:>, NodeHttpAddress: MININT-75DGDAM1.fareast.corp.microsoft.com:<:NUM:>, Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ]. RM may have assignment issues\", \"cluster_count\": 229}\n",
      "Input (49356): Connection retry failed with 4 attempts in 180 seconds\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 230, \"cluster_size\": 1, \"template_mined\": \"Connection retry failed with <:NUM:> attempts in <:NUM:> seconds\", \"cluster_count\": 230}\n",
      "Input (49357): Failed to connect to MININT-FNANLI5.fareast.corp.microsoft.com:13562 with 1 map outputs\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 231, \"cluster_size\": 1, \"template_mined\": \"Failed to connect to MININT-FNANLI5.fareast.corp.microsoft.com:<:NUM:> with <:NUM:> map outputs\", \"cluster_count\": 231}\n",
      "Input (49358): Connection timed out: connect\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 232, \"cluster_size\": 1, \"template_mined\": \"Connection timed out: connect\", \"cluster_count\": 232}\n",
      "Input (49359): Reporting fetch failure for attempt_1445087491445_0004_m_000005_0 to jobtracker.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 233, \"cluster_size\": 1, \"template_mined\": \"Reporting fetch failure for attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> to jobtracker.\", \"cluster_count\": 233}\n",
      "Processing line: 50000, rate 15922.1 lines/sec, 233 clusters so far.\n",
      "Input (51558): Task: attempt_1445182159119_0002_m_000007_0 - exited : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt_1445182159119_0002_m_000007_0/file.out\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 234, \"cluster_size\": 1, \"template_mined\": \"Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\", \"cluster_count\": 234}\n",
      "Input (51559): Diagnostics report from attempt_1445182159119_0002_m_000007_0: Error: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt_1445182159119_0002_m_000007_0/file.out\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 235, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\", \"cluster_count\": 235}\n",
      "Input (51565): attempt_1445182159119_0002_m_000007_0 TaskAttempt Transitioned from FAIL_CONTAINER_CLEANUP to FAIL_TASK_CLEANUP\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 161, \"cluster_size\": 72, \"template_mined\": \"attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> CONTAINER CLEANUP to <:*:> TASK CLEANUP\", \"cluster_count\": 235}\n",
      "Input (51569): 1 failures on node 04DN8IQ.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 236, \"cluster_size\": 1, \"template_mined\": \"<:NUM:> failures on node 04DN8IQ.fareast.corp.microsoft.com\", \"cluster_count\": 236}\n",
      "Input (53940): Task: attempt_1445182159119_0015_m_000005_0 - exited : java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 237, \"cluster_size\": 1, \"template_mined\": \"Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 237}\n",
      "Input (53941): Diagnostics report from attempt_1445182159119_0015_m_000005_0: Error: java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 238, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 238}\n",
      "Input (53967): Task: attempt_1445182159119_0015_m_000005_0 - exited : java.io.IOException: Spill failed\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 239, \"cluster_size\": 1, \"template_mined\": \"Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.io.IOException: Spill failed\", \"cluster_count\": 239}\n",
      "Input (53968): org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 240, \"cluster_size\": 1, \"template_mined\": \"org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 240}\n",
      "Input (53969): java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 241, \"cluster_size\": 1, \"template_mined\": \"java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 241}\n",
      "Input (53970): Diagnostics report from attempt_1445182159119_0015_m_000005_0: Error: java.io.IOException: Spill failed\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 242, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: java.io.IOException: Spill failed\", \"cluster_count\": 242}\n",
      "Input (54002): Added attempt_1445182159119_0015_m_000006_1 to list of failed maps\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 243, \"cluster_size\": 1, \"template_mined\": \"Added attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> to list of failed maps\", \"cluster_count\": 243}\n",
      "Input (54016): Assigning container Container: [ContainerId: container_1445182159119_0015_01_000012, NodeId: MSRA-SA-41.fareast.corp.microsoft.com:10769, NodeHttpAddress: MSRA-SA-41.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 5, Token: Token { kind: ContainerToken, service: 10.190.173.170:10769 }, ] to fast fail map\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 244, \"cluster_size\": 1, \"template_mined\": \"Assigning container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>, NodeHttpAddress: MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:>, Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] to fast fail map\", \"cluster_count\": 244}\n",
      "Input (54017): Assigned from earlierFailedMaps\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 245, \"cluster_size\": 1, \"template_mined\": \"Assigned from earlierFailedMaps\", \"cluster_count\": 245}\n",
      "Input (56572): Unable to parse prior job history, aborting recovery\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 246, \"cluster_size\": 1, \"template_mined\": \"Unable to parse prior job history, aborting recovery\", \"cluster_count\": 246}\n",
      "Input (56573): Incompatible event log version: null\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 247, \"cluster_size\": 1, \"template_mined\": \"Incompatible event log version: null\", \"cluster_count\": 247}\n",
      "Input (56576): Could not parse the old history file. Will not have old AMinfos\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 248, \"cluster_size\": 1, \"template_mined\": \"Could not parse the old history file. Will not have old AMinfos\", \"cluster_count\": 248}\n",
      "Processing line: 60000, rate 12629.8 lines/sec, 248 clusters so far.\n",
      "Input (66726): Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MININT-75DGDAM1/10.86.165.66\"; destination host is: \"04dn8iq.fareast.corp.microsoft.com\":49470;\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 249, \"cluster_size\": 1, \"template_mined\": \"Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \\\"MININT-75DGDAM1/<:IP:>\\\"; destination host is: \\\"04dn8iq.fareast.corp.microsoft.com\\\":<:NUM:>;\", \"cluster_count\": 249}\n",
      "Processing line: 70000, rate 18314.2 lines/sec, 249 clusters so far.\n",
      "Input (75954): No route to host: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 250, \"cluster_size\": 1, \"template_mined\": \"No route to host: no further information\", \"cluster_count\": 250}\n",
      "Input (75955): Failed to connect to /172.22.149.145:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 251, \"cluster_size\": 1, \"template_mined\": \"Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information\", \"cluster_count\": 251}\n",
      "Input (75961): Could not obtain BP-1347369012-10.190.173.170-1444972147527:blk#1073742989_2201 from any node: java.io.IOException: No live nodes contain block BP-1347369012-10.190.173.170-1444972147527:blk#1073742989_2201 after checking nodes = [172.22.149.145:50010, 10.190.173.170:50010], ignoredNodes = null No live nodes contain current block Block locations: 172.22.149.145:50010 10.190.173.170:50010 Dead nodes:  10.190.173.170:50010 172.22.149.145:50010. Will get new block locations from namenode and retry...\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 252, \"cluster_size\": 1, \"template_mined\": \"Could not obtain BP-<:NUM:>-<:IP:><:NUM:>:blk#<:NUM:> <:NUM:> from any node: java.io.IOException: No live nodes contain block BP-<:NUM:>-<:IP:><:NUM:>:blk#<:NUM:> <:NUM:> after checking nodes = [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>], ignoredNodes = null No live nodes contain current block Block locations: <:IP:>:<:NUM:> <:IP:>:<:NUM:> Dead nodes: <:IP:>:<:NUM:> <:IP:>:<:NUM:>. Will get new block locations from namenode and retry...\", \"cluster_count\": 252}\n",
      "Input (75962): DFS chooseDataNode: got # 1 IOException, will wait for 1857.517062515084 msec.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 253, \"cluster_size\": 1, \"template_mined\": \"DFS chooseDataNode: got # <:NUM:> IOException, will wait for <:NUM:>.<:NUM:> msec.\", \"cluster_count\": 253}\n",
      "Input (75972): Could not obtain BP-1347369012-10.190.173.170-1444972147527:blk#1073742989_2201 from any node: java.io.IOException: No live nodes contain block BP-1347369012-10.190.173.170-1444972147527:blk#1073742989_2201 after checking nodes = [172.22.149.145:50010, 10.190.173.170:50010], ignoredNodes = null No live nodes contain current block Block locations: 172.22.149.145:50010 10.190.173.170:50010 Dead nodes:  10.190.173.170:50010 172.22.149.145:50010 10.86.169.121:50010. Will get new block locations from namenode and retry...\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 254, \"cluster_size\": 1, \"template_mined\": \"Could not obtain BP-<:NUM:>-<:IP:><:NUM:>:blk#<:NUM:> <:NUM:> from any node: java.io.IOException: No live nodes contain block BP-<:NUM:>-<:IP:><:NUM:>:blk#<:NUM:> <:NUM:> after checking nodes = [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>], ignoredNodes = null No live nodes contain current block Block locations: <:IP:>:<:NUM:> <:IP:>:<:NUM:> Dead nodes: <:IP:>:<:NUM:> <:IP:>:<:NUM:> <:IP:>:<:NUM:>. Will get new block locations from namenode and retry...\", \"cluster_count\": 254}\n",
      "Input (75975): DFS Read\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 255, \"cluster_size\": 1, \"template_mined\": \"DFS Read\", \"cluster_count\": 255}\n",
      "Input (77624): Releasing unassigned and invalid container Container: [ContainerId: container_1445062781478_0013_01_000012, NodeId: MININT-FNANLI5.fareast.corp.microsoft.com:64642, NodeHttpAddress: MININT-FNANLI5.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.86.169.121:64642 }, ]. RM may have assignment issues\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 229, \"cluster_size\": 2, \"template_mined\": \"Releasing unassigned and invalid container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ]. RM may have assignment issues\", \"cluster_count\": 255}\n",
      "Processing line: 80000, rate 16907.3 lines/sec, 255 clusters so far.\n",
      "Input (86923): DataStreamer Exception\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 256, \"cluster_size\": 1, \"template_mined\": \"DataStreamer Exception\", \"cluster_count\": 256}\n",
      "Input (89293): Could not contact RM after 360000 milliseconds.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 257, \"cluster_size\": 1, \"template_mined\": \"Could not contact RM after <:NUM:> milliseconds.\", \"cluster_count\": 257}\n",
      "Input (89294): Error communicating with RM: Could not contact RM after 360000 milliseconds.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 258, \"cluster_size\": 1, \"template_mined\": \"Error communicating with RM: Could not contact RM after <:NUM:> milliseconds.\", \"cluster_count\": 258}\n",
      "Input (89297): Error writing History Event: org.apache.hadoop.mapreduce.jobhistory.JobUnsuccessfulCompletionEvent@13bb724a\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 259, \"cluster_size\": 1, \"template_mined\": \"Error writing History Event: org.apache.hadoop.mapreduce.jobhistory.JobUnsuccessfulCompletionEvent@13bb724a\", \"cluster_count\": 259}\n",
      "Input (89300): Thread Thread[eventHandlingThread,5,main] threw an Exception.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 260, \"cluster_size\": 1, \"template_mined\": \"Thread Thread[eventHandlingThread,<:NUM:>,main] threw an Exception.\", \"cluster_count\": 260}\n",
      "Input (89301): java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 261, \"cluster_size\": 1, \"template_mined\": \"java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 261}\n",
      "Input (89311): Found jobId job_1445175094696_0003 to have not been closed. Will close\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 262, \"cluster_size\": 1, \"template_mined\": \"Found jobId job <:NUM:> <:NUM:> to have not been closed. Will close\", \"cluster_count\": 262}\n",
      "Input (89312): Error writing History Event: org.apache.hadoop.mapreduce.jobhistory.JobUnsuccessfulCompletionEvent@2890300b\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 259, \"cluster_size\": 2, \"template_mined\": \"Error writing History Event: <:*:>\", \"cluster_count\": 262}\n",
      "Input (89315): Service JobHistoryEventHandler failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 263, \"cluster_size\": 1, \"template_mined\": \"Service JobHistoryEventHandler failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 263}\n",
      "Input (89319): When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 264, \"cluster_size\": 1, \"template_mined\": \"When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 264}\n",
      "Input (89325): cleanup failed for container container_1445175094696_0003_01_000012 : java.lang.IllegalArgumentException: java.net.UnknownHostException: MSRA-SA-39.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 265, \"cluster_size\": 1, \"template_mined\": \"cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: MSRA-SA-<:NUM:>.fareast.corp.microsoft.com\", \"cluster_count\": 265}\n",
      "Input (89326): java.net.UnknownHostException: MSRA-SA-39.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 266, \"cluster_size\": 1, \"template_mined\": \"java.net.UnknownHostException: MSRA-SA-<:NUM:>.fareast.corp.microsoft.com\", \"cluster_count\": 266}\n",
      "Input (89327): Diagnostics report from attempt_1445175094696_0003_r_000000_0: cleanup failed for container container_1445175094696_0003_01_000012 : java.lang.IllegalArgumentException: java.net.UnknownHostException: MSRA-SA-39.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 267, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>: cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: MSRA-SA-<:NUM:>.fareast.corp.microsoft.com\", \"cluster_count\": 267}\n",
      "Input (89339): Exception while unregistering\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 268, \"cluster_size\": 1, \"template_mined\": \"Exception while unregistering\", \"cluster_count\": 268}\n",
      "Input (89344): Skipping cleaning up the staging dir. assuming AM will be retried.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 269, \"cluster_size\": 1, \"template_mined\": \"Skipping cleaning up the staging dir. assuming AM will be retried.\", \"cluster_count\": 269}\n",
      "Input (89349): Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 263, \"cluster_size\": 2, \"template_mined\": \"Service <:*:> failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 269}\n",
      "Input (89353): Graceful stop failed\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 270, \"cluster_size\": 1, \"template_mined\": \"Graceful stop failed\", \"cluster_count\": 270}\n",
      "Processing line: 90000, rate 14871.1 lines/sec, 270 clusters so far.\n",
      "Input (97763): Failed on local exception: java.net.SocketException: Permission denied: no further information; Host Details : local host is: \"MININT-FNANLI5/127.0.0.1\"; destination host is: \"msra-sa-41\":9000;\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 216, \"cluster_size\": 174, \"template_mined\": \"Failed on local exception: <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> Host Details : local host is: \\\"MININT-FNANLI5/<:IP:>\\\"; destination host is: \\\"msra-sa-<:NUM:>\\\":<:NUM:>;\", \"cluster_count\": 270}\n",
      "Input (97764): java.net.SocketException: Permission denied: no further information\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 271, \"cluster_size\": 1, \"template_mined\": \"java.net.SocketException: Permission denied: no further information\", \"cluster_count\": 271}\n",
      "Processing line: 100000, rate 12896.1 lines/sec, 271 clusters so far.\n",
      "Input (101837): Diagnostics report from attempt_1445087491445_0009_r_000000_0: AttemptID:attempt_1445087491445_0009_r_000000_0 Timed out after 600 secs\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 165, \"cluster_size\": 5, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: AttemptID:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> Timed out after <:NUM:> secs\", \"cluster_count\": 271}\n",
      "Input (106776): MapCompletionEvents reques\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 272, \"cluster_size\": 1, \"template_mined\": \"MapCompletionEvents reques\", \"cluster_count\": 272}\n",
      "Processing line: 110000, rate 17439.7 lines/sec, 272 clusters so far.\n",
      "Processing line: 120000, rate 18806.5 lines/sec, 272 clusters so far.\n",
      "Input (126850): Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MININT-FNANLI5/10.86.169.121\"; destination host is: \"04dn8iq.fareast.corp.microsoft.com\":49470;\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 249, \"cluster_size\": 2, \"template_mined\": \"Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: \\\"04dn8iq.fareast.corp.microsoft.com\\\":<:NUM:>;\", \"cluster_count\": 272}\n",
      "Input (129754): WordCount and PageRank. Each application has been run for several times, simulating both normal and abnormal cases with injected specific failures.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 273, \"cluster_size\": 1, \"template_mined\": \"WordCount and PageRank. Each application has been run for several times, simulating both normal and abnormal cases with injected specific failures.\", \"cluster_count\": 273}\n",
      "Processing line: 130000, rate 19645.6 lines/sec, 273 clusters so far.\n",
      "Input (130459): Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: \"MSRA-SA-41/10.190.173.170\"; destination host is: \"minint-fnanli5.fareast.corp.microsoft.com\":62270;\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 249, \"cluster_size\": 3, \"template_mined\": \"Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\", \"cluster_count\": 273}\n",
      "Input (135789): Task attempt_1445182159119_0004_m_000004_0 failed : org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 274, \"cluster_size\": 1, \"template_mined\": \"Task attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> failed : org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 274}\n",
      "Input (137699): Progress of TaskAttempt attempt_1445087491445_0004_m_000003_1 is : 9.742042E-4\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 113, \"cluster_size\": 34147, \"template_mined\": \"Progress of TaskAttempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> is : <:*:>\", \"cluster_count\": 274}\n",
      "Input (138325): Error communicating with RM: Resource Manager doesn't recognize AttemptId: application_1445087491445_0004\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 275, \"cluster_size\": 1, \"template_mined\": \"Error communicating with RM: Resource Manager doesn't recognize AttemptId: application <:NUM:> <:NUM:>\", \"cluster_count\": 275}\n",
      "Input (138326): Resource Manager doesn't recognize AttemptId: application_1445087491445_0004\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 276, \"cluster_size\": 1, \"template_mined\": \"Resource Manager doesn't recognize AttemptId: application <:NUM:> <:NUM:>\", \"cluster_count\": 276}\n",
      "Input (138327): org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException: Application attempt appattempt_1445087491445_0004_000001 doesn't exist in ApplicationMasterService cache.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 277, \"cluster_size\": 1, \"template_mined\": \"org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException: Application attempt appattempt <:NUM:> <:NUM:> <:NUM:> doesn't exist in ApplicationMasterService cache.\", \"cluster_count\": 277}\n",
      "Input (138328): org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException): Application attempt appattempt_1445087491445_0004_000001 doesn't exist in ApplicationMasterService cache.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 278, \"cluster_size\": 1, \"template_mined\": \"org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException): Application attempt appattempt <:NUM:> <:NUM:> <:NUM:> doesn't exist in ApplicationMasterService cache.\", \"cluster_count\": 278}\n",
      "Input (138330): Notify RMCommunicator isAMLastRetry: false\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 127, \"cluster_size\": 75, \"template_mined\": \"Notify <:*:> isAMLastRetry: <:*:>\", \"cluster_count\": 278}\n",
      "Input (138331): RMCommunicator notified that shouldUnregistered is: false\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 128, \"cluster_size\": 38, \"template_mined\": \"RMCommunicator notified that shouldUnregistered is: <:*:>\", \"cluster_count\": 278}\n",
      "Input (138333): JobHistoryEventHandler notified that forceJobCompletion is false\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 129, \"cluster_size\": 38, \"template_mined\": \"JobHistoryEventHandler notified that forceJobCompletion is <:*:>\", \"cluster_count\": 278}\n",
      "Processing line: 140000, rate 16271.5 lines/sec, 278 clusters so far.\n",
      "Input (143018): Error closing writer for JobID: job_1445144423722_0023\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 279, \"cluster_size\": 1, \"template_mined\": \"Error closing writer for JobID: job <:NUM:> <:NUM:>\", \"cluster_count\": 279}\n",
      "Input (143026): Service JobHistoryEventHandler failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 280, \"cluster_size\": 1, \"template_mined\": \"Service JobHistoryEventHandler failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\", \"cluster_count\": 280}\n",
      "Input (143027): java.nio.channels.ClosedChannelException\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 281, \"cluster_size\": 1, \"template_mined\": \"java.nio.channels.ClosedChannelException\", \"cluster_count\": 281}\n",
      "Input (143029): When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 282, \"cluster_size\": 1, \"template_mined\": \"When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\", \"cluster_count\": 282}\n",
      "Input (143034): cleanup failed for container container_1445144423722_0023_01_000002 : java.lang.IllegalArgumentException: java.net.UnknownHostException: 04DN8IQ.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 265, \"cluster_size\": 2, \"template_mined\": \"cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: <:*:>\", \"cluster_count\": 282}\n",
      "Input (143035): java.net.UnknownHostException: 04DN8IQ.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 266, \"cluster_size\": 3, \"template_mined\": \"java.net.UnknownHostException: <:*:>\", \"cluster_count\": 282}\n",
      "Input (143037): Diagnostics report from attempt_1445144423722_0023_m_000000_0: cleanup failed for container container_1445144423722_0023_01_000002 : java.lang.IllegalArgumentException: java.net.UnknownHostException: 04DN8IQ.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 267, \"cluster_size\": 2, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: <:*:>\", \"cluster_count\": 282}\n",
      "Input (143090): Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 280, \"cluster_size\": 2, \"template_mined\": \"Service <:*:> failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\", \"cluster_count\": 282}\n",
      "Input (147542): Exception running child : java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 283, \"cluster_size\": 1, \"template_mined\": \"Exception running child : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 283}\n",
      "Input (147546): Exception cleaning up: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 284, \"cluster_size\": 1, \"template_mined\": \"Exception cleaning up: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 284}\n",
      "Processing line: 150000, rate 13086.4 lines/sec, 284 clusters so far.\n",
      "Input (151154): Task: attempt_1445182159119_0014_r_000000_0 - exited : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 285, \"cluster_size\": 1, \"template_mined\": \"Task: attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> - exited : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\", \"cluster_count\": 285}\n",
      "Input (151157): Diagnostics report from attempt_1445182159119_0014_r_000000_0: Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#1\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 286, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>: Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\", \"cluster_count\": 286}\n",
      "Input (153282): Task: attempt_1445182159119_0003_m_000000_0 - failed due to FSError: java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 287, \"cluster_size\": 1, \"template_mined\": \"Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - failed due to FSError: java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 287}\n",
      "Input (153283): Diagnostics report from attempt_1445182159119_0003_m_000000_0: FSError: java.io.IOException: There is not enough space on the disk\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 238, \"cluster_size\": 9, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: <:*:> java.io.IOException: There is not enough space on the disk\", \"cluster_count\": 287}\n",
      "Input (153337): Assigning container Container: [ContainerId: container_1445182159119_0003_01_000016, NodeId: 04DN8IQ.fareast.corp.microsoft.com:64260, NodeHttpAddress: 04DN8IQ.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 5, Token: Token { kind: ContainerToken, service: 10.86.164.138:64260 }, ] to fast fail map\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 244, \"cluster_size\": 4, \"template_mined\": \"Assigning container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] to fast fail map\", \"cluster_count\": 287}\n",
      "Processing line: 160000, rate 16252.0 lines/sec, 287 clusters so far.\n",
      "Input (164593): for url=13562/mapOutput?job=job_1445087491445_0002&reduce=0&map=attempt_1445087491445_0002_m_000002_0,attempt_1445087491445_0002_m_000003_1,attempt_1445087491445_0002_m_000004_0,attempt_1445087491445_0002_m_000005_0,attempt_1445087491445_0002_m_000008_0,attempt_1445087491445_0002_m_000009_0,attempt_1445087491445_0002_m_000011_0 sent hash and received reply\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 288, \"cluster_size\": 1, \"template_mined\": \"for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\", \"cluster_count\": 288}\n",
      "Input (166164): Task: attempt_1445144423722_0020_m_000002_0 - exited : java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 289, \"cluster_size\": 1, \"template_mined\": \"Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 289}\n",
      "Input (166166): Diagnostics report from attempt_1445144423722_0020_m_000002_0: Error: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 290, \"cluster_size\": 1, \"template_mined\": \"Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\", \"cluster_count\": 290}\n",
      "Input (166186): Task cleanup failed for attempt attempt_1445144423722_0020_m_000002_0\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 291, \"cluster_size\": 1, \"template_mined\": \"Task cleanup failed for attempt attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\", \"cluster_count\": 291}\n",
      "Input (166200): 1 failures on node MININT-FNANLI5.fareast.corp.microsoft.com\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 236, \"cluster_size\": 9, \"template_mined\": \"<:NUM:> failures on node <:*:>\", \"cluster_count\": 291}\n",
      "Input (168135): In stop, writing event MAP_ATTEMPT_FAILED\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 292, \"cluster_size\": 1, \"template_mined\": \"In stop, writing event MAP ATTEMPT FAILED\", \"cluster_count\": 292}\n",
      "Input (168136): Service JobHistoryEventHandler failed in state STOPPED; cause: org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 293, \"cluster_size\": 1, \"template_mined\": \"Service JobHistoryEventHandler failed in state STOPPED; cause: org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\", \"cluster_count\": 293}\n",
      "Input (168137): Attempt to process a enum when a union was expected.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 294, \"cluster_size\": 1, \"template_mined\": \"Attempt to process a enum when a union was expected.\", \"cluster_count\": 294}\n",
      "Input (168138): When stopping the service JobHistoryEventHandler : org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 295, \"cluster_size\": 1, \"template_mined\": \"When stopping the service JobHistoryEventHandler : org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\", \"cluster_count\": 295}\n",
      "Input (168228): Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state STOPPED; cause: org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 293, \"cluster_size\": 2, \"template_mined\": \"Service <:*:> failed in state STOPPED; cause: org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\", \"cluster_count\": 295}\n",
      "Processing line: 170000, rate 13426.5 lines/sec, 295 clusters so far.\n",
      "Processing line: 180000, rate 17813.9 lines/sec, 295 clusters so far.\n",
      "Input (182342): Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#3\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 296, \"cluster_size\": 1, \"template_mined\": \"Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\", \"cluster_count\": 296}\n",
      "Input (185691): Shuffle failed : local error on this node: 04DN8IQ/10.86.164.138\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 297, \"cluster_size\": 1, \"template_mined\": \"Shuffle failed : local error on this node: 04DN8IQ/<:IP:>\", \"cluster_count\": 297}\n",
      "Input (187182): Ignoring exception during close for org.apache.hadoop.mapred.MapTask$NewOutputCollector@6a6585ee\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 298, \"cluster_size\": 1, \"template_mined\": \"Ignoring exception during close for org.apache.hadoop.mapred.MapTask$NewOutputCollector@6a6585ee\", \"cluster_count\": 298}\n",
      "Input (187183): Exception running child : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt_1445182159119_0002_m_000007_0/file.out\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 299, \"cluster_size\": 1, \"template_mined\": \"Exception running child : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\", \"cluster_count\": 299}\n",
      "Processing line: 190000, rate 16498.3 lines/sec, 299 clusters so far.\n",
      "Input (192427): IPC Server handler 29 on 58622, call statusUpdate(attempt_1445094324383_0003_m_000000_0, org.apache.hadoop.mapred.MapTaskStatus@cdcdbf7), rpc version=2, client version=19, methodsFingerPrint=937413979 from 10.86.169.121:52490 Call#68 Retry#0: output error\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 300, \"cluster_size\": 1, \"template_mined\": \"IPC Server handler <:NUM:> on <:NUM:>, call statusUpdate(attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>, org.apache.hadoop.mapred.MapTaskStatus@cdcdbf7), rpc version=<:NUM:>, client version=<:NUM:>, methodsFingerPrint=<:NUM:> from <:IP:>:<:NUM:> Call#<:NUM:> Retry#<:NUM:>: output error\", \"cluster_count\": 300}\n",
      "Input (192428): IPC Server handler 29 on 58622 caught an exception\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 301, \"cluster_size\": 1, \"template_mined\": \"IPC Server handler <:NUM:> on <:NUM:> caught an exception\", \"cluster_count\": 301}\n",
      "--- Done processing file in 13.58 sec. Total of 193632 lines, rate 14261.9 lines/sec, 301 clusters\n",
      "ID=113   : size=45882     : Progress of TaskAttempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> is : <:*:>\n",
      "ID=112   : size=18140     : MapCompletionEvents request from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>. startIndex <:NUM:> maxEvents <:NUM:>\n",
      "ID=211   : size=5795      : Address change detected. Old: msra-sa-<:NUM:>/<:IP:>:<:NUM:> New: msra-sa-<:NUM:>:<:NUM:>\n",
      "ID=12    : size=5542      : (EQUATOR) <:NUM:> kvi <:NUM:>(<:NUM:>)\n",
      "ID=18    : size=5339      : Spilling map output\n",
      "ID=19    : size=5339      : bufstart = <:NUM:>; bufend = <:NUM:>; bufvoid = <:NUM:>\n",
      "ID=20    : size=5339      : kvstart = <:NUM:>(<:NUM:>); kvend = <:NUM:>(<:NUM:>); length = <:NUM:>/<:NUM:>\n",
      "ID=202   : size=5330      : java.net.NoRouteToHostException: No route to host: no further information\n",
      "ID=213   : size=5303      : No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=212   : size=5300      : Failed to renew lease for [DFSClient NONMAPREDUCE <:NUM:> <:NUM:>] for <:NUM:> seconds. Will retry shortly ...\n",
      "ID=21    : size=5204      : Finished spill <:NUM:>\n",
      "ID=22    : size=4579      : (RESET) equator <:NUM:> kv <:NUM:>(<:NUM:>) kvi <:NUM:>(<:NUM:>)\n",
      "ID=92    : size=4415      : Recalculating schedule, headroom=<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "ID=147   : size=3841      : Reduce slow start threshold not met. completedMapsForReduceSlowstart <:NUM:>\n",
      "ID=83    : size=3297      : Resolved <:*:> to /default-rack\n",
      "ID=85    : size=2921      : attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> to <:*:>\n",
      "ID=163   : size=2517      : Retrying connect to server: <:*:> Already tried <:NUM:> time(s); maxRetries=<:NUM:>\n",
      "ID=87    : size=2074      : task <:NUM:> <:NUM:> <:*:> <:NUM:> Task Transitioned from <:*:> to <:*:>\n",
      "ID=106   : size=1757      : Opening proxy : <:*:>\n",
      "ID=104   : size=1744      : Processing the event EventType: CONTAINER REMOTE <:*:> for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> taskAttempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=153   : size=1611      : attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> <:*:> <:*:> <:*:> <:*:>\n",
      "ID=95    : size=1249      : After Scheduling: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\n",
      "ID=3     : size=1140      : MapTask metrics system <:*:>\n",
      "ID=164   : size=1048      : Retrying connect to server: <:*:> Already tried <:NUM:> time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=<:NUM:>, sleepTime=<:NUM:> MILLISECONDS)\n",
      "ID=84    : size=1033      : TaskAttempt: [attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>] using containerId: [container <:NUM:> <:NUM:> <:NUM:> <:NUM:> on NM: <:*:>\n",
      "ID=1     : size=978       : loaded properties from hadoop-metrics2.properties\n",
      "ID=2     : size=978       : Scheduled snapshot period at <:NUM:> second(s).\n",
      "ID=4     : size=978       : Executing with tokens:\n",
      "ID=109   : size=962       : Auth successful for job <:NUM:> <:NUM:> (auth:SIMPLE)\n",
      "ID=5     : size=909       : Kind: mapreduce.job, Service: job <:NUM:> <:NUM:>, Ident: <:*:>\n",
      "ID=6     : size=909       : Sleeping for 0ms before retrying again. Got null now.\n",
      "ID=7     : size=907       : mapreduce.cluster.local.dir for child: /tmp/hadoop-msrabi/nm-local-dir/usercache/msrabi/appcache/application <:NUM:> <:NUM:>\n",
      "ID=8     : size=907       : session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "ID=99    : size=906       : Assigned container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> to attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=105   : size=906       : Launching attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=107   : size=906       : Shuffle port returned by ContainerManager for attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> : <:NUM:>\n",
      "ID=9     : size=905       : ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "ID=10    : size=905       : Using ResourceCalculatorProcessTree : <:*:>\n",
      "ID=108   : size=905       : ATTEMPT START task <:NUM:> <:NUM:> <:*:> <:NUM:>\n",
      "ID=110   : size=902       : JVM with ID : jvm <:NUM:> <:NUM:> <:*:> <:NUM:> asked for a task\n",
      "ID=111   : size=902       : JVM with ID: jvm <:NUM:> <:NUM:> <:*:> <:NUM:> given task: attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=96    : size=899       : getResources() for application <:NUM:> <:NUM:>: ask=<:NUM:> release= <:NUM:> newContainers=<:NUM:> finishedContainers=<:NUM:> resourcelimit=<memory:<:NUM:>, vCores:<:NUM:>> knownNMs=<:NUM:>\n",
      "ID=124   : size=851       : KILLING attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=11    : size=834       : Processing split: <:*:>\n",
      "ID=13    : size=834       : mapreduce.task.io.sort.mb: <:NUM:>\n",
      "ID=14    : size=834       : soft limit at <:NUM:>\n",
      "ID=15    : size=834       : bufstart = <:NUM:>; bufvoid = <:NUM:>\n",
      "ID=16    : size=834       : kvstart = <:NUM:>; length = <:NUM:>\n",
      "ID=17    : size=834       : Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "ID=149   : size=826       : Received completed container container <:NUM:> <:NUM:> <:NUM:> <:NUM:>\n",
      "ID=91    : size=797       : Before Scheduling: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\n",
      "ID=159   : size=733       : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: Container killed by the ApplicationMaster.\n",
      "ID=86    : size=701       : Task succeeded with attempt attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=89    : size=701       : Num completed Tasks: <:NUM:>\n",
      "ID=24    : size=678       : Merging <:NUM:> sorted segments\n",
      "ID=25    : size=678       : Down to the last merge-pass, with <:NUM:> segments left of total size: <:NUM:> bytes\n",
      "ID=177   : size=667       : attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Shuffling to disk since <:NUM:> is greater than maxSingleShuffleLimit (<:NUM:>)\n",
      "ID=178   : size=664       : fetcher#<:NUM:> about to shuffle output of map attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> decomp: <:NUM:> len: <:NUM:> to DISK\n",
      "ID=184   : size=649       : Read <:NUM:> bytes from map-output for attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "ID=23    : size=644       : Starting flush of map output\n",
      "ID=26    : size=625       : Task:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> is done. And is in the process of committing\n",
      "ID=41    : size=621       : Registering class <:*:> for class <:*:>\n",
      "ID=27    : size=616       : Task 'attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>' done.\n",
      "ID=97    : size=606       : Got allocated containers <:NUM:>\n",
      "ID=122   : size=605       : Done acknowledgement from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=157   : size=531       : completedMapPercent <:NUM:>.<:NUM:> totalResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> finalMapResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> finalReduceResourceLimit:<memory:<:NUM:>, vCores:<:NUM:>> netScheduledMapResource:<memory:<:NUM:>, vCores:<:NUM:>> netScheduledReduceResource:<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "ID=216   : size=487       : Failed on local exception: <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> Host Details : local host is: \"MININT-FNANLI5/<:IP:>\"; destination host is: \"msra-sa-<:NUM:>\":<:NUM:>;\n",
      "ID=214   : size=480       : ERROR IN CONTACTING RM.\n",
      "ID=217   : size=479       : java.io.IOException: Couldn't set up IO streams\n",
      "ID=218   : size=479       : java.nio.channels.UnresolvedAddressException\n",
      "ID=174   : size=472       : Assigning <:*:> with <:NUM:> to fetcher#<:NUM:>\n",
      "ID=175   : size=472       : assigned <:NUM:> of <:NUM:> to <:*:> to fetcher#<:NUM:>\n",
      "ID=173   : size=414       : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>: Got <:NUM:> new map-outputs\n",
      "ID=185   : size=383       : MSRA-SA-<:NUM:>.fareast.corp.microsoft.com:<:NUM:> freed by fetcher#<:NUM:> in <:*:>\n",
      "ID=180   : size=381       : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=81    : size=335       : Processing the event EventType: <:*:> <:*:>\n",
      "ID=34    : size=317       : Stopping <:*:> metrics system...\n",
      "ID=53    : size=306       : job <:NUM:> <:*:> Transitioned from <:*:> to <:*:>\n",
      "ID=35    : size=302       : MapTask metrics system shutdown complete.\n",
      "ID=154   : size=255       : DefaultSpeculator.addSpeculativeAttempt -- we are speculating task <:NUM:> <:NUM:> m <:NUM:>\n",
      "ID=155   : size=255       : We launched <:NUM:> speculations. Sleeping <:NUM:> milliseconds.\n",
      "ID=42    : size=226       : Default file system [hdfs://msra-sa-<:NUM:>:<:NUM:>]\n",
      "ID=152   : size=218       : An existing connection was forcibly closed by the remote host\n",
      "ID=151   : size=217       : Socket Reader #<:NUM:> for port <:NUM:>: readAndProcess from client <:IP:> threw exception [java.io.IOException: An existing connection was forcibly closed by the remote host]\n",
      "ID=161   : size=216       : attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> TaskAttempt Transitioned from <:*:> CONTAINER CLEANUP to <:*:> TASK CLEANUP\n",
      "ID=162   : size=216       : Could not delete <:*:> temporary/<:NUM:>/ temporary/attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>\n",
      "ID=156   : size=194       : Scheduling a redundant attempt for task task <:NUM:> <:NUM:> m <:NUM:>\n",
      "ID=160   : size=182       : Issuing kill to other attempt attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "ID=135   : size=141       : Moved tmp to done: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "ID=55    : size=138       : Using callQueue class java.util.concurrent.LinkedBlockingQueue\n",
      "ID=56    : size=138       : Starting Socket Reader #<:NUM:> for port <:NUM:>\n",
      "ID=58    : size=138       : IPC Server listener on <:NUM:>: starting\n",
      "ID=59    : size=138       : IPC Server Responder: starting\n",
      "ID=64    : size=138       : Added filter AM PROXY FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context <:*:>\n",
      "ID=65    : size=138       : adding path spec: <:*:>\n",
      "ID=100   : size=138       : The <:*:> file on the remote FS is <:*:> <:NUM:> <:*:>\n",
      "ID=205   : size=126       : <:NUM:>\n",
      "ID=119   : size=119       : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:> <:*:>\n",
      "ID=127   : size=104       : Notify <:*:> isAMLastRetry: <:*:>\n",
      "ID=221   : size=100       : Ramping down all scheduled reduces:<:NUM:>\n",
      "ID=222   : size=100       : Going to preempt <:NUM:> due to lack of space for maps\n",
      "ID=46    : size=96        : Read from history task task <:NUM:> <:NUM:> m <:NUM:>\n",
      "ID=82    : size=96        : Recovering task task <:NUM:> <:NUM:> m <:NUM:> from prior app attempt, status was SUCCEEDED\n",
      "ID=133   : size=94        : Copied to done location: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "ID=170   : size=86        : ReduceTask metrics system <:*:>\n",
      "ID=179   : size=84        : Ignoring obsolete output of <:*:> map-task: 'attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>'\n",
      "ID=98    : size=73        : Assigned to reduce\n",
      "ID=200   : size=72        : <:*:> freed by fetcher#<:NUM:> in <:*:>\n",
      "ID=171   : size=71        : Using ShuffleConsumerPlugin: <:*:>\n",
      "ID=172   : size=71        : MergerManager: memoryLimit=<:NUM:>, maxSingleShuffleLimit=<:NUM:>, mergeThreshold=<:NUM:>, ioSortFactor=<:NUM:>, memToMemMergeOutputsThreshold=<:NUM:>\n",
      "ID=36    : size=69        : Created MRAppMaster for application appattempt <:NUM:> <:NUM:> <:NUM:>\n",
      "ID=37    : size=69        : Kind: YARN AM RM TOKEN, Service: , Ident: (appAttemptId { application id { id: <:NUM:> cluster timestamp: <:NUM:> } attemptId: <:NUM:> } keyId: <:NUM:>)\n",
      "ID=38    : size=69        : Using mapred newApiCommitter.\n",
      "ID=39    : size=69        : OutputCommitter set in config null\n",
      "ID=40    : size=69        : OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "ID=43    : size=69        : Emitting job history data to the timeline server is not enabled\n",
      "ID=48    : size=69        : MRAppMaster metrics system started\n",
      "ID=49    : size=69        : Adding job token for job <:NUM:> <:NUM:> to jobTokenSecretManager\n",
      "ID=50    : size=69        : Not uberizing job <:NUM:> <:NUM:> because: not enabled; too many maps; too much input;\n",
      "ID=51    : size=69        : Input size for job job <:NUM:> <:NUM:> = <:NUM:>. Number of splits = <:NUM:>\n",
      "ID=52    : size=69        : Number of reduces for job job <:NUM:> <:NUM:> = <:NUM:>\n",
      "ID=54    : size=69        : MRAppMaster launching normal, non-uberized, multi-container job job <:NUM:> <:NUM:>.\n",
      "ID=57    : size=69        : Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server\n",
      "ID=60    : size=69        : Instantiated MRClientService at <:*:>\n",
      "ID=61    : size=69        : Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n",
      "ID=62    : size=69        : Http request log for http.requests.mapreduce is not defined\n",
      "ID=63    : size=69        : Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n",
      "ID=66    : size=69        : Jetty bound to port <:NUM:>\n",
      "ID=67    : size=69        : jetty-<:NUM:>.<:NUM:>.<:NUM:>\n",
      "ID=68    : size=69        : Extract jar:file:/D:/hadoop-<:NUM:>.<:NUM:>.<:NUM:>-localbox/share/hadoop/yarn/hadoop-yarn-common-<:NUM:>.<:NUM:>.<:NUM:>-SNAPSHOT.jar!/webapps/mapreduce to <:*:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> <:NUM:> mapreduce <:*:>\n",
      "ID=69    : size=69        : Started HttpServer2$SelectChannelConnectorWithSafeStartup@<:IP:>:<:NUM:>\n",
      "ID=70    : size=69        : Web app /mapreduce started at <:NUM:>\n",
      "ID=71    : size=69        : Registered webapp guice modules\n",
      "ID=72    : size=69        : JOB CREATE job <:NUM:> <:NUM:>\n",
      "ID=73    : size=69        : nodeBlacklistingEnabled:true\n",
      "ID=74    : size=69        : maxTaskFailuresPerNode is <:NUM:>\n",
      "ID=75    : size=69        : blacklistDisablePercent is <:NUM:>\n",
      "ID=76    : size=69        : Connecting to ResourceManager at <:*:>\n",
      "ID=77    : size=69        : maxContainerCapability: <memory:<:NUM:>, vCores:<:NUM:>>\n",
      "ID=78    : size=69        : queue: default\n",
      "ID=79    : size=69        : Upper limit on the thread pool size is <:NUM:>\n",
      "ID=80    : size=69        : yarn.client.max-cached-nodemanagers-proxies : <:NUM:>\n",
      "ID=88    : size=69        : Event Writer setup for JobId: job <:NUM:> <:NUM:>, File: hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist\n",
      "ID=90    : size=69        : reduceResourceRequest:<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "ID=101   : size=69        : Adding #<:NUM:> tokens and #<:NUM:> secret keys for NM use for launching container\n",
      "ID=102   : size=69        : Size of containertokens dob is <:NUM:>\n",
      "ID=103   : size=69        : Putting shuffle token in serviceData\n",
      "ID=93    : size=65        : Reduce slow start threshold reached. Scheduling reduces.\n",
      "ID=146   : size=65        : mapResourceRequest:<memory:<:NUM:>, vCores:<:NUM:>>\n",
      "ID=189   : size=56        : finalMerge called with <:NUM:> in-memory map-outputs and <:NUM:> on-disk map-outputs\n",
      "ID=190   : size=56        : Merging <:NUM:> files, <:NUM:> bytes from disk\n",
      "ID=191   : size=56        : Merging <:NUM:> segments, <:NUM:> bytes from memory into reduce\n",
      "ID=192   : size=56        : mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "ID=166   : size=55        : Killing taskAttempt:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> because it is running on unusable <:*:>\n",
      "ID=167   : size=55        : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: Container released on a *lost* node\n",
      "ID=169   : size=55        : java.io.IOException: An existing connection was forcibly closed by the remote host\n",
      "ID=188   : size=54        : EventFetcher is interrupted.. Returning\n",
      "ID=128   : size=52        : RMCommunicator notified that shouldUnregistered is: <:*:>\n",
      "ID=129   : size=52        : JobHistoryEventHandler notified that forceJobCompletion is <:*:>\n",
      "ID=130   : size=52        : Calling stop for all the services\n",
      "ID=131   : size=52        : Stopping JobHistoryEventHandler. Size of the outstanding queue size is <:NUM:>\n",
      "ID=126   : size=50        : We are finishing cleanly so this is the last retry\n",
      "ID=136   : size=48        : Stopped JobHistoryEventHandler. super.stop()\n",
      "ID=137   : size=48        : Setting job diagnostics to\n",
      "ID=138   : size=48        : History url is <:*:> <:NUM:> <:NUM:>\n",
      "ID=140   : size=48        : Final Stats: PendingReds:<:NUM:> ScheduledMaps:<:NUM:> ScheduledReds:<:NUM:> AssignedMaps:<:NUM:> AssignedReds:<:NUM:> CompletedMaps:<:NUM:> CompletedReds:<:NUM:> ContAlloc:<:NUM:> ContRel:<:NUM:> HostLocal:<:NUM:> RackLocal:<:NUM:>\n",
      "ID=142   : size=48        : Stopping server on <:NUM:>\n",
      "ID=143   : size=48        : Stopping IPC Server listener on <:NUM:>\n",
      "ID=144   : size=48        : TaskHeartbeatHandler thread interrupted\n",
      "ID=145   : size=48        : Stopping IPC Server Responder\n",
      "ID=193   : size=48        : Task attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> is allowed to commit now\n",
      "ID=194   : size=48        : Saved output of task 'attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>' to <:*:> temporary/<:NUM:>/task <:NUM:> <:NUM:> r <:NUM:>\n",
      "ID=117   : size=47        : Commit-pending state update from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "ID=118   : size=47        : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> TaskAttempt Transitioned from RUNNING to COMMIT PENDING\n",
      "ID=120   : size=47        : Commit go/no-go request from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "ID=121   : size=47        : Result of canCommit for attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>:true\n",
      "ID=123   : size=47        : attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> TaskAttempt Transitioned from COMMIT PENDING to SUCCESS CONTAINER CLEANUP\n",
      "ID=125   : size=47        : Calling handler for JobFinishedEvent\n",
      "ID=132   : size=47        : Copying hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist to hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "ID=134   : size=47        : Copying hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:> conf.xml to hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging\n",
      "ID=139   : size=45        : Waiting for application to be successfully unregistered.\n",
      "ID=141   : size=45        : Deleting staging directory hdfs://msra-sa-<:NUM:>:<:NUM:> /tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>\n",
      "ID=168   : size=44        : Communication exception: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\n",
      "ID=94    : size=43        : All maps assigned. Ramping up all remaining reduces:<:NUM:>\n",
      "ID=158   : size=31        : Ramping up <:NUM:>\n",
      "ID=196   : size=30        : Exception in createBlockOutputStream\n",
      "ID=197   : size=30        : Bad connect ack with firstBadLink as <:IP:>:<:NUM:>\n",
      "ID=198   : size=30        : Abandoning BP-<:NUM:>-<:IP:><:NUM:>:blk#<:NUM:> <:NUM:>\n",
      "ID=199   : size=30        : Excluding datanode <:IP:>:<:NUM:>\n",
      "ID=186   : size=29        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=207   : size=25        : RUNNABLE\n",
      "ID=220   : size=25        : TaskAttempt killed because it ran on unusable node <:*:> AttemptId:attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "ID=266   : size=24        : java.net.UnknownHostException: <:*:>\n",
      "ID=209   : size=21        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=206   : size=20        : TIMED WAITING\n",
      "ID=250   : size=20        : No route to host: no further information\n",
      "ID=45    : size=19        : Previous history file is at hdfs://msra-sa-<:NUM:>:<:NUM:>/tmp/hadoop-yarn/staging/msrabi/.staging/job <:NUM:> <:NUM:>/job <:NUM:> <:NUM:> <:NUM:>.jhist\n",
      "ID=150   : size=19        : Container complete event for unknown container id container <:NUM:> <:NUM:> <:NUM:> <:NUM:>\n",
      "ID=181   : size=19        : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>:\n",
      "ID=204   : size=18        : WAITING\n",
      "ID=241   : size=18        : java.io.IOException: There is not enough space on the disk\n",
      "ID=240   : size=17        : org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\n",
      "ID=28    : size=16        : I/O error constructing remote block reader.\n",
      "ID=238   : size=16        : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: <:*:> java.io.IOException: There is not enough space on the disk\n",
      "ID=44    : size=15        : Recovery is enabled. Will try to recover from previous life on best effort basis.\n",
      "ID=195   : size=15        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=228   : size=15        : ReduceTask metrics system shutdown complete.\n",
      "ID=114   : size=14        : DFSOutputStream ResponseProcessor exception for block BP-<:NUM:>-<:IP:><:NUM:>:blk#<:NUM:> <:NUM:>\n",
      "ID=115   : size=14        : Bad response ERROR for block BP-<:NUM:>-<:IP:><:NUM:>:blk#<:NUM:> <:NUM:> from datanode <:IP:>:<:NUM:>\n",
      "ID=116   : size=14        : Error Recovery for block BP-<:NUM:>-<:IP:><:NUM:>:blk#<:NUM:> <:NUM:> in pipeline <:IP:>:<:NUM:>, <:IP:>:<:NUM:>: bad datanode <:IP:>:<:NUM:>\n",
      "ID=261   : size=14        : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=148   : size=12        : Cannot assign container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] for a map as either container memory less than required <memory:<:NUM:>, vCores:<:NUM:>> or no pending map tasks - maps.isEmpty=true\n",
      "ID=187   : size=12        : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=265   : size=12        : cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: <:*:>\n",
      "ID=267   : size=12        : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: cleanup failed for container container <:NUM:> <:NUM:> <:NUM:> <:NUM:> : java.lang.IllegalArgumentException: java.net.UnknownHostException: <:*:>\n",
      "ID=47    : size=11        : Read completed tasks from history <:NUM:>\n",
      "ID=29    : size=10        : Connection timed out: no further information\n",
      "ID=201   : size=10        : Communication exception: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to <:IP:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=236   : size=10        : <:NUM:> failures on node <:*:>\n",
      "ID=243   : size=10        : Added attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> to list of failed maps\n",
      "ID=251   : size=10        : Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host: no further information\n",
      "ID=182   : size=9         : Slow ReadProcessor read fields took <:*:> (threshold=30000ms); ack: seqno: <:NUM:> status: SUCCESS status: ERROR downstreamAckTimeNanos: <:NUM:>, targets: [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>]\n",
      "ID=176   : size=8         : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=244   : size=8         : Assigning container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ] to fast fail map\n",
      "ID=245   : size=8         : Assigned from earlierFailedMaps\n",
      "ID=247   : size=8         : Incompatible event log version: null\n",
      "ID=271   : size=8         : java.net.SocketException: Permission denied: no further information\n",
      "ID=281   : size=8         : java.nio.channels.ClosedChannelException\n",
      "ID=165   : size=7         : Diagnostics report from attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:>: AttemptID:attempt <:NUM:> <:NUM:> <:*:> <:NUM:> <:NUM:> Timed out after <:NUM:> secs\n",
      "ID=215   : size=7         : Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\n",
      "ID=225   : size=7         : Merging <:NUM:> intermediate segments out of a total of <:NUM:>\n",
      "ID=229   : size=7         : Releasing unassigned and invalid container Container: [ContainerId: container <:NUM:> <:NUM:> <:NUM:> <:NUM:>, NodeId: <:*:> NodeHttpAddress: <:*:> Resource: <memory:<:NUM:>, vCores:<:NUM:>>, Priority: <:NUM:>, Token: Token { kind: ContainerToken, service: <:IP:>:<:NUM:> }, ]. RM may have assignment issues\n",
      "ID=31    : size=6         : Successfully connected to /<:IP:>:<:NUM:> for BP-<:NUM:>-<:IP:><:NUM:>:blk#<:NUM:> <:NUM:>\n",
      "ID=210   : size=6         : Runnning cleanup for the task\n",
      "ID=237   : size=6         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.io.IOException: There is not enough space on the disk\n",
      "ID=242   : size=6         : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: java.io.IOException: Spill failed\n",
      "ID=253   : size=6         : DFS chooseDataNode: got # <:NUM:> IOException, will wait for <:NUM:>.<:NUM:> msec.\n",
      "ID=257   : size=6         : Could not contact RM after <:NUM:> milliseconds.\n",
      "ID=30    : size=5         : Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information\n",
      "ID=183   : size=5         : In stop, writing event <:*:> FINISHED\n",
      "ID=203   : size=5         : Process Thread Dump: Communication exception\n",
      "ID=208   : size=5         : Last retry, killing attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "ID=252   : size=5         : Could not obtain BP-<:NUM:>-<:IP:><:NUM:>:blk#<:NUM:> <:NUM:> from any node: java.io.IOException: No live nodes contain block BP-<:NUM:>-<:IP:><:NUM:>:blk#<:NUM:> <:NUM:> after checking nodes = [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>], ignoredNodes = null No live nodes contain current block Block locations: <:IP:>:<:NUM:> <:IP:>:<:NUM:> Dead nodes: <:IP:>:<:NUM:> <:IP:>:<:NUM:>. Will get new block locations from namenode and retry...\n",
      "ID=246   : size=4         : Unable to parse prior job history, aborting recovery\n",
      "ID=248   : size=4         : Could not parse the old history file. Will not have old AMinfos\n",
      "ID=249   : size=4         : Failure sending status update: java.io.IOException: Failed on local exception: java.io.IOException: An existing connection was forcibly closed by the remote host; Host Details : local host is: <:*:> destination host is: <:*:>\n",
      "ID=259   : size=4         : Error writing History Event: <:*:>\n",
      "ID=286   : size=4         : Diagnostics report from attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>: Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\n",
      "ID=290   : size=4         : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=294   : size=4         : Attempt to process a enum when a union was expected.\n",
      "ID=219   : size=3         : Exception in getting events\n",
      "ID=239   : size=3         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.io.IOException: Spill failed\n",
      "ID=255   : size=3         : DFS Read\n",
      "ID=256   : size=3         : DataStreamer Exception\n",
      "ID=258   : size=3         : Error communicating with RM: Could not contact RM after <:NUM:> milliseconds.\n",
      "ID=260   : size=3         : Thread Thread[eventHandlingThread,<:NUM:>,main] threw an Exception.\n",
      "ID=268   : size=3         : Exception while unregistering\n",
      "ID=269   : size=3         : Skipping cleaning up the staging dir. assuming AM will be retried.\n",
      "ID=270   : size=3         : Graceful stop failed\n",
      "ID=288   : size=3         : for url=<:NUM:>/mapOutput?job=job <:NUM:> <:NUM:>&reduce=<:NUM:>&map=attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>,attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> sent hash and received reply\n",
      "ID=32    : size=2         : Connection refused: no further information\n",
      "ID=235   : size=2         : Diagnostics report from attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>: Error: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\n",
      "ID=262   : size=2         : Found jobId job <:NUM:> <:NUM:> to have not been closed. Will close\n",
      "ID=263   : size=2         : Service <:*:> failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=275   : size=2         : Error communicating with RM: Resource Manager doesn't recognize AttemptId: application <:NUM:> <:NUM:>\n",
      "ID=276   : size=2         : Resource Manager doesn't recognize AttemptId: application <:NUM:> <:NUM:>\n",
      "ID=277   : size=2         : org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException: Application attempt appattempt <:NUM:> <:NUM:> <:NUM:> doesn't exist in ApplicationMasterService cache.\n",
      "ID=278   : size=2         : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException): Application attempt appattempt <:NUM:> <:NUM:> <:NUM:> doesn't exist in ApplicationMasterService cache.\n",
      "ID=280   : size=2         : Service <:*:> failed in state STOPPED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n",
      "ID=283   : size=2         : Exception running child : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=284   : size=2         : Exception cleaning up: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=285   : size=2         : Task: attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:> - exited : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\n",
      "ID=287   : size=2         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - failed due to FSError: java.io.IOException: There is not enough space on the disk\n",
      "ID=289   : size=2         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=291   : size=2         : Task cleanup failed for attempt attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>\n",
      "ID=293   : size=2         : Service <:*:> failed in state STOPPED; cause: org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\n",
      "ID=296   : size=2         : Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#<:NUM:>\n",
      "ID=297   : size=2         : Shuffle failed : local error on this node: 04DN8IQ/<:IP:>\n",
      "ID=33    : size=1         : Failed to connect to /<:IP:>:<:NUM:> for block, add to deadNodes and continue. java.net.ConnectException: Connection refused: no further information\n",
      "ID=223   : size=1         : Preempting attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "ID=224   : size=1         : Reduce preemption successful attempt <:NUM:> <:NUM:> r <:NUM:> <:NUM:>\n",
      "ID=226   : size=1         : Communication exception: java.net.ConnectException: Call From MSRA-SA-<:NUM:>/<:IP:> to minint-fnanli5.fareast.corp.microsoft.com:<:NUM:> failed on connection exception: java.net.ConnectException: Connection timed out: no further information; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused\n",
      "ID=227   : size=1         : java.net.ConnectException: Connection timed out: no further information\n",
      "ID=230   : size=1         : Connection retry failed with <:NUM:> attempts in <:NUM:> seconds\n",
      "ID=231   : size=1         : Failed to connect to MININT-FNANLI5.fareast.corp.microsoft.com:<:NUM:> with <:NUM:> map outputs\n",
      "ID=232   : size=1         : Connection timed out: connect\n",
      "ID=233   : size=1         : Reporting fetch failure for attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> to jobtracker.\n",
      "ID=234   : size=1         : Task: attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> - exited : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\n",
      "ID=254   : size=1         : Could not obtain BP-<:NUM:>-<:IP:><:NUM:>:blk#<:NUM:> <:NUM:> from any node: java.io.IOException: No live nodes contain block BP-<:NUM:>-<:IP:><:NUM:>:blk#<:NUM:> <:NUM:> after checking nodes = [<:IP:>:<:NUM:>, <:IP:>:<:NUM:>], ignoredNodes = null No live nodes contain current block Block locations: <:IP:>:<:NUM:> <:IP:>:<:NUM:> Dead nodes: <:IP:>:<:NUM:> <:IP:>:<:NUM:> <:IP:>:<:NUM:>. Will get new block locations from namenode and retry...\n",
      "ID=264   : size=1         : When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from MININT-FNANLI5/<:IP:> to msra-sa-<:NUM:>:<:NUM:> failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "ID=272   : size=1         : MapCompletionEvents reques\n",
      "ID=273   : size=1         : WordCount and PageRank. Each application has been run for several times, simulating both normal and abnormal cases with injected specific failures.\n",
      "ID=274   : size=1         : Task attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:> failed : org.apache.hadoop.fs.FSError: java.io.IOException: There is not enough space on the disk\n",
      "ID=279   : size=1         : Error closing writer for JobID: job <:NUM:> <:NUM:>\n",
      "ID=282   : size=1         : When stopping the service JobHistoryEventHandler : org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n",
      "ID=292   : size=1         : In stop, writing event MAP ATTEMPT FAILED\n",
      "ID=295   : size=1         : When stopping the service JobHistoryEventHandler : org.apache.avro.AvroTypeException: Attempt to process a enum when a union was expected.\n",
      "ID=298   : size=1         : Ignoring exception during close for org.apache.hadoop.mapred.MapTask$NewOutputCollector@6a6585ee\n",
      "ID=299   : size=1         : Exception running child : org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>/file.out\n",
      "ID=300   : size=1         : IPC Server handler <:NUM:> on <:NUM:>, call statusUpdate(attempt <:NUM:> <:NUM:> m <:NUM:> <:NUM:>, org.apache.hadoop.mapred.MapTaskStatus@cdcdbf7), rpc version=<:NUM:>, client version=<:NUM:>, methodsFingerPrint=<:NUM:> from <:IP:>:<:NUM:> Call#<:NUM:> Retry#<:NUM:>: output error\n",
      "ID=301   : size=1         : IPC Server handler <:NUM:> on <:NUM:> caught an exception\n"
     ]
    }
   ],
   "source": [
    "mine_template = mineModel(f = open(\"../data/merged_hadoop/merged.log\"),cfg='../data/drain3.ini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "def57a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_log = ['1445087491445_0005','1445087491445_0007', \n",
    "'1445175094696_0005','1445062781478_0011','1445062781478_0016','1445062781478_0019'\n",
    "'1445076437777_0002','1445076437777_0005','1445144423722_0021','1445144423722_0024'\n",
    "'1445182159119_0012']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "904ba29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total          : took    12.06 s (100.00%),    193,633 samples,   62.29 ms / 1000 samples,       16,052.84 hz\n",
      "mask           : took     7.40 s ( 61.36%),    193,633 samples,   38.22 ms / 1000 samples,       26,162.14 hz\n",
      "drain          : took     3.46 s ( 28.67%),    193,633 samples,   17.86 ms / 1000 samples,       55,999.23 hz\n",
      "tree_search    : took     1.37 s ( 11.36%),    193,633 samples,    7.08 ms / 1000 samples,      141,287.14 hz\n",
      "cluster_exist  : took     0.92 s (  7.63%),    193,332 samples,    4.76 ms / 1000 samples,      210,105.46 hz\n",
      "create_cluster : took     0.01 s (  0.06%),        301 samples,   25.32 ms / 1000 samples,       39,495.87 hz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = 'Successfully connected to /10.190.173.170:50010 for BP-1347369012-10.190.173.170-1444972147527:blk_1073742826_2022'\n",
    "mine_template.add_log_message(line)\n",
    "match = mine_template.match(line)\n",
    "match.cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30f89434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to /10.190.173.170:50010 for BP-1347369012-10.190.173.170-1444972147527:blk_1073742826_2022\n",
      "Successfully connected to /10.190.173.170:50010 for BP-1347369012-10.190.173.170-1444972147527:blk_1073742829_2025\n",
      "DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743028_2240\n",
      "DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743509_2728\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073743056_2272\n",
      "Successfully connected to /10.190.173.170:50010 for BP-1347369012-10.190.173.170-1444972147527:blk_1073742826_2022\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073743517_2738\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073742931_2130\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073742903_2102\n",
      "DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073742860_2056\n",
      "Successfully connected to /10.190.173.170:50010 for BP-1347369012-10.190.173.170-1444972147527:blk_1073742635_1830\n",
      "DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743510_2729\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073742898_2097\n",
      "DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743032_2244\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073742663_1858\n",
      "DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743034_2246\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073742518_1713\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073743677_2902\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073743688_2913\n",
      "Could not obtain BP-1347369012-10.190.173.170-1444972147527:blk_1073742989_2201 from any node: java.io.IOException: No live nodes contain block BP-1347369012-10.190.173.170-1444972147527:blk_1073742989_2201 after checking nodes = [172.22.149.145:50010, 10.190.173.170:50010], ignoredNodes = null No live nodes contain current block Block locations: 172.22.149.145:50010 10.190.173.170:50010 Dead nodes:  10.190.173.170:50010 172.22.149.145:50010. Will get new block locations from namenode and retry...\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073744061_3286\n",
      "DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743653_2876\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073742515_1710\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073743674_2899\n",
      "DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743655_2878\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073742531_1726\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073744058_3283\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073742517_1712\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073744056_3281\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073743071_2287\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073742524_1719\n",
      "DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743654_2877\n",
      "DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073742514_1708\n",
      "Successfully connected to /10.190.173.170:50010 for BP-1347369012-10.190.173.170-1444972147527:blk_1073744042_3267\n",
      "Successfully connected to /10.86.169.121:50010 for BP-1347369012-10.190.173.170-1444972147527:blk_1073744042_3267\n",
      "Successfully connected to /10.86.169.121:50010 for BP-1347369012-10.190.173.170-1444972147527:blk_1073744042_3267\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073743665_2890\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073742895_2094\n",
      "DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073742862_2058\n",
      "Could not obtain BP-1347369012-10.190.173.170-1444972147527:blk_1073741828_1004 from any node: java.io.IOException: No live nodes contain block BP-1347369012-10.190.173.170-1444972147527:blk_1073741828_1004 after checking nodes = [172.22.149.145:50010, 10.190.173.170:50010], ignoredNodes = null No live nodes contain current block Block locations: 172.22.149.145:50010 10.190.173.170:50010 Dead nodes:  10.190.173.170:50010 172.22.149.145:50010. Will get new block locations from namenode and retry...\n",
      "DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743512_2731\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073742660_1855\n",
      "Abandoning BP-1347369012-10.190.173.170-1444972147527:blk_1073742656_1851\n",
      "DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073742863_2059\n",
      "Could not obtain BP-1347369012-10.190.173.170-1444972147527:blk_1073741827_1003 from any node: java.io.IOException: No live nodes contain block BP-1347369012-10.190.173.170-1444972147527:blk_1073741827_1003 after checking nodes = [172.22.149.145:50010, 10.190.173.170:50010], ignoredNodes = null No live nodes contain current block Block locations: 172.22.149.145:50010 10.190.173.170:50010 Dead nodes:  10.190.173.170:50010 172.22.149.145:50010. Will get new block locations from namenode and retry...\n",
      "DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743025_2237\n"
     ]
    }
   ],
   "source": [
    "label = []\n",
    "events = []\n",
    "identifier = []\n",
    "\n",
    "# open files one by one\n",
    "meragefiledir = '../data/merged_hadoop/'\n",
    "filenames=os.listdir(meragefiledir)  \n",
    "   \n",
    "for filename in filenames: \n",
    "    sequence = ''\n",
    "    filepath=meragefiledir+filename   \n",
    "    if filename[10:28] in normal_log:\n",
    "        label.append(0)\n",
    "    else:\n",
    "        label.append(1) \n",
    "    for line in open(filepath):  \n",
    "        try:\n",
    "            line = line.rstrip()\n",
    "            line = line.partition(\": \")[2]\n",
    "            line = line.replace('blk_','blk#')\n",
    "            if line == '' : continue\n",
    "            match = mine_template.match(line)\n",
    "            sequence=sequence + ('E'+ str(match.cluster_id)+ ' ')\n",
    "        except:\n",
    "            print(line)\n",
    "            break        \n",
    "    identifier.append(filename)\n",
    "    events.append(sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0563bdb",
   "metadata": {},
   "source": [
    "### Parse Log files from HDFS\n",
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5c5871b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (1): Receiving block blk#-1608999687919862906 src: /10.250.19.102:54106 dest: /10.250.19.102:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 1, \"cluster_size\": 1, \"template_mined\": \"Receiving block <:BLOCKID:> src: /10.250.19.102:54106 dest: /10.250.19.102:50010\", \"cluster_count\": 1}\n",
      "Input (2): BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.jar. blk#-1608999687919862906\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 2, \"cluster_size\": 1, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job 200811092030 0001/job.jar. <:BLOCKID:>\", \"cluster_count\": 2}\n",
      "Input (3): Receiving block blk#-1608999687919862906 src: /10.250.10.6:40524 dest: /10.250.10.6:50010\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 1, \"cluster_size\": 2, \"template_mined\": \"Receiving block <:BLOCKID:> src: <:*:> dest: <:*:>\", \"cluster_count\": 2}\n",
      "Input (5): PacketResponder 1 for block blk#-1608999687919862906 terminating\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 3, \"cluster_size\": 1, \"template_mined\": \"PacketResponder 1 for block <:BLOCKID:> terminating\", \"cluster_count\": 3}\n",
      "Input (6): PacketResponder 2 for block blk#-1608999687919862906 terminating\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 3, \"cluster_size\": 2, \"template_mined\": \"PacketResponder <:*:> for block <:BLOCKID:> terminating\", \"cluster_count\": 3}\n",
      "Input (7): Received block blk#-1608999687919862906 of size 91178 from /10.250.10.6\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 4, \"cluster_size\": 1, \"template_mined\": \"Received block <:BLOCKID:> of size 91178 from /10.250.10.6\", \"cluster_count\": 4}\n",
      "Input (8): Received block blk#-1608999687919862906 of size 91178 from /10.250.19.102\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 4, \"cluster_size\": 2, \"template_mined\": \"Received block <:BLOCKID:> of size 91178 from <:*:>\", \"cluster_count\": 4}\n",
      "Input (11): BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.10.6:50010 is added to blk#-1608999687919862906 size 91178\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 5, \"cluster_size\": 1, \"template_mined\": \"BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.10.6:50010 is added to <:BLOCKID:> size 91178\", \"cluster_count\": 5}\n",
      "Input (12): BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.209:50010 is added to blk#-1608999687919862906 size 91178\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 5, \"cluster_size\": 2, \"template_mined\": \"BLOCK* NameSystem.addStoredBlock: blockMap updated: <:*:> is added to <:BLOCKID:> size 91178\", \"cluster_count\": 5}\n",
      "Input (16): BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.split. blk#7503483334202473044\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 2, \"cluster_size\": 2, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job 200811092030 <:*:> <:BLOCKID:>\", \"cluster_count\": 5}\n",
      "Input (17): Received block blk#-1608999687919862906 src: /10.251.215.16:52002 dest: /10.251.215.16:50010 of size 91178\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 6, \"cluster_size\": 1, \"template_mined\": \"Received block <:BLOCKID:> src: /10.251.215.16:52002 dest: /10.251.215.16:50010 of size 91178\", \"cluster_count\": 6}\n",
      "Input (22): Received block blk#7503483334202473044 of size 233217 from /10.251.215.16\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 4, \"cluster_size\": 4, \"template_mined\": \"Received block <:BLOCKID:> of size <:*:> from <:*:>\", \"cluster_count\": 6}\n",
      "Input (25): 10.250.14.224:50010:Transmitted block blk#-1608999687919862906 to /10.251.215.16:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 7, \"cluster_size\": 1, \"template_mined\": \"10.250.14.224:50010:Transmitted block <:BLOCKID:> to /10.251.215.16:50010\", \"cluster_count\": 7}\n",
      "Input (26): Received block blk#-1608999687919862906 src: /10.250.14.224:35754 dest: /10.250.14.224:50010 of size 91178\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 6, \"cluster_size\": 2, \"template_mined\": \"Received block <:BLOCKID:> src: <:*:> dest: <:*:> of size 91178\", \"cluster_count\": 7}\n",
      "Input (30): 10.250.14.224:50010 Starting thread to transfer block blk#-1608999687919862906 to 10.251.215.16:50010, 10.251.71.193:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 8, \"cluster_size\": 1, \"template_mined\": \"10.250.14.224:50010 Starting thread to transfer block <:BLOCKID:> to 10.251.215.16:50010, 10.251.71.193:50010\", \"cluster_count\": 8}\n",
      "Input (31): BLOCK* ask 10.250.14.224:50010 to replicate blk#-1608999687919862906 to datanode(s) 10.251.215.16:50010 10.251.71.193:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 9, \"cluster_size\": 1, \"template_mined\": \"BLOCK* ask 10.250.14.224:50010 to replicate <:BLOCKID:> to datanode(s) 10.251.215.16:50010 10.251.71.193:50010\", \"cluster_count\": 9}\n",
      "Input (32): BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.106.10:50010 is added to blk#7503483334202473044 size 233217\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 5, \"cluster_size\": 4, \"template_mined\": \"BLOCK* NameSystem.addStoredBlock: blockMap updated: <:*:> is added to <:BLOCKID:> size <:*:>\", \"cluster_count\": 9}\n",
      "Input (47): 10.250.11.100:50010 Served block blk#-3544583377289625738 to /10.250.19.102\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 10, \"cluster_size\": 1, \"template_mined\": \"10.250.11.100:50010 Served block <:BLOCKID:> to /10.250.19.102\", \"cluster_count\": 10}\n",
      "Input (48): 10.251.111.209:50010 Served block blk#-1608999687919862906 to /10.250.19.102\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 10, \"cluster_size\": 2, \"template_mined\": \"<:*:> Served block <:BLOCKID:> to /10.250.19.102\", \"cluster_count\": 10}\n",
      "Input (49): BLOCK* NameSystem.allocateBlock: /user/root/rand/_logs/history/ip-10-250-19-102.ec2.internal_1226291400491_job_200811092030_0001_conf.xml. blk#-9073992586687739851\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 11, \"cluster_size\": 1, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: /user/root/rand/ logs/history/ip-10-250-19-102.ec2.internal 1226291400491 job 200811092030 0001 conf.xml. <:BLOCKID:>\", \"cluster_count\": 11}\n",
      "Input (61): BLOCK* ask 10.251.215.16:50010 to replicate blk#-1608999687919862906 to datanode(s) 10.251.74.79:50010 10.251.107.19:50010\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 9, \"cluster_size\": 2, \"template_mined\": \"BLOCK* ask <:*:> to replicate <:BLOCKID:> to datanode(s) <:*:> <:*:>\", \"cluster_count\": 11}\n",
      "Input (70): 10.251.39.179:50010 Served block blk#-3544583377289625738 to /10.250.18.114\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 10, \"cluster_size\": 4, \"template_mined\": \"<:*:> Served block <:BLOCKID:> to <:*:>\", \"cluster_count\": 11}\n",
      "Input (73): 10.251.215.16:50010:Transmitted block blk#-1608999687919862906 to /10.251.74.79:50010\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 7, \"cluster_size\": 2, \"template_mined\": \"<:*:> block <:BLOCKID:> to <:*:>\", \"cluster_count\": 11}\n",
      "Input (74): 10.251.215.16:50010 Starting thread to transfer block blk#-1608999687919862906 to 10.251.74.79:50010, 10.251.107.19:50010\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 8, \"cluster_size\": 2, \"template_mined\": \"<:*:> Starting thread to transfer block <:BLOCKID:> to <:*:> <:*:>\", \"cluster_count\": 11}\n",
      "Input (201): BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811092030_0001_m_000079_0/part-00079. blk#7854771516489510256\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 12, \"cluster_size\": 1, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: /user/root/rand/ temporary/ task 200811092030 0001 m 000079 0/part-00079. <:BLOCKID:>\", \"cluster_count\": 12}\n",
      "Input (271): BLOCK* ask 10.251.31.5:50010 to replicate blk#-1608999687919862906 to datanode(s) 10.251.90.64:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 13, \"cluster_size\": 1, \"template_mined\": \"BLOCK* ask 10.251.31.5:50010 to replicate <:BLOCKID:> to datanode(s) 10.251.90.64:50010\", \"cluster_count\": 13}\n",
      "Input (272): BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811092030_0001_m_000111_0/part-00111. blk#1717858812220360316\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 12, \"cluster_size\": 2, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: /user/root/rand/ temporary/ task 200811092030 0001 m <:*:> <:*:> <:BLOCKID:>\", \"cluster_count\": 13}\n",
      "Input (393): 10.251.31.5:50010 Starting thread to transfer block blk#-1608999687919862906 to 10.251.90.64:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 14, \"cluster_size\": 1, \"template_mined\": \"10.251.31.5:50010 Starting thread to transfer block <:BLOCKID:> to 10.251.90.64:50010\", \"cluster_count\": 14}\n",
      "Input (2082): Verification succeeded for blk#-9073992586687739851\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 15, \"cluster_size\": 1, \"template_mined\": \"Verification succeeded for <:BLOCKID:>\", \"cluster_count\": 15}\n",
      "Input (6336): writeBlock blk#-3102267849859399193 received exception java.net.SocketTimeoutException\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 16, \"cluster_size\": 1, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.net.SocketTimeoutException\", \"cluster_count\": 16}\n",
      "Input (6493): PacketResponder blk#-3102267849859399193 2 Exception java.io.EOFException\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 17, \"cluster_size\": 1, \"template_mined\": \"PacketResponder <:BLOCKID:> 2 Exception java.io.EOFException\", \"cluster_count\": 17}\n",
      "Processing line: 10000, rate 14194.2 lines/sec, 17 clusters so far.\n",
      "Processing line: 20000, rate 41977.5 lines/sec, 17 clusters so far.\n",
      "Input (28755): writeBlock blk#7648436334209344608 received exception java.io.IOException: Could not read from stream\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 18, \"cluster_size\": 1, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.io.IOException: Could not read from stream\", \"cluster_count\": 18}\n",
      "Processing line: 30000, rate 40318.4 lines/sec, 18 clusters so far.\n",
      "Processing line: 40000, rate 32534.9 lines/sec, 18 clusters so far.\n",
      "Processing line: 50000, rate 36903.1 lines/sec, 18 clusters so far.\n",
      "Processing line: 60000, rate 43020.8 lines/sec, 18 clusters so far.\n",
      "Processing line: 70000, rate 36331.8 lines/sec, 18 clusters so far.\n",
      "Input (72217): BLOCK* ask 10.251.43.21:50010 to replicate blk#-8213344449220111733 to datanode(s) 10.251.30.134:50010\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 13, \"cluster_size\": 2, \"template_mined\": \"BLOCK* ask <:*:> to replicate <:BLOCKID:> to datanode(s) <:*:>\", \"cluster_count\": 18}\n",
      "Input (72372): 10.251.43.21:50010 Starting thread to transfer block blk#-8213344449220111733 to 10.251.30.134:50010\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 14, \"cluster_size\": 2, \"template_mined\": \"<:*:> Starting thread to transfer block <:BLOCKID:> to <:*:>\", \"cluster_count\": 18}\n",
      "Input (72539): Received block blk#-8213344449220111733 src: /10.251.43.21:44237 dest: /10.251.43.21:50010 of size 3553474\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 6, \"cluster_size\": 8, \"template_mined\": \"Received block <:BLOCKID:> src: <:*:> dest: <:*:> of size <:*:>\", \"cluster_count\": 18}\n",
      "Input (72694): Deleting block blk#-8213344449220111733 file /mnt/hadoop/dfs/data/current/subdir39/blk#-8213344449220111733\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 7, \"cluster_size\": 6, \"template_mined\": \"<:*:> block <:BLOCKID:> <:*:> <:*:>\", \"cluster_count\": 18}\n",
      "Input (73506): Receiving empty packet for block blk#-3842070622043972712\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 19, \"cluster_size\": 1, \"template_mined\": \"Receiving empty packet for block <:BLOCKID:>\", \"cluster_count\": 19}\n",
      "Input (79121): BLOCK* NameSystem.allocateBlock: /user/root/rand/_logs/history/ip-10-250-19-102.ec2.internal_1226291400491_job_200811092030_0001_root_random-writer. blk#-7007586470894854585\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 20, \"cluster_size\": 1, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: /user/root/rand/ logs/history/ip-10-250-19-102.ec2.internal 1226291400491 job 200811092030 0001 root random-writer. <:BLOCKID:>\", \"cluster_count\": 20}\n",
      "Processing line: 80000, rate 34148.7 lines/sec, 20 clusters so far.\n",
      "Processing line: 90000, rate 34340.7 lines/sec, 20 clusters so far.\n",
      "Input (91149): Exception in receiveBlock for block blk#-3102267849859399193 java.io.IOException: Connection reset by peer\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 21, \"cluster_size\": 1, \"template_mined\": \"Exception in receiveBlock for block <:BLOCKID:> java.io.IOException: Connection reset by peer\", \"cluster_count\": 21}\n",
      "Input (91150): writeBlock blk#-3102267849859399193 received exception java.io.IOException: Connection reset by peer\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 22, \"cluster_size\": 1, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.io.IOException: Connection reset by peer\", \"cluster_count\": 22}\n",
      "Processing line: 100000, rate 36322.7 lines/sec, 22 clusters so far.\n",
      "Input (105299): BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for blk#3888635850409849568 on 10.251.107.227:50010 size 67108864\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 23, \"cluster_size\": 1, \"template_mined\": \"BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for <:BLOCKID:> on 10.251.107.227:50010 size 67108864\", \"cluster_count\": 23}\n",
      "Processing line: 110000, rate 40313.4 lines/sec, 23 clusters so far.\n",
      "Input (118679): PacketResponder blk#4241467193520768333 2 Exception java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.251.107.242:58971 remote=/10.251.123.33:50010]\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 24, \"cluster_size\": 1, \"template_mined\": \"PacketResponder <:BLOCKID:> 2 Exception java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.251.107.242:58971 remote=/10.251.123.33:50010]\", \"cluster_count\": 24}\n",
      "Processing line: 120000, rate 42627.8 lines/sec, 24 clusters so far.\n",
      "Input (123967): PacketResponder blk#4241467193520768333 1 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.251.123.33:39066 remote=/10.250.7.32:50010]. 59942 millis timeout left.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 25, \"cluster_size\": 1, \"template_mined\": \"PacketResponder <:BLOCKID:> 1 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.251.123.33:39066 remote=/10.250.7.32:50010]. 59942 millis timeout left.\", \"cluster_count\": 25}\n",
      "Input (124080): Exception in receiveBlock for block blk#4241467193520768333 java.io.EOFException\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 26, \"cluster_size\": 1, \"template_mined\": \"Exception in receiveBlock for block <:BLOCKID:> java.io.EOFException\", \"cluster_count\": 26}\n",
      "Input (124081): writeBlock blk#4241467193520768333 received exception java.io.EOFException\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 16, \"cluster_size\": 2, \"template_mined\": \"writeBlock <:BLOCKID:> received exception <:*:>\", \"cluster_count\": 26}\n",
      "Input (124082): PacketResponder 0 for block blk#4241467193520768333 Interrupted.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 3, \"cluster_size\": 27684, \"template_mined\": \"PacketResponder <:*:> for block <:BLOCKID:> <:*:>\", \"cluster_count\": 26}\n",
      "Input (124116): Changing block file offset of block blk#4241467193520768333 from 0 to 30867456 meta file offset to 241159\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 27, \"cluster_size\": 1, \"template_mined\": \"Changing block file offset of block <:BLOCKID:> from 0 to 30867456 meta file offset to 241159\", \"cluster_count\": 27}\n",
      "Input (126473): BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for blk#-8692505341300910301 on 10.251.43.192:50010 size 67108864\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 23, \"cluster_size\": 2, \"template_mined\": \"BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for <:BLOCKID:> on <:*:> size 67108864\", \"cluster_count\": 27}\n",
      "Processing line: 130000, rate 33176.8 lines/sec, 27 clusters so far.\n",
      "Processing line: 140000, rate 40856.0 lines/sec, 27 clusters so far.\n",
      "Processing line: 150000, rate 41214.7 lines/sec, 27 clusters so far.\n",
      "Processing line: 160000, rate 24951.1 lines/sec, 27 clusters so far.\n",
      "Processing line: 170000, rate 39180.5 lines/sec, 27 clusters so far.\n",
      "Processing line: 180000, rate 43838.7 lines/sec, 27 clusters so far.\n",
      "Processing line: 190000, rate 44091.1 lines/sec, 27 clusters so far.\n",
      "Processing line: 200000, rate 43441.2 lines/sec, 27 clusters so far.\n",
      "Processing line: 210000, rate 44652.0 lines/sec, 27 clusters so far.\n",
      "Processing line: 220000, rate 43945.0 lines/sec, 27 clusters so far.\n",
      "Processing line: 230000, rate 42013.5 lines/sec, 27 clusters so far.\n",
      "Processing line: 240000, rate 45291.7 lines/sec, 27 clusters so far.\n",
      "Input (246878): PacketResponder blk#3858821904894294369 2 Exception java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.250.13.188:58401 remote=/10.251.39.160:50010]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 24, \"cluster_size\": 2, \"template_mined\": \"PacketResponder <:BLOCKID:> 2 Exception java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected <:*:> <:*:>\", \"cluster_count\": 27}\n",
      "Input (248163): PacketResponder blk#3858821904894294369 1 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.251.39.160:57395 remote=/10.251.214.175:50010]. 59728 millis timeout left.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 25, \"cluster_size\": 2, \"template_mined\": \"PacketResponder <:BLOCKID:> 1 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected <:*:> <:*:> <:*:> millis timeout left.\", \"cluster_count\": 27}\n",
      "Input (248164): 10.250.13.188:50010:Exception writing block blk#3858821904894294369 to mirror 10.251.39.160:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 28, \"cluster_size\": 1, \"template_mined\": \"10.250.13.188:50010:Exception writing block <:BLOCKID:> to mirror 10.251.39.160:50010\", \"cluster_count\": 28}\n",
      "Input (248165): Exception in receiveBlock for block blk#3858821904894294369 java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.250.13.188:58401 remote=/10.251.39.160:50010]. 489996 millis timeout left.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 29, \"cluster_size\": 1, \"template_mined\": \"Exception in receiveBlock for block <:BLOCKID:> java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.250.13.188:58401 remote=/10.251.39.160:50010]. 489996 millis timeout left.\", \"cluster_count\": 29}\n",
      "Input (248166): writeBlock blk#3858821904894294369 received exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.250.13.188:58401 remote=/10.251.39.160:50010]. 489996 millis timeout left.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 30, \"cluster_size\": 1, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.250.13.188:58401 remote=/10.251.39.160:50010]. 489996 millis timeout left.\", \"cluster_count\": 30}\n",
      "Input (248325): PacketResponder blk#3858821904894294369 0 Exception java.io.IOException: Broken pipe\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 31, \"cluster_size\": 1, \"template_mined\": \"PacketResponder <:BLOCKID:> 0 Exception java.io.IOException: Broken pipe\", \"cluster_count\": 31}\n",
      "Input (248364): Changing block file offset of block blk#3858821904894294369 from 0 to 23724032 meta file offset to 185351\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 27, \"cluster_size\": 3, \"template_mined\": \"Changing block file offset of block <:BLOCKID:> from 0 to <:*:> meta file offset to <:*:>\", \"cluster_count\": 31}\n",
      "Processing line: 250000, rate 39437.6 lines/sec, 31 clusters so far.\n",
      "Processing line: 260000, rate 42903.9 lines/sec, 31 clusters so far.\n",
      "Processing line: 270000, rate 43184.6 lines/sec, 31 clusters so far.\n",
      "Processing line: 280000, rate 45143.2 lines/sec, 31 clusters so far.\n",
      "Input (282885): writeBlock blk#992101295951175683 received exception java.io.IOException: Block blk#992101295951175683 is valid, and cannot be written to.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 32, \"cluster_size\": 1, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.io.IOException: Block <:BLOCKID:> is valid, and cannot be written to.\", \"cluster_count\": 32}\n",
      "Input (282919): 10.251.194.147:50010:Failed to transfer blk#992101295951175683 to 10.251.214.112:50010 got java.io.IOException: Connection reset by peer\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 33, \"cluster_size\": 1, \"template_mined\": \"10.251.194.147:50010:Failed to transfer <:BLOCKID:> to 10.251.214.112:50010 got java.io.IOException: Connection reset by peer\", \"cluster_count\": 33}\n",
      "Processing line: 290000, rate 41692.4 lines/sec, 33 clusters so far.\n",
      "Processing line: 300000, rate 44390.0 lines/sec, 33 clusters so far.\n",
      "Processing line: 310000, rate 44452.8 lines/sec, 33 clusters so far.\n",
      "Processing line: 320000, rate 45229.6 lines/sec, 33 clusters so far.\n",
      "Processing line: 330000, rate 45047.4 lines/sec, 33 clusters so far.\n",
      "Input (331835): 10.251.39.242:50010:Failed to transfer blk#5862064168004546076 to 10.251.35.1:50010 got java.io.IOException: Connection reset by peer\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 33, \"cluster_size\": 2, \"template_mined\": \"<:*:> to transfer <:BLOCKID:> to <:*:> got java.io.IOException: Connection reset by peer\", \"cluster_count\": 33}\n",
      "Processing line: 340000, rate 43051.2 lines/sec, 33 clusters so far.\n",
      "Processing line: 350000, rate 43259.6 lines/sec, 33 clusters so far.\n",
      "Processing line: 360000, rate 45223.3 lines/sec, 33 clusters so far.\n",
      "Processing line: 370000, rate 45343.5 lines/sec, 33 clusters so far.\n",
      "Processing line: 380000, rate 44297.8 lines/sec, 33 clusters so far.\n",
      "Processing line: 390000, rate 44666.8 lines/sec, 33 clusters so far.\n",
      "Processing line: 400000, rate 45323.5 lines/sec, 33 clusters so far.\n",
      "Processing line: 410000, rate 44478.1 lines/sec, 33 clusters so far.\n",
      "Processing line: 420000, rate 44589.5 lines/sec, 33 clusters so far.\n",
      "Processing line: 430000, rate 43612.2 lines/sec, 33 clusters so far.\n",
      "Processing line: 440000, rate 44785.8 lines/sec, 33 clusters so far.\n",
      "Input (444883): BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for blk#-3174232733041654340 on 10.251.106.10:50010 size 44607007\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 23, \"cluster_size\": 28, \"template_mined\": \"BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for <:BLOCKID:> on <:*:> size <:*:>\", \"cluster_count\": 33}\n",
      "Processing line: 450000, rate 42964.4 lines/sec, 33 clusters so far.\n",
      "Processing line: 460000, rate 45124.7 lines/sec, 33 clusters so far.\n",
      "Processing line: 470000, rate 43735.7 lines/sec, 33 clusters so far.\n",
      "Input (472837): BLOCK* NameSystem.delete: blk#-1608999687919862906 is added to invalidSet of 10.250.10.6:50010\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 34, \"cluster_size\": 1, \"template_mined\": \"BLOCK* NameSystem.delete: <:BLOCKID:> is added to invalidSet of 10.250.10.6:50010\", \"cluster_count\": 34}\n",
      "Input (472838): BLOCK* NameSystem.delete: blk#-1608999687919862906 is added to invalidSet of 10.250.14.224:50010\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 34, \"cluster_size\": 2, \"template_mined\": \"BLOCK* NameSystem.delete: <:BLOCKID:> is added to invalidSet of <:*:>\", \"cluster_count\": 34}\n",
      "Input (474740): Unexpected error trying to delete block blk#-1067131609371010449. BlockInfo not found in volumeMap.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 35, \"cluster_size\": 1, \"template_mined\": \"Unexpected error trying to delete block <:BLOCKID:>. BlockInfo not found in volumeMap.\", \"cluster_count\": 35}\n",
      "Input (478206): BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for blk#5398314277015661293 on 10.251.199.86:50010 size 67108864 But it does not belong to any file.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 36, \"cluster_size\": 1, \"template_mined\": \"BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for <:BLOCKID:> on 10.251.199.86:50010 size 67108864 But it does not belong to any file.\", \"cluster_count\": 36}\n",
      "Input (478208): BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for blk#5398314277015661293 on 10.251.122.79:50010 size 67108864 But it does not belong to any file.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 36, \"cluster_size\": 2, \"template_mined\": \"BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for <:BLOCKID:> on <:*:> size 67108864 But it does not belong to any file.\", \"cluster_count\": 36}\n",
      "Input (478315): BLOCK* NameSystem.allocateBlock: /user/root/sortrand/_logs/history/ip-10-250-19-102.ec2.internal_1226291400491_job_200811092030_0002_conf.xml. blk#-4615135864434101729\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 11, \"cluster_size\": 2, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: <:*:> logs/history/ip-10-250-19-102.ec2.internal 1226291400491 job 200811092030 <:*:> conf.xml. <:BLOCKID:>\", \"cluster_count\": 36}\n",
      "Processing line: 480000, rate 41975.4 lines/sec, 36 clusters so far.\n",
      "Input (481408): 10.251.71.68:50010:Got exception while serving blk#-8781759536960110370 to /10.250.17.225:\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 37, \"cluster_size\": 1, \"template_mined\": \"10.251.71.68:50010:Got exception while serving <:BLOCKID:> to /10.250.17.225:\", \"cluster_count\": 37}\n",
      "Input (481409): 10.251.27.63:50010:Got exception while serving blk#1732876454483854279 to /10.251.27.63:\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 37, \"cluster_size\": 2, \"template_mined\": \"<:*:> exception while serving <:BLOCKID:> to <:*:>\", \"cluster_count\": 37}\n",
      "Input (482608): BLOCK* NameSystem.allocateBlock: /user/root/sortrand/_logs/history/ip-10-250-19-102.ec2.internal_1226291400491_job_200811092030_0002_root_sorter. blk#-8950178633263590328\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 20, \"cluster_size\": 2, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: <:*:> logs/history/ip-10-250-19-102.ec2.internal 1226291400491 job 200811092030 <:*:> root <:*:> <:BLOCKID:>\", \"cluster_count\": 37}\n",
      "Processing line: 490000, rate 43434.0 lines/sec, 37 clusters so far.\n",
      "Processing line: 500000, rate 43003.9 lines/sec, 37 clusters so far.\n",
      "Processing line: 510000, rate 42587.2 lines/sec, 37 clusters so far.\n",
      "Processing line: 520000, rate 44293.7 lines/sec, 37 clusters so far.\n",
      "Processing line: 530000, rate 44820.4 lines/sec, 37 clusters so far.\n",
      "Processing line: 540000, rate 44598.9 lines/sec, 37 clusters so far.\n",
      "Processing line: 550000, rate 45773.5 lines/sec, 37 clusters so far.\n",
      "Processing line: 560000, rate 43913.8 lines/sec, 37 clusters so far.\n",
      "Processing line: 570000, rate 45275.7 lines/sec, 37 clusters so far.\n",
      "Processing line: 580000, rate 44897.0 lines/sec, 37 clusters so far.\n",
      "Processing line: 590000, rate 45256.8 lines/sec, 37 clusters so far.\n",
      "Processing line: 600000, rate 42355.8 lines/sec, 37 clusters so far.\n",
      "Processing line: 610000, rate 45395.4 lines/sec, 37 clusters so far.\n",
      "Input (615247): BLOCK* NameSystem.allocateBlock: /user/root/sortrand/_temporary/_task_200811092030_0002_r_000174_0/part-00174. blk#6048716917685366339\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 12, \"cluster_size\": 36296, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: <:*:> temporary/ task 200811092030 <:*:> <:*:> <:*:> <:*:> <:BLOCKID:>\", \"cluster_count\": 37}\n",
      "Processing line: 620000, rate 43717.6 lines/sec, 37 clusters so far.\n",
      "Processing line: 630000, rate 37436.3 lines/sec, 37 clusters so far.\n",
      "Processing line: 640000, rate 38545.7 lines/sec, 37 clusters so far.\n",
      "Processing line: 650000, rate 40977.2 lines/sec, 37 clusters so far.\n",
      "Processing line: 660000, rate 43675.0 lines/sec, 37 clusters so far.\n",
      "Processing line: 670000, rate 43964.2 lines/sec, 37 clusters so far.\n",
      "Processing line: 680000, rate 44167.4 lines/sec, 37 clusters so far.\n",
      "Processing line: 690000, rate 44479.7 lines/sec, 37 clusters so far.\n",
      "Processing line: 700000, rate 42983.8 lines/sec, 37 clusters so far.\n",
      "Processing line: 710000, rate 43078.3 lines/sec, 37 clusters so far.\n",
      "Processing line: 720000, rate 44228.6 lines/sec, 37 clusters so far.\n",
      "Processing line: 730000, rate 43636.1 lines/sec, 37 clusters so far.\n",
      "Processing line: 740000, rate 44577.4 lines/sec, 37 clusters so far.\n",
      "Processing line: 750000, rate 43015.2 lines/sec, 37 clusters so far.\n",
      "Processing line: 760000, rate 44357.9 lines/sec, 37 clusters so far.\n",
      "Processing line: 770000, rate 44530.0 lines/sec, 37 clusters so far.\n",
      "Processing line: 780000, rate 44225.3 lines/sec, 37 clusters so far.\n",
      "Processing line: 790000, rate 40435.1 lines/sec, 37 clusters so far.\n",
      "Input (791057): PacketResponder blk#-4567777441263358151 1 Exception java.io.IOException: Connection reset by peer\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 38, \"cluster_size\": 1, \"template_mined\": \"PacketResponder <:BLOCKID:> 1 Exception java.io.IOException: Connection reset by peer\", \"cluster_count\": 38}\n",
      "Input (791061): 10.251.199.225:50010:Exception writing block blk#-4567777441263358151 to mirror 10.251.107.227:50010\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 28, \"cluster_size\": 2, \"template_mined\": \"<:*:> writing block <:BLOCKID:> to mirror <:*:>\", \"cluster_count\": 38}\n",
      "Input (791062): Exception in receiveBlock for block blk#-4567777441263358151 java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.251.199.225:50760 remote=/10.251.107.227:50010]. 489959 millis timeout left.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 29, \"cluster_size\": 2, \"template_mined\": \"Exception in receiveBlock for block <:BLOCKID:> java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected <:*:> <:*:> <:*:> millis timeout left.\", \"cluster_count\": 38}\n",
      "Input (791063): writeBlock blk#-4567777441263358151 received exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.251.199.225:50760 remote=/10.251.107.227:50010]. 489959 millis timeout left.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 30, \"cluster_size\": 2, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected <:*:> <:*:> <:*:> millis timeout left.\", \"cluster_count\": 38}\n",
      "Processing line: 800000, rate 39296.3 lines/sec, 38 clusters so far.\n",
      "Processing line: 810000, rate 44623.1 lines/sec, 38 clusters so far.\n",
      "Processing line: 820000, rate 44584.5 lines/sec, 38 clusters so far.\n",
      "Processing line: 830000, rate 43176.2 lines/sec, 38 clusters so far.\n",
      "Processing line: 840000, rate 44409.9 lines/sec, 38 clusters so far.\n",
      "Processing line: 850000, rate 44200.7 lines/sec, 38 clusters so far.\n",
      "Processing line: 860000, rate 44307.6 lines/sec, 38 clusters so far.\n",
      "Processing line: 870000, rate 43246.1 lines/sec, 38 clusters so far.\n",
      "Processing line: 880000, rate 43523.1 lines/sec, 38 clusters so far.\n",
      "Processing line: 890000, rate 44258.8 lines/sec, 38 clusters so far.\n",
      "Processing line: 900000, rate 43384.1 lines/sec, 38 clusters so far.\n",
      "Processing line: 910000, rate 44677.0 lines/sec, 38 clusters so far.\n",
      "Processing line: 920000, rate 44571.5 lines/sec, 38 clusters so far.\n",
      "Processing line: 930000, rate 44852.3 lines/sec, 38 clusters so far.\n",
      "Processing line: 940000, rate 43800.5 lines/sec, 38 clusters so far.\n",
      "Processing line: 950000, rate 43593.9 lines/sec, 38 clusters so far.\n",
      "Processing line: 960000, rate 44457.7 lines/sec, 38 clusters so far.\n",
      "Processing line: 970000, rate 44803.2 lines/sec, 38 clusters so far.\n",
      "Processing line: 980000, rate 44961.8 lines/sec, 38 clusters so far.\n",
      "Processing line: 990000, rate 43180.8 lines/sec, 38 clusters so far.\n",
      "Processing line: 1000000, rate 38185.7 lines/sec, 38 clusters so far.\n",
      "Processing line: 1010000, rate 44303.9 lines/sec, 38 clusters so far.\n",
      "Processing line: 1020000, rate 45041.0 lines/sec, 38 clusters so far.\n",
      "Processing line: 1030000, rate 42694.5 lines/sec, 38 clusters so far.\n",
      "Processing line: 1040000, rate 42614.5 lines/sec, 38 clusters so far.\n",
      "Processing line: 1050000, rate 44200.9 lines/sec, 38 clusters so far.\n",
      "Processing line: 1060000, rate 45290.5 lines/sec, 38 clusters so far.\n",
      "Processing line: 1070000, rate 44816.9 lines/sec, 38 clusters so far.\n",
      "Processing line: 1080000, rate 44084.3 lines/sec, 38 clusters so far.\n",
      "total          : took    22.66 s (100.00%),  1,086,792 samples,   20.85 ms / 1000 samples,       47,951.00 hz\n",
      "drain          : took    13.05 s ( 57.58%),  1,086,792 samples,   12.01 ms / 1000 samples,       83,273.82 hz\n",
      "tree_search    : took     5.56 s ( 24.53%),  1,086,792 samples,    5.12 ms / 1000 samples,      195,485.19 hz\n",
      "mask           : took     5.15 s ( 22.74%),  1,086,792 samples,    4.74 ms / 1000 samples,      210,823.36 hz\n",
      "cluster_exist  : took     3.37 s ( 14.87%),  1,086,754 samples,    3.10 ms / 1000 samples,      322,387.48 hz\n",
      "create_cluster : took     0.00 s (  0.00%),         38 samples,   20.03 ms / 1000 samples,       49,932.19 hz\n",
      "Processing line: 1090000, rate 43164.5 lines/sec, 38 clusters so far.\n",
      "Processing line: 1100000, rate 43244.9 lines/sec, 38 clusters so far.\n",
      "Processing line: 1110000, rate 42426.3 lines/sec, 38 clusters so far.\n",
      "Processing line: 1120000, rate 44337.8 lines/sec, 38 clusters so far.\n",
      "Processing line: 1130000, rate 43085.6 lines/sec, 38 clusters so far.\n",
      "Processing line: 1140000, rate 40015.2 lines/sec, 38 clusters so far.\n",
      "Processing line: 1150000, rate 39338.8 lines/sec, 38 clusters so far.\n",
      "Processing line: 1160000, rate 42412.3 lines/sec, 38 clusters so far.\n",
      "Processing line: 1170000, rate 43838.9 lines/sec, 38 clusters so far.\n",
      "Processing line: 1180000, rate 43651.7 lines/sec, 38 clusters so far.\n",
      "Processing line: 1190000, rate 43167.7 lines/sec, 38 clusters so far.\n",
      "Processing line: 1200000, rate 42659.0 lines/sec, 38 clusters so far.\n",
      "Processing line: 1210000, rate 42324.8 lines/sec, 38 clusters so far.\n",
      "Processing line: 1220000, rate 44062.0 lines/sec, 38 clusters so far.\n",
      "Processing line: 1230000, rate 44571.7 lines/sec, 38 clusters so far.\n",
      "Processing line: 1240000, rate 43756.9 lines/sec, 38 clusters so far.\n",
      "Processing line: 1250000, rate 43715.9 lines/sec, 38 clusters so far.\n",
      "Processing line: 1260000, rate 42673.6 lines/sec, 38 clusters so far.\n",
      "Processing line: 1270000, rate 43571.3 lines/sec, 38 clusters so far.\n",
      "Processing line: 1280000, rate 43762.7 lines/sec, 38 clusters so far.\n",
      "Processing line: 1290000, rate 41658.0 lines/sec, 38 clusters so far.\n",
      "Processing line: 1300000, rate 43597.7 lines/sec, 38 clusters so far.\n",
      "Processing line: 1310000, rate 43368.1 lines/sec, 38 clusters so far.\n",
      "Processing line: 1320000, rate 44097.9 lines/sec, 38 clusters so far.\n",
      "Processing line: 1330000, rate 43869.7 lines/sec, 38 clusters so far.\n",
      "Processing line: 1340000, rate 43073.0 lines/sec, 38 clusters so far.\n",
      "Processing line: 1350000, rate 43041.5 lines/sec, 38 clusters so far.\n",
      "Processing line: 1360000, rate 44054.4 lines/sec, 38 clusters so far.\n",
      "Processing line: 1370000, rate 43661.5 lines/sec, 38 clusters so far.\n",
      "Input (1372949): Exception in receiveBlock for block blk#7008279672769077211 java.net.SocketTimeoutException: 490000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.251.42.191:43873 remote=/10.251.123.33:50010]\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 39, \"cluster_size\": 1, \"template_mined\": \"Exception in receiveBlock for block <:BLOCKID:> java.net.SocketTimeoutException: 490000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.251.42.191:43873 remote=/10.251.123.33:50010]\", \"cluster_count\": 39}\n",
      "Input (1373103): writeBlock blk#7008279672769077211 received exception java.net.SocketTimeoutException: 490000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.251.42.191:43873 remote=/10.251.123.33:50010]\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 40, \"cluster_size\": 1, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.net.SocketTimeoutException: 490000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.251.42.191:43873 remote=/10.251.123.33:50010]\", \"cluster_count\": 40}\n",
      "Input (1373106): PacketResponder blk#7008279672769077211 2 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.251.42.191:43873 remote=/10.251.123.33:50010]. 115663 millis timeout left.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 25, \"cluster_size\": 3, \"template_mined\": \"PacketResponder <:BLOCKID:> <:*:> Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected <:*:> <:*:> <:*:> millis timeout left.\", \"cluster_count\": 40}\n",
      "Processing line: 1380000, rate 40440.6 lines/sec, 40 clusters so far.\n",
      "Input (1389320): Exception in receiveBlock for block blk#7008279672769077211 java.nio.channels.ClosedByInterruptException\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 26, \"cluster_size\": 4, \"template_mined\": \"Exception in receiveBlock for block <:BLOCKID:> <:*:>\", \"cluster_count\": 40}\n",
      "Input (1389402): Reopen Block blk#7008279672769077211\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 41, \"cluster_size\": 1, \"template_mined\": \"Reopen Block <:BLOCKID:>\", \"cluster_count\": 41}\n",
      "Processing line: 1390000, rate 41725.0 lines/sec, 41 clusters so far.\n",
      "Processing line: 1400000, rate 42666.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 1410000, rate 44721.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 1420000, rate 44040.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 1430000, rate 44533.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 1440000, rate 43600.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 1450000, rate 41251.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 1460000, rate 43754.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 1470000, rate 39546.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 1480000, rate 43662.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 1490000, rate 42226.7 lines/sec, 41 clusters so far.\n",
      "Input (1496648): PacketResponder blk#8085135783040518166 1 Exception java.nio.channels.ClosedByInterruptException\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 17, \"cluster_size\": 3, \"template_mined\": \"PacketResponder <:BLOCKID:> <:*:> Exception <:*:>\", \"cluster_count\": 41}\n",
      "Processing line: 1500000, rate 42219.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 1510000, rate 43676.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 1520000, rate 43355.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 1530000, rate 43172.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 1540000, rate 43684.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 1550000, rate 46989.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 1560000, rate 48101.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 1570000, rate 47894.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 1580000, rate 47070.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 1590000, rate 47253.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 1600000, rate 47651.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 1610000, rate 47454.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 1620000, rate 47976.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 1630000, rate 45692.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 1640000, rate 46614.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 1650000, rate 45226.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 1660000, rate 47140.0 lines/sec, 41 clusters so far.\n",
      "Processing line: 1670000, rate 46213.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 1680000, rate 45754.0 lines/sec, 41 clusters so far.\n",
      "Processing line: 1690000, rate 45920.0 lines/sec, 41 clusters so far.\n",
      "Processing line: 1700000, rate 47544.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 1710000, rate 47747.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 1720000, rate 46796.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 1730000, rate 46257.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 1740000, rate 46395.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 1750000, rate 45421.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 1760000, rate 46705.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 1770000, rate 46895.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 1780000, rate 49369.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 1790000, rate 47333.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 1800000, rate 47429.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 1810000, rate 45898.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 1820000, rate 47121.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 1830000, rate 46687.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 1840000, rate 44583.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 1850000, rate 46487.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 1860000, rate 45118.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 1870000, rate 46361.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 1880000, rate 46280.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 1890000, rate 47861.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 1900000, rate 48075.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 1910000, rate 46758.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 1920000, rate 47136.0 lines/sec, 41 clusters so far.\n",
      "Processing line: 1930000, rate 44133.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 1940000, rate 45906.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 1950000, rate 41431.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 1960000, rate 42963.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 1970000, rate 41943.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 1980000, rate 42060.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 1990000, rate 43086.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 2000000, rate 41748.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 2010000, rate 43100.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 2020000, rate 42865.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 2030000, rate 43307.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 2040000, rate 42706.0 lines/sec, 41 clusters so far.\n",
      "Processing line: 2050000, rate 42764.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 2060000, rate 42868.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 2070000, rate 42057.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 2080000, rate 43267.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 2090000, rate 42950.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 2100000, rate 42407.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 2110000, rate 39563.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2120000, rate 42526.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 2130000, rate 42938.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2140000, rate 42619.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 2150000, rate 42319.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 2160000, rate 42981.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2170000, rate 43178.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 2180000, rate 42778.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 2190000, rate 43250.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 2200000, rate 41680.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 2210000, rate 42863.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 2220000, rate 42218.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 2230000, rate 43658.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2240000, rate 42725.7 lines/sec, 41 clusters so far.\n",
      "Input (2247681): BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811101024_0001/job.jar. blk#-3907603481176056256\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 2, \"cluster_size\": 28, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job <:*:> <:*:> <:BLOCKID:>\", \"cluster_count\": 41}\n",
      "Input (2247854): BLOCK* NameSystem.allocateBlock: /user/root/rand/_logs/history/ip-10-250-19-102.ec2.internal_1226341485991_job_200811101024_0001_conf.xml. blk#8305330044595670869\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 11, \"cluster_size\": 10, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: <:*:> logs/history/ip-10-250-19-102.ec2.internal <:*:> job <:*:> <:*:> conf.xml. <:BLOCKID:>\", \"cluster_count\": 41}\n",
      "Input (2248504): BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811101024_0001_m_000000_0/part-00000. blk#6199125144269351162\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 12, \"cluster_size\": 108003, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: <:*:> temporary/ task <:*:> <:*:> <:*:> <:*:> <:*:> <:BLOCKID:>\", \"cluster_count\": 41}\n",
      "Processing line: 2250000, rate 42275.6 lines/sec, 41 clusters so far.\n",
      "Processing line: 2260000, rate 45013.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2270000, rate 47459.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 2280000, rate 47168.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2290000, rate 48232.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 2300000, rate 48982.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 2310000, rate 41377.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 2320000, rate 48321.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 2330000, rate 48192.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 2340000, rate 48185.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 2350000, rate 47931.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 2360000, rate 48269.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 2370000, rate 45187.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 2380000, rate 47733.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 2390000, rate 46278.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 2400000, rate 47120.0 lines/sec, 41 clusters so far.\n",
      "Processing line: 2410000, rate 48666.8 lines/sec, 41 clusters so far.\n",
      "total          : took    49.07 s (100.00%),  2,416,151 samples,   20.31 ms / 1000 samples,       49,242.51 hz\n",
      "drain          : took    28.14 s ( 57.34%),  2,416,151 samples,   11.64 ms / 1000 samples,       85,875.59 hz\n",
      "tree_search    : took    11.89 s ( 24.23%),  2,416,151 samples,    4.92 ms / 1000 samples,      203,238.72 hz\n",
      "mask           : took    11.15 s ( 22.73%),  2,416,151 samples,    4.62 ms / 1000 samples,      216,620.63 hz\n",
      "cluster_exist  : took     7.26 s ( 14.80%),  2,416,110 samples,    3.01 ms / 1000 samples,      332,718.88 hz\n",
      "create_cluster : took     0.00 s (  0.00%),         41 samples,   20.54 ms / 1000 samples,       48,688.13 hz\n",
      "Processing line: 2420000, rate 48021.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 2430000, rate 46205.0 lines/sec, 41 clusters so far.\n",
      "Processing line: 2440000, rate 48026.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 2450000, rate 47847.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 2460000, rate 48191.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 2470000, rate 48815.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2480000, rate 47707.9 lines/sec, 41 clusters so far.\n",
      "Processing line: 2490000, rate 48146.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 2500000, rate 47816.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 2510000, rate 48191.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2520000, rate 46839.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 2530000, rate 48089.2 lines/sec, 41 clusters so far.\n",
      "Processing line: 2540000, rate 48329.5 lines/sec, 41 clusters so far.\n",
      "Processing line: 2550000, rate 47343.8 lines/sec, 41 clusters so far.\n",
      "Processing line: 2560000, rate 45373.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 2570000, rate 46151.1 lines/sec, 41 clusters so far.\n",
      "Processing line: 2580000, rate 43915.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2590000, rate 43042.7 lines/sec, 41 clusters so far.\n",
      "Processing line: 2600000, rate 44005.4 lines/sec, 41 clusters so far.\n",
      "Processing line: 2610000, rate 43688.3 lines/sec, 41 clusters so far.\n",
      "Processing line: 2620000, rate 43887.0 lines/sec, 41 clusters so far.\n",
      "Input (2620400): BLOCK* Removing block blk#-3530301067936445915 from neededReplications as it does not belong to any file.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 42, \"cluster_size\": 1, \"template_mined\": \"BLOCK* Removing block <:BLOCKID:> from neededReplications as it does not belong to any file.\", \"cluster_count\": 42}\n",
      "Input (2621297): BLOCK* NameSystem.allocateBlock: /user/root/rand/_logs/history/ip-10-250-19-102.ec2.internal_1226341485991_job_200811101024_0001_root_random-writer. blk#-6652811131921977095\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 20, \"cluster_size\": 10, \"template_mined\": \"BLOCK* NameSystem.allocateBlock: <:*:> logs/history/ip-10-250-19-102.ec2.internal <:*:> job <:*:> <:*:> root <:*:> <:BLOCKID:>\", \"cluster_count\": 42}\n",
      "Processing line: 2630000, rate 41940.0 lines/sec, 42 clusters so far.\n",
      "Input (2635992): BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for blk#-4109536386405623117 on 10.251.106.10:50010 size 3528482 But it does not belong to any file.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 36, \"cluster_size\": 24, \"template_mined\": \"BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for <:BLOCKID:> on <:*:> size <:*:> But it does not belong to any file.\", \"cluster_count\": 42}\n",
      "Processing line: 2640000, rate 40473.5 lines/sec, 42 clusters so far.\n",
      "Processing line: 2650000, rate 43780.0 lines/sec, 42 clusters so far.\n",
      "Input (2654215): PacketResponder blk#8516616325149469651 2 Exception java.io.IOException: Broken pipe\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 31, \"cluster_size\": 4, \"template_mined\": \"PacketResponder <:BLOCKID:> <:*:> Exception java.io.IOException: Broken pipe\", \"cluster_count\": 42}\n",
      "Processing line: 2660000, rate 43414.0 lines/sec, 42 clusters so far.\n",
      "Input (2663876): PendingReplicationMonitor timed out block blk#-5057834626410636236\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 43, \"cluster_size\": 1, \"template_mined\": \"PendingReplicationMonitor timed out block <:BLOCKID:>\", \"cluster_count\": 43}\n",
      "Processing line: 2670000, rate 42207.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 2680000, rate 44423.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 2690000, rate 44359.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 2700000, rate 43322.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 2710000, rate 43557.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 2720000, rate 44011.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 2730000, rate 44097.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 2740000, rate 42112.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 2750000, rate 43737.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 2760000, rate 40507.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 2770000, rate 39959.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 2780000, rate 38098.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 2790000, rate 37205.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 2800000, rate 40693.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 2810000, rate 41913.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 2820000, rate 36181.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 2830000, rate 38679.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 2840000, rate 42185.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 2850000, rate 41138.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 2860000, rate 41299.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 2870000, rate 42998.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 2880000, rate 36291.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 2890000, rate 42484.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 2900000, rate 42655.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 2910000, rate 42388.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 2920000, rate 42518.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 2930000, rate 41490.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 2940000, rate 41411.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 2950000, rate 42078.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 2960000, rate 42682.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 2970000, rate 42191.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 2980000, rate 42473.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 2990000, rate 42773.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 3000000, rate 42622.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 3010000, rate 42092.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 3020000, rate 42495.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3030000, rate 41869.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3040000, rate 43375.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3050000, rate 42362.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3060000, rate 42544.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3070000, rate 41412.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3080000, rate 42205.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3090000, rate 42280.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3100000, rate 42169.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3110000, rate 42458.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3120000, rate 41540.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3130000, rate 42963.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 3140000, rate 41529.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3150000, rate 42446.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3160000, rate 42487.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3170000, rate 41938.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 3180000, rate 41936.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3190000, rate 42218.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3200000, rate 43154.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3210000, rate 42263.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 3220000, rate 42045.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 3230000, rate 43237.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3240000, rate 38066.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3250000, rate 41580.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3260000, rate 41738.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 3270000, rate 42497.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3280000, rate 42361.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3290000, rate 42418.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3300000, rate 42070.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 3310000, rate 42063.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3320000, rate 42175.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 3330000, rate 43175.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 3340000, rate 42330.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3350000, rate 41673.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3360000, rate 42783.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 3370000, rate 42503.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 3380000, rate 42514.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 3390000, rate 40800.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3400000, rate 42857.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 3410000, rate 38288.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3420000, rate 42149.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 3430000, rate 42216.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3440000, rate 42326.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 3450000, rate 42441.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3460000, rate 42597.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 3470000, rate 42236.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3480000, rate 41409.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 3490000, rate 41937.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3500000, rate 42781.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3510000, rate 42319.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3520000, rate 45543.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 3530000, rate 43184.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 3540000, rate 43531.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3550000, rate 43314.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 3560000, rate 38139.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 3570000, rate 42391.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 3580000, rate 43655.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 3590000, rate 43447.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3600000, rate 43675.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 3610000, rate 43704.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3620000, rate 43156.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3630000, rate 43510.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3640000, rate 43459.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3650000, rate 44283.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3660000, rate 40830.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3670000, rate 39943.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 3680000, rate 42567.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 3690000, rate 42879.8 lines/sec, 43 clusters so far.\n",
      "total          : took    75.48 s (100.00%),  3,693,805 samples,   20.43 ms / 1000 samples,       48,940.54 hz\n",
      "drain          : took    43.28 s ( 57.35%),  3,693,805 samples,   11.72 ms / 1000 samples,       85,338.15 hz\n",
      "tree_search    : took    18.28 s ( 24.22%),  3,693,805 samples,    4.95 ms / 1000 samples,      202,085.47 hz\n",
      "mask           : took    17.17 s ( 22.74%),  3,693,805 samples,    4.65 ms / 1000 samples,      215,182.33 hz\n",
      "cluster_exist  : took    11.20 s ( 14.84%),  3,693,762 samples,    3.03 ms / 1000 samples,      329,877.06 hz\n",
      "create_cluster : took     0.00 s (  0.00%),         43 samples,   20.46 ms / 1000 samples,       48,876.71 hz\n",
      "Processing line: 3700000, rate 41493.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 3710000, rate 41028.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3720000, rate 42685.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3730000, rate 42115.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3740000, rate 42729.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3750000, rate 42398.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3760000, rate 42942.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3770000, rate 42689.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3780000, rate 42808.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3790000, rate 42351.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3800000, rate 42396.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 3810000, rate 42581.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3820000, rate 42194.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3830000, rate 42106.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3840000, rate 42636.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3850000, rate 42147.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3860000, rate 41717.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 3870000, rate 42441.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3880000, rate 42485.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 3890000, rate 42747.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 3900000, rate 42677.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3910000, rate 42033.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 3920000, rate 42612.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 3930000, rate 42859.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3940000, rate 42201.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3950000, rate 42418.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 3960000, rate 41937.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 3970000, rate 42089.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 3980000, rate 42504.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 3990000, rate 42892.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 4000000, rate 41098.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 4010000, rate 42760.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4020000, rate 41796.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4030000, rate 42640.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 4040000, rate 41262.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 4050000, rate 42491.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 4060000, rate 42589.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 4070000, rate 42991.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 4080000, rate 43362.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 4090000, rate 41275.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 4100000, rate 46708.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 4110000, rate 45217.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 4120000, rate 46023.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4130000, rate 45846.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 4140000, rate 46451.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 4150000, rate 45684.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 4160000, rate 44950.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 4170000, rate 44807.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 4180000, rate 45407.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 4190000, rate 45925.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 4200000, rate 45388.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 4210000, rate 44092.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 4220000, rate 42188.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 4230000, rate 41390.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 4240000, rate 41578.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 4250000, rate 41313.4 lines/sec, 43 clusters so far.\n",
      "Processing line: 4260000, rate 41231.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 4270000, rate 41421.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 4280000, rate 41583.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 4290000, rate 41280.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 4300000, rate 41173.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4310000, rate 35018.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 4320000, rate 41372.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4330000, rate 41099.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 4340000, rate 40721.6 lines/sec, 43 clusters so far.\n",
      "Processing line: 4350000, rate 41807.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4360000, rate 39479.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 4370000, rate 41909.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 4380000, rate 41119.2 lines/sec, 43 clusters so far.\n",
      "Processing line: 4390000, rate 41660.9 lines/sec, 43 clusters so far.\n",
      "Processing line: 4400000, rate 41289.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 4410000, rate 41467.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 4420000, rate 42457.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4430000, rate 45582.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 4440000, rate 45632.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4450000, rate 46079.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 4460000, rate 45814.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 4470000, rate 47407.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 4480000, rate 46314.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 4490000, rate 46781.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 4500000, rate 47173.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 4510000, rate 46311.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 4520000, rate 45783.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4530000, rate 43781.7 lines/sec, 43 clusters so far.\n",
      "Processing line: 4540000, rate 46309.0 lines/sec, 43 clusters so far.\n",
      "Processing line: 4550000, rate 46673.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 4560000, rate 45450.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4570000, rate 46068.3 lines/sec, 43 clusters so far.\n",
      "Processing line: 4580000, rate 46078.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 4590000, rate 45902.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 4600000, rate 46437.5 lines/sec, 43 clusters so far.\n",
      "Processing line: 4610000, rate 45999.1 lines/sec, 43 clusters so far.\n",
      "Processing line: 4620000, rate 45614.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4630000, rate 44438.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4640000, rate 43524.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4650000, rate 42394.8 lines/sec, 43 clusters so far.\n",
      "Processing line: 4660000, rate 43164.1 lines/sec, 43 clusters so far.\n",
      "Input (4663237): writeBlock blk#-516029862489211143 received exception java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.251.197.161:44245 remote=/10.251.123.33:50010]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 40, \"cluster_size\": 2, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.net.SocketTimeoutException: <:*:> millis timeout while waiting for channel to be ready for <:*:> ch : java.nio.channels.SocketChannel[connected <:*:> remote=/10.251.123.33:50010]\", \"cluster_count\": 43}\n",
      "Processing line: 4670000, rate 38335.9 lines/sec, 43 clusters so far.\n",
      "Input (4673664): writeBlock blk#-4250706752040073149 received exception java.io.IOException: Interrupted receiveBlock\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 44, \"cluster_size\": 1, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.io.IOException: Interrupted receiveBlock\", \"cluster_count\": 44}\n",
      "Input (4673666): PacketResponder blk#-4250706752040073149 1 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 59985 millis timeout left.\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 45, \"cluster_size\": 1, \"template_mined\": \"PacketResponder <:BLOCKID:> 1 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 59985 millis timeout left.\", \"cluster_count\": 45}\n",
      "Processing line: 4680000, rate 37321.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 4690000, rate 38344.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 4700000, rate 41674.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 4710000, rate 41469.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 4720000, rate 41997.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 4730000, rate 42228.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 4740000, rate 42064.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 4750000, rate 42268.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 4760000, rate 42443.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 4770000, rate 41659.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 4780000, rate 42476.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 4790000, rate 41969.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 4800000, rate 42554.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 4810000, rate 41972.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 4820000, rate 42843.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 4830000, rate 41251.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 4840000, rate 42596.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 4850000, rate 41803.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 4860000, rate 42723.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 4870000, rate 42912.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 4880000, rate 42189.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 4890000, rate 43087.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 4900000, rate 41977.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 4910000, rate 41865.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 4920000, rate 42384.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 4930000, rate 42253.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 4940000, rate 42232.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 4950000, rate 39046.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 4960000, rate 41611.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 4970000, rate 41884.9 lines/sec, 45 clusters so far.\n",
      "total          : took   101.89 s (100.00%),  4,974,322 samples,   20.48 ms / 1000 samples,       48,822.18 hz\n",
      "drain          : took    58.42 s ( 57.34%),  4,974,322 samples,   11.74 ms / 1000 samples,       85,148.04 hz\n",
      "tree_search    : took    24.66 s ( 24.20%),  4,974,322 samples,    4.96 ms / 1000 samples,      201,712.71 hz\n",
      "mask           : took    23.17 s ( 22.74%),  4,974,322 samples,    4.66 ms / 1000 samples,      214,713.35 hz\n",
      "cluster_exist  : took    15.11 s ( 14.83%),  4,974,277 samples,    3.04 ms / 1000 samples,      329,131.70 hz\n",
      "create_cluster : took     0.00 s (  0.00%),         45 samples,   20.36 ms / 1000 samples,       49,126.41 hz\n",
      "Processing line: 4980000, rate 42763.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 4990000, rate 41632.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 5000000, rate 42761.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 5010000, rate 41638.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 5020000, rate 42243.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 5030000, rate 42877.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 5040000, rate 36569.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 5050000, rate 42205.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5060000, rate 42861.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5070000, rate 43220.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 5080000, rate 43031.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 5090000, rate 42568.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 5100000, rate 42344.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5110000, rate 40988.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 5120000, rate 41565.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5130000, rate 41662.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 5140000, rate 41455.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5150000, rate 40370.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 5160000, rate 40490.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 5170000, rate 41251.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5180000, rate 41010.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5190000, rate 40812.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5200000, rate 41590.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 5210000, rate 44650.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 5220000, rate 46081.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 5230000, rate 47005.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 5240000, rate 46439.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 5250000, rate 46380.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 5260000, rate 45750.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 5270000, rate 45782.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 5280000, rate 45773.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 5290000, rate 46283.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 5300000, rate 45048.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5310000, rate 44850.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5320000, rate 44620.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5330000, rate 44131.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5340000, rate 42624.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 5350000, rate 42229.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 5360000, rate 42630.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5370000, rate 42714.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 5380000, rate 41254.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 5390000, rate 40640.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5400000, rate 41940.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 5410000, rate 41298.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 5420000, rate 42599.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5430000, rate 42571.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 5440000, rate 42707.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5450000, rate 42667.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5460000, rate 41498.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 5470000, rate 42404.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 5480000, rate 41164.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 5490000, rate 43063.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5500000, rate 42517.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5510000, rate 42812.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 5520000, rate 41530.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5530000, rate 42655.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 5540000, rate 43169.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 5550000, rate 40694.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5560000, rate 42205.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5570000, rate 42010.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 5580000, rate 42360.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 5590000, rate 42240.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 5600000, rate 42299.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5610000, rate 42626.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5620000, rate 41950.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 5630000, rate 42720.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5640000, rate 42164.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5650000, rate 42124.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 5660000, rate 40854.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5670000, rate 42334.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5680000, rate 42179.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 5690000, rate 42067.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 5700000, rate 42190.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 5710000, rate 42679.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 5720000, rate 41451.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5730000, rate 42837.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5740000, rate 42649.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5750000, rate 42171.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 5760000, rate 42957.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 5770000, rate 42833.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 5780000, rate 42247.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5790000, rate 41738.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5800000, rate 45424.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5810000, rate 41315.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 5820000, rate 45571.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 5830000, rate 44289.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5840000, rate 45293.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 5850000, rate 45371.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 5860000, rate 39134.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 5870000, rate 45534.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5880000, rate 41790.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5890000, rate 41610.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 5900000, rate 41084.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5910000, rate 41660.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 5920000, rate 41424.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5930000, rate 41418.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5940000, rate 41213.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5950000, rate 41186.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 5960000, rate 37178.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 5970000, rate 41828.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 5980000, rate 41910.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 5990000, rate 45899.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6000000, rate 45787.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6010000, rate 42798.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6020000, rate 47370.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 6030000, rate 45660.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6040000, rate 45829.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6050000, rate 47615.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 6060000, rate 46276.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6070000, rate 44625.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 6080000, rate 45470.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6090000, rate 43954.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6100000, rate 42897.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6110000, rate 42609.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 6120000, rate 41601.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6130000, rate 42347.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6140000, rate 41291.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6150000, rate 41981.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 6160000, rate 42783.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6170000, rate 42581.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6180000, rate 42439.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 6190000, rate 42014.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6200000, rate 43117.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6210000, rate 42895.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6220000, rate 42522.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6230000, rate 42452.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 6240000, rate 39941.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6250000, rate 43271.2 lines/sec, 45 clusters so far.\n",
      "total          : took   128.31 s (100.00%),  6,251,908 samples,   20.52 ms / 1000 samples,       48,724.47 hz\n",
      "drain          : took    73.58 s ( 57.34%),  6,251,908 samples,   11.77 ms / 1000 samples,       84,967.73 hz\n",
      "tree_search    : took    31.08 s ( 24.23%),  6,251,908 samples,    4.97 ms / 1000 samples,      201,128.74 hz\n",
      "mask           : took    29.20 s ( 22.75%),  6,251,908 samples,    4.67 ms / 1000 samples,      214,137.08 hz\n",
      "cluster_exist  : took    19.03 s ( 14.83%),  6,251,863 samples,    3.04 ms / 1000 samples,      328,473.66 hz\n",
      "create_cluster : took     0.00 s (  0.00%),         45 samples,   20.36 ms / 1000 samples,       49,126.41 hz\n",
      "Processing line: 6260000, rate 41984.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 6270000, rate 40993.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6280000, rate 42535.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6290000, rate 42168.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 6300000, rate 40199.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 6310000, rate 42553.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6320000, rate 42790.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 6330000, rate 42042.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 6340000, rate 43038.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 6350000, rate 42704.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6360000, rate 42519.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6370000, rate 42683.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 6380000, rate 41792.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 6390000, rate 42928.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 6400000, rate 42436.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6410000, rate 42534.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 6420000, rate 42341.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 6430000, rate 42406.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6440000, rate 42283.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6450000, rate 42486.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6460000, rate 42414.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 6470000, rate 41827.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 6480000, rate 42640.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6490000, rate 42632.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 6500000, rate 42644.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6510000, rate 42738.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6520000, rate 42753.7 lines/sec, 45 clusters so far.\n",
      "Input (6520468): PacketResponder blk#-4380884313697280861 1 Exception java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.251.107.50:40367 remote=/10.251.30.85:50010]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 24, \"cluster_size\": 7, \"template_mined\": \"PacketResponder <:BLOCKID:> <:*:> Exception java.net.SocketTimeoutException: <:*:> millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected <:*:> <:*:>\", \"cluster_count\": 45}\n",
      "Processing line: 6530000, rate 37588.5 lines/sec, 45 clusters so far.\n",
      "Input (6530498): writeBlock blk#1684134505299265593 received exception java.net.NoRouteToHostException: No route to host\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 22, \"cluster_size\": 26, \"template_mined\": \"writeBlock <:BLOCKID:> received exception <:*:> <:*:> <:*:> <:*:> <:*:>\", \"cluster_count\": 45}\n",
      "Processing line: 6540000, rate 40323.4 lines/sec, 45 clusters so far.\n",
      "Input (6549008): writeBlock blk#6189089042276997489 received exception java.io.IOException: Broken pipe\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 44, \"cluster_size\": 2, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.io.IOException: <:*:> <:*:>\", \"cluster_count\": 45}\n",
      "Processing line: 6550000, rate 41954.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 6560000, rate 39322.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6570000, rate 42700.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6580000, rate 41966.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 6590000, rate 41626.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6600000, rate 42757.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6610000, rate 42241.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6620000, rate 41271.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 6630000, rate 39819.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6640000, rate 42178.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 6650000, rate 42946.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6660000, rate 40254.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 6670000, rate 43317.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6680000, rate 41993.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6690000, rate 42035.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6700000, rate 42680.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6710000, rate 42163.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6720000, rate 42533.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 6730000, rate 43075.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 6740000, rate 40854.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6750000, rate 42548.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6760000, rate 42265.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6770000, rate 42584.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 6780000, rate 42233.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6790000, rate 42078.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6800000, rate 42348.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 6810000, rate 42613.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 6820000, rate 42609.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 6830000, rate 42666.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 6840000, rate 42743.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 6850000, rate 42152.1 lines/sec, 45 clusters so far.\n",
      "Processing line: 6860000, rate 42263.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 6870000, rate 42002.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6880000, rate 41754.6 lines/sec, 45 clusters so far.\n",
      "Processing line: 6890000, rate 41872.9 lines/sec, 45 clusters so far.\n",
      "Processing line: 6900000, rate 42288.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6910000, rate 40149.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6920000, rate 42363.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6930000, rate 42606.5 lines/sec, 45 clusters so far.\n",
      "Processing line: 6940000, rate 42536.2 lines/sec, 45 clusters so far.\n",
      "Processing line: 6950000, rate 41346.4 lines/sec, 45 clusters so far.\n",
      "Processing line: 6960000, rate 42251.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 6970000, rate 41632.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 6980000, rate 43202.0 lines/sec, 45 clusters so far.\n",
      "Processing line: 6990000, rate 42638.3 lines/sec, 45 clusters so far.\n",
      "Processing line: 7000000, rate 41755.8 lines/sec, 45 clusters so far.\n",
      "Processing line: 7010000, rate 42182.7 lines/sec, 45 clusters so far.\n",
      "Processing line: 7020000, rate 41497.7 lines/sec, 45 clusters so far.\n",
      "Input (7023749): Adding an already existing block blk#-2074647664485597823\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 46, \"cluster_size\": 1, \"template_mined\": \"Adding an already existing block <:BLOCKID:>\", \"cluster_count\": 46}\n",
      "Processing line: 7030000, rate 39161.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7040000, rate 42683.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 7050000, rate 41654.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7060000, rate 40469.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7070000, rate 41508.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 7080000, rate 40234.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7090000, rate 39005.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7100000, rate 41303.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 7110000, rate 41467.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 7120000, rate 40977.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 7130000, rate 41679.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 7140000, rate 41474.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7150000, rate 41577.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 7160000, rate 41730.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 7170000, rate 41782.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7180000, rate 41656.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 7190000, rate 42152.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 7200000, rate 41627.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 7210000, rate 41020.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 7220000, rate 37469.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7230000, rate 41382.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 7240000, rate 39385.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7250000, rate 41459.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 7260000, rate 44128.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 7270000, rate 45320.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7280000, rate 45689.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7290000, rate 46688.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 7300000, rate 45974.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7310000, rate 46141.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7320000, rate 45133.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 7330000, rate 46054.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7340000, rate 44907.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 7350000, rate 45810.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 7360000, rate 45957.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7370000, rate 46005.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 7380000, rate 46426.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 7390000, rate 46250.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 7400000, rate 45364.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7410000, rate 44654.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7420000, rate 41022.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 7430000, rate 45909.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7440000, rate 44906.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 7450000, rate 45560.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 7460000, rate 45445.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 7470000, rate 44540.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 7480000, rate 42832.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7490000, rate 42180.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7500000, rate 42163.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 7510000, rate 42516.2 lines/sec, 46 clusters so far.\n",
      "total          : took   154.74 s (100.00%),  7,519,642 samples,   20.58 ms / 1000 samples,       48,596.63 hz\n",
      "drain          : took    88.74 s ( 57.35%),  7,519,642 samples,   11.80 ms / 1000 samples,       84,738.52 hz\n",
      "tree_search    : took    37.53 s ( 24.25%),  7,519,642 samples,    4.99 ms / 1000 samples,      200,382.64 hz\n",
      "mask           : took    35.24 s ( 22.77%),  7,519,642 samples,    4.69 ms / 1000 samples,      213,407.81 hz\n",
      "cluster_exist  : took    22.95 s ( 14.83%),  7,519,596 samples,    3.05 ms / 1000 samples,      327,666.70 hz\n",
      "create_cluster : took     0.00 s (  0.00%),         46 samples,   20.33 ms / 1000 samples,       49,193.77 hz\n",
      "Processing line: 7520000, rate 39208.8 lines/sec, 46 clusters so far.\n",
      "Input (7527735): PacketResponder blk#2928690555251770634 2 Exception java.io.IOException: Connection reset by peer\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 38, \"cluster_size\": 3, \"template_mined\": \"PacketResponder <:BLOCKID:> <:*:> Exception java.io.IOException: Connection reset by peer\", \"cluster_count\": 46}\n",
      "Processing line: 7530000, rate 40819.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 7540000, rate 38902.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 7550000, rate 40542.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 7560000, rate 42588.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 7570000, rate 41057.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7580000, rate 42122.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 7590000, rate 42300.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7600000, rate 42903.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 7610000, rate 42695.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 7620000, rate 42316.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 7630000, rate 42752.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 7640000, rate 42527.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 7650000, rate 42078.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 7660000, rate 41723.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7670000, rate 42792.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 7680000, rate 42168.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 7690000, rate 42166.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 7700000, rate 41323.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7710000, rate 40814.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7720000, rate 42152.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7730000, rate 41639.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7740000, rate 42300.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 7750000, rate 42765.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 7760000, rate 41788.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 7770000, rate 41904.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7780000, rate 41650.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 7790000, rate 42608.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 7800000, rate 42708.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 7810000, rate 42155.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 7820000, rate 42275.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 7830000, rate 43149.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7840000, rate 41350.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 7850000, rate 42719.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 7860000, rate 42312.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 7870000, rate 42564.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 7880000, rate 40364.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7890000, rate 42526.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7900000, rate 42605.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 7910000, rate 42905.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 7920000, rate 42916.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 7930000, rate 42629.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7940000, rate 38800.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7950000, rate 44657.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 7960000, rate 42640.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 7970000, rate 42436.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 7980000, rate 42385.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 7990000, rate 41811.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 8000000, rate 42679.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 8010000, rate 42700.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 8020000, rate 42662.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 8030000, rate 42159.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 8040000, rate 42721.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 8050000, rate 39955.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 8060000, rate 42028.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 8070000, rate 43098.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 8080000, rate 42929.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 8090000, rate 41352.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 8100000, rate 41784.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 8110000, rate 42210.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 8120000, rate 42362.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 8130000, rate 42567.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 8140000, rate 41981.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 8150000, rate 41441.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 8160000, rate 41381.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 8170000, rate 42201.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 8180000, rate 42675.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 8190000, rate 42460.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 8200000, rate 41732.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 8210000, rate 36653.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 8220000, rate 40997.7 lines/sec, 46 clusters so far.\n",
      "Input (8226043): writeBlock blk#-2308563252795628677 received exception java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.251.90.134:46191 remote=/10.250.13.188:50010]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 40, \"cluster_size\": 9, \"template_mined\": \"writeBlock <:BLOCKID:> received exception java.net.SocketTimeoutException: <:*:> millis timeout while waiting for channel to be ready for <:*:> ch : java.nio.channels.SocketChannel[connected <:*:> <:*:>\", \"cluster_count\": 46}\n",
      "Processing line: 8230000, rate 38932.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 8240000, rate 41987.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 8250000, rate 42026.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 8260000, rate 41635.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 8270000, rate 41412.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 8280000, rate 41370.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 8290000, rate 42480.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 8300000, rate 42192.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 8310000, rate 42521.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 8320000, rate 41970.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 8330000, rate 42227.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 8340000, rate 42416.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 8350000, rate 39272.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 8360000, rate 36773.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 8370000, rate 43297.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 8380000, rate 42997.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 8390000, rate 42953.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 8400000, rate 42083.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 8410000, rate 42818.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 8420000, rate 42005.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 8430000, rate 40516.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 8440000, rate 41001.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 8450000, rate 40472.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 8460000, rate 41796.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 8470000, rate 41586.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 8480000, rate 41630.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 8490000, rate 38160.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 8500000, rate 41679.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 8510000, rate 39800.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 8520000, rate 41258.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 8530000, rate 44752.6 lines/sec, 46 clusters so far.\n",
      "Processing line: 8540000, rate 45691.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 8550000, rate 46523.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 8560000, rate 46348.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 8570000, rate 46474.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 8580000, rate 46789.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 8590000, rate 45664.2 lines/sec, 46 clusters so far.\n",
      "Processing line: 8600000, rate 46153.9 lines/sec, 46 clusters so far.\n",
      "Processing line: 8610000, rate 46101.4 lines/sec, 46 clusters so far.\n",
      "Processing line: 8620000, rate 45865.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 8630000, rate 43074.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 8640000, rate 43761.0 lines/sec, 46 clusters so far.\n",
      "Processing line: 8650000, rate 43275.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 8660000, rate 42585.3 lines/sec, 46 clusters so far.\n",
      "Processing line: 8670000, rate 40506.5 lines/sec, 46 clusters so far.\n",
      "Processing line: 8680000, rate 42614.1 lines/sec, 46 clusters so far.\n",
      "Processing line: 8690000, rate 42071.8 lines/sec, 46 clusters so far.\n",
      "Processing line: 8700000, rate 42582.7 lines/sec, 46 clusters so far.\n",
      "Processing line: 8710000, rate 43052.4 lines/sec, 46 clusters so far.\n",
      "Input (8718961): Exception in receiveBlock for block blk#6224343649004202692 java.net.SocketTimeoutException: 490000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.251.127.243:44092 remote=/10.251.107.50:50010]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 39, \"cluster_size\": 2, \"template_mined\": \"Exception in receiveBlock for block <:BLOCKID:> java.net.SocketTimeoutException: 490000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected <:*:> <:*:>\", \"cluster_count\": 46}\n",
      "Processing line: 8720000, rate 41372.8 lines/sec, 46 clusters so far.\n",
      "Input (8729166): Exception in receiveBlock for block blk#-6994808880344424033 java.io.IOException: Broken pipe\n",
      "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 47, \"cluster_size\": 1, \"template_mined\": \"Exception in receiveBlock for block <:BLOCKID:> java.io.IOException: Broken pipe\", \"cluster_count\": 47}\n",
      "Processing line: 8730000, rate 36899.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 8740000, rate 39089.6 lines/sec, 47 clusters so far.\n",
      "Input (8746310): PacketResponder blk#-4723951162006187997 1 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 59799 millis timeout left.\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 45, \"cluster_size\": 2, \"template_mined\": \"PacketResponder <:BLOCKID:> 1 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. <:*:> millis timeout left.\", \"cluster_count\": 47}\n",
      "Processing line: 8750000, rate 38209.2 lines/sec, 47 clusters so far.\n",
      "Input (8757961): PacketResponder blk#8006271611835981128 1 Exception java.io.IOException: The stream is closed\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 38, \"cluster_size\": 9, \"template_mined\": \"PacketResponder <:BLOCKID:> <:*:> Exception java.io.IOException: <:*:> <:*:> <:*:> <:*:>\", \"cluster_count\": 47}\n",
      "Processing line: 8760000, rate 39376.4 lines/sec, 47 clusters so far.\n",
      "Input (8760413): Exception in receiveBlock for block blk#584730932939516842 java.net.SocketTimeoutException: 485000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.250.19.227:55473 remote=/10.251.107.50:50010]\n",
      "Result: {\"change_type\": \"cluster_template_changed\", \"cluster_id\": 39, \"cluster_size\": 4, \"template_mined\": \"Exception in receiveBlock for block <:BLOCKID:> java.net.SocketTimeoutException: <:*:> millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected <:*:> <:*:>\", \"cluster_count\": 47}\n",
      "Processing line: 8770000, rate 36816.3 lines/sec, 47 clusters so far.\n",
      "total          : took   181.18 s (100.00%),  8,775,579 samples,   20.65 ms / 1000 samples,       48,436.27 hz\n",
      "drain          : took   103.96 s ( 57.38%),  8,775,579 samples,   11.85 ms / 1000 samples,       84,412.14 hz\n",
      "tree_search    : took    44.03 s ( 24.30%),  8,775,579 samples,    5.02 ms / 1000 samples,      199,331.15 hz\n",
      "mask           : took    41.24 s ( 22.76%),  8,775,579 samples,    4.70 ms / 1000 samples,      212,788.64 hz\n",
      "cluster_exist  : took    26.88 s ( 14.84%),  8,775,532 samples,    3.06 ms / 1000 samples,      326,485.49 hz\n",
      "create_cluster : took     0.00 s (  0.00%),         47 samples,   20.30 ms / 1000 samples,       49,258.44 hz\n",
      "Processing line: 8780000, rate 39277.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 8790000, rate 39687.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 8800000, rate 42647.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 8810000, rate 42548.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 8820000, rate 42179.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 8830000, rate 42574.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 8840000, rate 42631.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 8850000, rate 42736.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 8860000, rate 41524.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 8870000, rate 41428.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 8880000, rate 42264.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 8890000, rate 40920.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 8900000, rate 42634.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 8910000, rate 36241.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 8920000, rate 41847.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 8930000, rate 42277.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 8940000, rate 43335.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 8950000, rate 42713.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 8960000, rate 40873.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 8970000, rate 41748.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 8980000, rate 42922.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 8990000, rate 42353.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9000000, rate 42663.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9010000, rate 42687.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 9020000, rate 42000.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 9030000, rate 42529.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 9040000, rate 42450.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9050000, rate 42337.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 9060000, rate 40621.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 9070000, rate 41752.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9080000, rate 42189.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 9090000, rate 42722.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 9100000, rate 42443.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 9110000, rate 42640.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 9120000, rate 41720.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9130000, rate 41283.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9140000, rate 40941.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 9150000, rate 41306.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 9160000, rate 41408.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 9170000, rate 40575.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9180000, rate 41770.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9190000, rate 41289.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9200000, rate 38993.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9210000, rate 41434.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9220000, rate 39707.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 9230000, rate 42470.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 9240000, rate 45405.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9250000, rate 45233.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 9260000, rate 46560.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9270000, rate 46263.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 9280000, rate 46379.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9290000, rate 45971.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9300000, rate 46537.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 9310000, rate 45225.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 9320000, rate 45151.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 9330000, rate 44886.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9340000, rate 42791.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 9350000, rate 42752.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 9360000, rate 43067.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 9370000, rate 41957.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 9380000, rate 40634.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 9390000, rate 42459.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 9400000, rate 41855.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 9410000, rate 42625.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 9420000, rate 41957.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 9430000, rate 42329.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9440000, rate 41857.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 9450000, rate 41919.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9460000, rate 42215.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9470000, rate 42646.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9480000, rate 42593.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 9490000, rate 42892.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9500000, rate 42497.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 9510000, rate 42089.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 9520000, rate 42566.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9530000, rate 42730.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9540000, rate 42605.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 9550000, rate 40915.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9560000, rate 41192.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 9570000, rate 41833.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 9580000, rate 42664.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 9590000, rate 41922.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9600000, rate 42847.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9610000, rate 41243.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9620000, rate 42035.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9630000, rate 40607.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 9640000, rate 42814.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 9650000, rate 42005.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 9660000, rate 42288.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 9670000, rate 42007.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9680000, rate 42483.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 9690000, rate 42170.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 9700000, rate 39533.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9710000, rate 40949.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 9720000, rate 42026.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9730000, rate 37319.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 9740000, rate 42238.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9750000, rate 37825.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9760000, rate 42465.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 9770000, rate 42015.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9780000, rate 42858.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 9790000, rate 42238.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9800000, rate 41903.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9810000, rate 41118.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 9820000, rate 41308.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 9830000, rate 41401.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9840000, rate 41537.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 9850000, rate 41410.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 9860000, rate 40611.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9870000, rate 39528.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 9880000, rate 40770.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9890000, rate 41344.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9900000, rate 41644.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9910000, rate 42290.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9920000, rate 45126.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 9930000, rate 45906.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 9940000, rate 45362.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 9950000, rate 46200.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 9960000, rate 46591.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 9970000, rate 45748.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 9980000, rate 46046.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 9990000, rate 44766.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10000000, rate 45296.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10010000, rate 43342.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10020000, rate 43727.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10030000, rate 43922.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10040000, rate 41992.0 lines/sec, 47 clusters so far.\n",
      "total          : took   207.59 s (100.00%), 10,041,887 samples,   20.67 ms / 1000 samples,       48,372.71 hz\n",
      "drain          : took   119.13 s ( 57.39%), 10,041,887 samples,   11.86 ms / 1000 samples,       84,293.12 hz\n",
      "tree_search    : took    50.48 s ( 24.32%), 10,041,887 samples,    5.03 ms / 1000 samples,      198,908.56 hz\n",
      "mask           : took    47.28 s ( 22.77%), 10,041,887 samples,    4.71 ms / 1000 samples,      212,411.51 hz\n",
      "cluster_exist  : took    30.79 s ( 14.83%), 10,041,840 samples,    3.07 ms / 1000 samples,      326,139.96 hz\n",
      "create_cluster : took     0.00 s (  0.00%),         47 samples,   20.30 ms / 1000 samples,       49,258.44 hz\n",
      "Processing line: 10050000, rate 41486.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10060000, rate 37833.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10070000, rate 43000.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10080000, rate 41715.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 10090000, rate 42627.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10100000, rate 42365.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10110000, rate 42485.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 10120000, rate 42185.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10130000, rate 42465.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 10140000, rate 40594.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10150000, rate 41222.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 10160000, rate 42536.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 10170000, rate 42378.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 10180000, rate 43160.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 10190000, rate 42522.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 10200000, rate 43225.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10210000, rate 42497.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 10220000, rate 41773.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 10230000, rate 41288.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10240000, rate 42037.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10250000, rate 42371.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10260000, rate 42597.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10270000, rate 42323.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10280000, rate 42498.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 10290000, rate 41128.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10300000, rate 42219.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10310000, rate 42822.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 10320000, rate 42240.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10330000, rate 42551.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10340000, rate 41570.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10350000, rate 42525.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 10360000, rate 42923.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10370000, rate 41991.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10380000, rate 42754.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10390000, rate 42663.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10400000, rate 42439.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 10410000, rate 41967.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10420000, rate 41154.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 10430000, rate 42399.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 10440000, rate 42102.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 10450000, rate 43044.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10460000, rate 42732.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10470000, rate 41876.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10480000, rate 39537.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10490000, rate 43008.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10500000, rate 43126.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10510000, rate 42168.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10520000, rate 35404.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10530000, rate 41559.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10540000, rate 42351.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 10550000, rate 42471.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10560000, rate 42429.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 10570000, rate 41280.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10580000, rate 40741.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10590000, rate 41961.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10600000, rate 42227.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 10610000, rate 40833.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10620000, rate 42080.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 10630000, rate 41850.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10640000, rate 42579.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10650000, rate 42904.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10660000, rate 42497.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10670000, rate 42614.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10680000, rate 41460.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10690000, rate 42567.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 10700000, rate 42305.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10710000, rate 40835.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 10720000, rate 42259.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 10730000, rate 41095.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10740000, rate 42401.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 10750000, rate 42330.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 10760000, rate 42340.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10770000, rate 41969.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10780000, rate 42498.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 10790000, rate 42869.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10800000, rate 42778.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10810000, rate 41774.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 10820000, rate 41956.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 10830000, rate 42882.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 10840000, rate 42280.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10850000, rate 42273.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 10860000, rate 40999.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 10870000, rate 38984.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10880000, rate 40876.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10890000, rate 42391.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10900000, rate 38640.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10910000, rate 35444.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10920000, rate 42803.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10930000, rate 40914.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 10940000, rate 42149.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 10950000, rate 42291.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10960000, rate 42043.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 10970000, rate 40645.9 lines/sec, 47 clusters so far.\n",
      "Processing line: 10980000, rate 41354.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 10990000, rate 40106.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 11000000, rate 41132.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 11010000, rate 36815.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 11020000, rate 41455.6 lines/sec, 47 clusters so far.\n",
      "Processing line: 11030000, rate 40635.4 lines/sec, 47 clusters so far.\n",
      "Processing line: 11040000, rate 40648.1 lines/sec, 47 clusters so far.\n",
      "Processing line: 11050000, rate 41889.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 11060000, rate 40715.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 11070000, rate 42185.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 11080000, rate 46056.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 11090000, rate 45813.8 lines/sec, 47 clusters so far.\n",
      "Processing line: 11100000, rate 45523.0 lines/sec, 47 clusters so far.\n",
      "Processing line: 11110000, rate 46503.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 11120000, rate 45690.3 lines/sec, 47 clusters so far.\n",
      "Processing line: 11130000, rate 45905.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 11140000, rate 44570.2 lines/sec, 47 clusters so far.\n",
      "Processing line: 11150000, rate 46945.7 lines/sec, 47 clusters so far.\n",
      "Processing line: 11160000, rate 45176.5 lines/sec, 47 clusters so far.\n",
      "Processing line: 11170000, rate 44801.1 lines/sec, 47 clusters so far.\n",
      "--- Done processing file in 263.18 sec. Total of 11175629 lines, rate 42463.9 lines/sec, 47 clusters\n",
      "ID=1     : size=1723232   : Receiving block <:BLOCKID:> src: <:*:> dest: <:*:>\n",
      "ID=5     : size=1719741   : BLOCK* NameSystem.addStoredBlock: blockMap updated: <:*:> is added to <:BLOCKID:> size <:*:>\n",
      "ID=3     : size=1706728   : PacketResponder <:*:> for block <:BLOCKID:> <:*:>\n",
      "ID=4     : size=1706514   : Received block <:BLOCKID:> of size <:*:> from <:*:>\n",
      "ID=7     : size=1408984   : <:*:> block <:BLOCKID:> <:*:> <:*:>\n",
      "ID=34    : size=1396174   : BLOCK* NameSystem.delete: <:BLOCKID:> is added to invalidSet of <:*:>\n",
      "ID=12    : size=574940    : BLOCK* NameSystem.allocateBlock: <:*:> temporary/ task <:*:> <:*:> <:*:> <:*:> <:*:> <:BLOCKID:>\n",
      "ID=10    : size=428726    : <:*:> Served block <:BLOCKID:> to <:*:>\n",
      "ID=37    : size=356207    : <:*:> exception while serving <:BLOCKID:> to <:*:>\n",
      "ID=15    : size=120036    : Verification succeeded for <:BLOCKID:>\n",
      "ID=6     : size=7097      : Received block <:BLOCKID:> src: <:*:> dest: <:*:> of size <:*:>\n",
      "ID=13    : size=6837      : BLOCK* ask <:*:> to replicate <:BLOCKID:> to datanode(s) <:*:>\n",
      "ID=14    : size=6837      : <:*:> Starting thread to transfer block <:BLOCKID:> to <:*:>\n",
      "ID=35    : size=5545      : Unexpected error trying to delete block <:BLOCKID:>. BlockInfo not found in volumeMap.\n",
      "ID=18    : size=3226      : writeBlock <:BLOCKID:> received exception java.io.IOException: Could not read from stream\n",
      "ID=19    : size=1464      : Receiving empty packet for block <:BLOCKID:>\n",
      "ID=36    : size=1288      : BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for <:BLOCKID:> on <:*:> size <:*:> But it does not belong to any file.\n",
      "ID=23    : size=975       : BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for <:BLOCKID:> on <:*:> size <:*:>\n",
      "ID=8     : size=165       : <:*:> Starting thread to transfer block <:BLOCKID:> to <:*:> <:*:>\n",
      "ID=9     : size=165       : BLOCK* ask <:*:> to replicate <:BLOCKID:> to datanode(s) <:*:> <:*:>\n",
      "ID=22    : size=76        : writeBlock <:BLOCKID:> received exception <:*:> <:*:> <:*:> <:*:> <:*:>\n",
      "ID=21    : size=75        : Exception in receiveBlock for block <:BLOCKID:> java.io.IOException: Connection reset by peer\n",
      "ID=2     : size=73        : BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job <:*:> <:*:> <:BLOCKID:>\n",
      "ID=16    : size=67        : writeBlock <:BLOCKID:> received exception <:*:>\n",
      "ID=27    : size=65        : Changing block file offset of block <:BLOCKID:> from 0 to <:*:> meta file offset to <:*:>\n",
      "ID=26    : size=56        : Exception in receiveBlock for block <:BLOCKID:> <:*:>\n",
      "ID=43    : size=47        : PendingReplicationMonitor timed out block <:BLOCKID:>\n",
      "ID=28    : size=34        : <:*:> writing block <:BLOCKID:> to mirror <:*:>\n",
      "ID=25    : size=33        : PacketResponder <:BLOCKID:> <:*:> Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected <:*:> <:*:> <:*:> millis timeout left.\n",
      "ID=17    : size=29        : PacketResponder <:BLOCKID:> <:*:> Exception <:*:>\n",
      "ID=11    : size=24        : BLOCK* NameSystem.allocateBlock: <:*:> logs/history/ip-10-250-19-102.ec2.internal <:*:> job <:*:> <:*:> conf.xml. <:BLOCKID:>\n",
      "ID=20    : size=24        : BLOCK* NameSystem.allocateBlock: <:*:> logs/history/ip-10-250-19-102.ec2.internal <:*:> job <:*:> <:*:> root <:*:> <:BLOCKID:>\n",
      "ID=24    : size=22        : PacketResponder <:BLOCKID:> <:*:> Exception java.net.SocketTimeoutException: <:*:> millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected <:*:> <:*:>\n",
      "ID=29    : size=16        : Exception in receiveBlock for block <:BLOCKID:> java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected <:*:> <:*:> <:*:> millis timeout left.\n",
      "ID=44    : size=16        : writeBlock <:BLOCKID:> received exception java.io.IOException: <:*:> <:*:>\n",
      "ID=40    : size=15        : writeBlock <:BLOCKID:> received exception java.net.SocketTimeoutException: <:*:> millis timeout while waiting for channel to be ready for <:*:> ch : java.nio.channels.SocketChannel[connected <:*:> <:*:>\n",
      "ID=38    : size=13        : PacketResponder <:BLOCKID:> <:*:> Exception java.io.IOException: <:*:> <:*:> <:*:> <:*:>\n",
      "ID=46    : size=10        : Adding an already existing block <:BLOCKID:>\n",
      "ID=32    : size=9         : writeBlock <:BLOCKID:> received exception java.io.IOException: Block <:BLOCKID:> is valid, and cannot be written to.\n",
      "ID=33    : size=9         : <:*:> to transfer <:BLOCKID:> to <:*:> got java.io.IOException: Connection reset by peer\n",
      "ID=30    : size=7         : writeBlock <:BLOCKID:> received exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected <:*:> <:*:> <:*:> millis timeout left.\n",
      "ID=31    : size=6         : PacketResponder <:BLOCKID:> <:*:> Exception java.io.IOException: Broken pipe\n",
      "ID=39    : size=5         : Exception in receiveBlock for block <:BLOCKID:> java.net.SocketTimeoutException: <:*:> millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected <:*:> <:*:>\n",
      "ID=41    : size=5         : Reopen Block <:BLOCKID:>\n",
      "ID=45    : size=5         : PacketResponder <:BLOCKID:> 1 Exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. <:*:> millis timeout left.\n",
      "ID=42    : size=4         : BLOCK* Removing block <:BLOCKID:> from neededReplications as it does not belong to any file.\n",
      "ID=47    : size=3         : Exception in receiveBlock for block <:BLOCKID:> java.io.IOException: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "mine_template = mineModel(f = open(\"../DATA/HDFS.log\"),cfg='../data/drain3_hdfs.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b606034e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received block blk#-1608999687919862906 src: /10.250.14.224:35754 dest: /10.250.14.224:50010 of size 91178\n",
      "Received block <:BLOCKID:> src: <:*:> dest: <:*:> of size <:*:>\n",
      "ExtractedParameter(value='blk#-1608999687919862906', mask_name='BLOCKID')\n",
      "ExtractedParameter(value='/10.250.14.224:35754', mask_name='*')\n",
      "ExtractedParameter(value='/10.250.14.224:50010', mask_name='*')\n",
      "ExtractedParameter(value='91178', mask_name='*')\n"
     ]
    }
   ],
   "source": [
    "line = 'Received block blk_-1608999687919862906 src: /10.250.14.224:35754 dest: /10.250.14.224:50010 of size 91178'\n",
    "line = line.replace('blk_','blk#')\n",
    "print(line)\n",
    "match = mine_template.match(line)\n",
    "print(match.get_template())\n",
    "for p in  mine_template.extract_parameters(match.get_template(),line,False):\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f7dca",
   "metadata": {},
   "source": [
    "#### Get Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5e27e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = []\n",
    "templates = []\n",
    "identifier = []\n",
    "\n",
    "f = open(\"../DATA/HDFS.log\")\n",
    "lines = f.readlines()     \n",
    "\n",
    "for line in lines:   \n",
    "   try:\n",
    "      line = line.partition(': ')[2]\n",
    "      line = line.replace('blk_','blk#')\n",
    "      match = mine_template.match(line)\n",
    "      event.append('E'+ str(match.cluster_id))\n",
    "      templates.append(match.get_template())\n",
    "      ps = mine_template.extract_parameters(match.get_template(),line,False)\n",
    "      for p in ps:\n",
    "         if p.mask_name == 'BLOCKID':\n",
    "            identifier.append(p.value)\n",
    "            break\n",
    "   except:\n",
    "      print(line)\n",
    "\n",
    "hdfs_templates = pd.DataFrame({'event': event,'template': templates,'identifier':identifier})\n",
    "hdfs_templates['identifier'] = hdfs_templates['identifier'].str.replace('blk#','blk_')\n",
    "hdfs_templates.to_csv('../data/hdfs_templates.csv', index=False)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcb5fbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event</th>\n",
       "      <th>template</th>\n",
       "      <th>identifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E1</td>\n",
       "      <td>Receiving block &lt;:BLOCKID:&gt; src: &lt;:*:&gt; dest: &lt;...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E2</td>\n",
       "      <td>BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E1</td>\n",
       "      <td>Receiving block &lt;:BLOCKID:&gt; src: &lt;:*:&gt; dest: &lt;...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E1</td>\n",
       "      <td>Receiving block &lt;:BLOCKID:&gt; src: &lt;:*:&gt; dest: &lt;...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E3</td>\n",
       "      <td>PacketResponder &lt;:*:&gt; for block &lt;:BLOCKID:&gt; &lt;:*:&gt;</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11175624</th>\n",
       "      <td>E15</td>\n",
       "      <td>Verification succeeded for &lt;:BLOCKID:&gt;</td>\n",
       "      <td>blk_-6171368032583208892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11175625</th>\n",
       "      <td>E15</td>\n",
       "      <td>Verification succeeded for &lt;:BLOCKID:&gt;</td>\n",
       "      <td>blk_6195025276114316035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11175626</th>\n",
       "      <td>E15</td>\n",
       "      <td>Verification succeeded for &lt;:BLOCKID:&gt;</td>\n",
       "      <td>blk_-3339773404714332088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11175627</th>\n",
       "      <td>E15</td>\n",
       "      <td>Verification succeeded for &lt;:BLOCKID:&gt;</td>\n",
       "      <td>blk_1037231945509285002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11175628</th>\n",
       "      <td>E15</td>\n",
       "      <td>Verification succeeded for &lt;:BLOCKID:&gt;</td>\n",
       "      <td>blk_4258862871822415442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11175629 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         event                                           template  \\\n",
       "0           E1  Receiving block <:BLOCKID:> src: <:*:> dest: <...   \n",
       "1           E2  BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...   \n",
       "2           E1  Receiving block <:BLOCKID:> src: <:*:> dest: <...   \n",
       "3           E1  Receiving block <:BLOCKID:> src: <:*:> dest: <...   \n",
       "4           E3  PacketResponder <:*:> for block <:BLOCKID:> <:*:>   \n",
       "...        ...                                                ...   \n",
       "11175624   E15             Verification succeeded for <:BLOCKID:>   \n",
       "11175625   E15             Verification succeeded for <:BLOCKID:>   \n",
       "11175626   E15             Verification succeeded for <:BLOCKID:>   \n",
       "11175627   E15             Verification succeeded for <:BLOCKID:>   \n",
       "11175628   E15             Verification succeeded for <:BLOCKID:>   \n",
       "\n",
       "                        identifier  \n",
       "0         blk_-1608999687919862906  \n",
       "1         blk_-1608999687919862906  \n",
       "2         blk_-1608999687919862906  \n",
       "3         blk_-1608999687919862906  \n",
       "4         blk_-1608999687919862906  \n",
       "...                            ...  \n",
       "11175624  blk_-6171368032583208892  \n",
       "11175625   blk_6195025276114316035  \n",
       "11175626  blk_-3339773404714332088  \n",
       "11175627   blk_1037231945509285002  \n",
       "11175628   blk_4258862871822415442  \n",
       "\n",
       "[11175629 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7498e2",
   "metadata": {},
   "source": [
    "#### Generate template sequence, group by identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e0005a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_log_df = hdfs_templates.groupby('identifier')['event'].apply(lambda x:x.str.cat(sep=' ')).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1326df4d",
   "metadata": {},
   "source": [
    "#### Merged Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79d0a649",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.read_csv('../data/anomaly_label.csv')\n",
    "hdfs_log_df = label_df.merge(hdfs_log_df, left_on='BlockId', right_on='identifier')\n",
    "for label in [\"Normal\",\"Anomaly\"]:\n",
    "    if label == \"Normal\":\n",
    "        hdfs_log_df.iloc[hdfs_log_df[label==hdfs_log_df['Label']].index.tolist(),1] = 0\n",
    "    else:\n",
    "        hdfs_log_df.iloc[hdfs_log_df[label==hdfs_log_df['Label']].index.tolist(),1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa8964be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockId</th>\n",
       "      <th>Label</th>\n",
       "      <th>identifier</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>0</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>E1 E2 E1 E1 E3 E3 E4 E4 E3 E4 E5 E5 E5 E6 E1 E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_7503483334202473044</td>\n",
       "      <td>0</td>\n",
       "      <td>blk_7503483334202473044</td>\n",
       "      <td>E1 E1 E2 E1 E3 E4 E3 E4 E3 E4 E5 E5 E5 E10 E15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>1</td>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>E1 E2 E1 E1 E3 E4 E3 E4 E3 E4 E10 E5 E5 E5 E10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-9073992586687739851</td>\n",
       "      <td>0</td>\n",
       "      <td>blk_-9073992586687739851</td>\n",
       "      <td>E1 E11 E1 E1 E3 E4 E3 E4 E3 E4 E5 E5 E5 E15 E1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blk_7854771516489510256</td>\n",
       "      <td>0</td>\n",
       "      <td>blk_7854771516489510256</td>\n",
       "      <td>E1 E1 E12 E1 E3 E4 E3 E4 E3 E4 E5 E5 E5 E15 E1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575056</th>\n",
       "      <td>blk_1019720114020043203</td>\n",
       "      <td>0</td>\n",
       "      <td>blk_1019720114020043203</td>\n",
       "      <td>E1 E12 E1 E1 E5 E3 E4 E3 E4 E3 E4 E5 E5 E34 E3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575057</th>\n",
       "      <td>blk_-2683116845478050414</td>\n",
       "      <td>0</td>\n",
       "      <td>blk_-2683116845478050414</td>\n",
       "      <td>E1 E12 E1 E1 E3 E4 E5 E3 E4 E3 E4 E5 E5 E34 E3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575058</th>\n",
       "      <td>blk_5595059397348477632</td>\n",
       "      <td>0</td>\n",
       "      <td>blk_5595059397348477632</td>\n",
       "      <td>E1 E1 E1 E12 E3 E4 E3 E4 E3 E4 E5 E5 E5 E34 E3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575059</th>\n",
       "      <td>blk_1513937873877967730</td>\n",
       "      <td>0</td>\n",
       "      <td>blk_1513937873877967730</td>\n",
       "      <td>E1 E1 E1 E12 E3 E4 E3 E4 E3 E4 E5 E5 E5 E34 E3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575060</th>\n",
       "      <td>blk_-9128742458709757181</td>\n",
       "      <td>1</td>\n",
       "      <td>blk_-9128742458709757181</td>\n",
       "      <td>E1 E1 E1 E12 E3 E4 E3 E4 E3 E4 E36 E5 E36 E5 E...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>575061 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         BlockId Label                identifier  \\\n",
       "0       blk_-1608999687919862906     0  blk_-1608999687919862906   \n",
       "1        blk_7503483334202473044     0   blk_7503483334202473044   \n",
       "2       blk_-3544583377289625738     1  blk_-3544583377289625738   \n",
       "3       blk_-9073992586687739851     0  blk_-9073992586687739851   \n",
       "4        blk_7854771516489510256     0   blk_7854771516489510256   \n",
       "...                          ...   ...                       ...   \n",
       "575056   blk_1019720114020043203     0   blk_1019720114020043203   \n",
       "575057  blk_-2683116845478050414     0  blk_-2683116845478050414   \n",
       "575058   blk_5595059397348477632     0   blk_5595059397348477632   \n",
       "575059   blk_1513937873877967730     0   blk_1513937873877967730   \n",
       "575060  blk_-9128742458709757181     1  blk_-9128742458709757181   \n",
       "\n",
       "                                                    event  \n",
       "0       E1 E2 E1 E1 E3 E3 E4 E4 E3 E4 E5 E5 E5 E6 E1 E...  \n",
       "1       E1 E1 E2 E1 E3 E4 E3 E4 E3 E4 E5 E5 E5 E10 E15...  \n",
       "2       E1 E2 E1 E1 E3 E4 E3 E4 E3 E4 E10 E5 E5 E5 E10...  \n",
       "3       E1 E11 E1 E1 E3 E4 E3 E4 E3 E4 E5 E5 E5 E15 E1...  \n",
       "4       E1 E1 E12 E1 E3 E4 E3 E4 E3 E4 E5 E5 E5 E15 E1...  \n",
       "...                                                   ...  \n",
       "575056  E1 E12 E1 E1 E5 E3 E4 E3 E4 E3 E4 E5 E5 E34 E3...  \n",
       "575057  E1 E12 E1 E1 E3 E4 E5 E3 E4 E3 E4 E5 E5 E34 E3...  \n",
       "575058  E1 E1 E1 E12 E3 E4 E3 E4 E3 E4 E5 E5 E5 E34 E3...  \n",
       "575059  E1 E1 E1 E12 E3 E4 E3 E4 E3 E4 E5 E5 E5 E34 E3...  \n",
       "575060  E1 E1 E1 E12 E3 E4 E3 E4 E3 E4 E36 E5 E36 E5 E...  \n",
       "\n",
       "[575061 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d705c184",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_log_df.to_csv(\"../data/hdfs_log.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
